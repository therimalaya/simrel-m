\documentclass[12pt,A4paper,authoryear]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
%% Other Customizations
\usepackage[hyphens]{url}
\usepackage{lineno} % add
\usepackage{caption}


\usepackage{setspace}
\setstretch{1.25}

\usepackage{mathpazo}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\biboptions{sort&compress} % For natbib
\usepackage{booktabs} % book-quality tables

% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \def\@oddfoot{\it \hfill\today}%
  \let\@evenfoot\@oddfoot}
\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath}

\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[utf8]{inputenc}
\else % if luatex or xelatex
\usepackage{fontspec}
\ifxetex
\usepackage{xltxtra,xunicode}
\fi
\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
\newcommand{\euro}{€}
\setmonofont{Source Code Pro}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable}
\ifxetex
\usepackage[setpagesize=false, % page size defined by xetex
unicode=false, % unicode breaks when used with xetex
xetex]{hyperref}
\else
\usepackage[unicode=true]{hyperref}
\fi

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header


% \usepackage[nomarkers]{endfloat}

%% My customizations %%%%%%%%
%% Loading Packages
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{tabularx}
\usepackage[dvipsnames]{xcolor}
\usepackage{pgf, tikz}
\usepackage{amsthm}

%% Custom macros
\newtheorem{mydef}{Definition}
\newcommand{\bs}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
\newcommand{\seq}[3][1]{\ensuremath{#2_{#1},\ldots,\,#2_{#3}}}
\newcommand{\note}[1]{\marginpar{\scriptsize\tt{\color{RoyalBlue}#1}}}
\newcommand{\edit}[1]{{\color{OrangeRed} #1}}

%% Custom Lengths
\setlength{\parindent}{0pt}
\setlength{\parskip}{9pt}

%% Define Colors
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{YellowOrange}
\colorlet{myurlcolor}{Aquamarine}

%% Hyperlink Setup
\AtBeginDocument{%
  \hypersetup{breaklinks=true,
    bookmarks=true,
    pdfauthor={},
    pdftitle={simrel-m: A versatile tool for simulating multi-response linear model data},
    colorlinks=true,
    urlcolor=myurlcolor!\myshade!black,
    linkcolor=mylinkcolor!\myshade!black,
    citecolor=mycitecolor!\myshade!black,
    pdfborder={0 0 0}}
}

\urlstyle{same}  % don't use monospace font for urls

%% End my customizations %%%%%%%%%

\begin{document}
\begin{frontmatter}

  \title{\texttt{simrel-m}: A versatile tool for simulating multi-response linear
model data}
    \author[]{Raju Rimal}
  
  
    \author[]{Trygve Almøy}
  
  
    \author[Norwegian University of Life Sciences]{Solve Sæbø\corref{c1}}
  
   \cortext[c1]{Dep. of Chemistry and Food Science, NMBU, Ås
(\href{http://nmbu.no}{nmbu.no})}
    
  \begin{abstract}
    \small
    Data science is generating enormous amounts of data, and new and
    advanced analytical methods are constantly being developed to cope with
    the challenge of extracting information from such ``big-data''.
    Researchers often use simulated data to assess and document the
    properties of these new methods, and in this paper we present
    \texttt{simrel-m}, which is a versatile and transparent tool for
    simulating linear model data with extensive range of adjustable
    properties. The method is based on the concept of relevant components
    \citet{helland1994comparison}, which is equivalent to the envelope model
    \citet{cook2013envelopes}. It is a multi-response extension of
    \texttt{simrel} \citet{saebo2015simrel}, and as \texttt{simrel} the new
    approach is essentially based on random rotations of latent relevant
    components to obtain a predictor matrix \(\mathbf{X}\), but in addition
    we introduce random rotations of latent components spanning a response
    space in order to obtain a multivariate response matrix \(\mathbf{Y}\).
    The properties of the linear relation between \(\mathbf{X}\) and
    \(\mathbf{Y}\) are defined by a small set of input parameters which
    allow versatile and adjustable simulations. Sub-space rotations also
    allow for generating data suitable for testing variable selection
    methods in multi-response settings. The method is implemented as an
    R-package which serves as an extension of the existing \texttt{simrel}
    packages \citet{saebo2015simrel}.
  \end{abstract}
   \begin{keyword} \footnotesize \texttt{simrel-2.0}, \texttt{simrel} package in r, data
simulation, linear model, \texttt{simrel-m} \sep \end{keyword}
\end{frontmatter}

\section{Introduction}\label{introduction}

Technological advancement has opened a door for complex and
sophisticated scientific experiments that was not possible before. Due
to this change, enormous amounts of raw data are generated which
contains massive information but difficult to excavate. Finding
information and performing scientific research on these raw data has now
become another problem. In order to tackle this situation new methods
are being developed. However, before implementing any method, it is
essential to test its performance. Often, researchers use simulated data
for the purpose which itself is a time-consuming process. The main focus
of this paper is to present a simulation method, along with an r-package
called \texttt{simrel-m}, that is versatile in nature and yet simple to
use.

The simulation method we are discussing here is based on principal of
relevant space for prediction \citep{helland1994comparison} which
assumes that there exists a subspace in the complete space of response
variables that is spanned by a subset of eigenvectors of predictor
variables. The r-package based on this method lets user to specify
various population properties such as which components of predictors
\((\mathbf{X})\) are relevant for a component of responses
\(\mathbf{Y}\) and how the eigenvalues of \(\mathbf{X}\) decreases. This
enables the possibility to construct data for evaluating estimation
methods and methods developed for variable selection.

Among several literatures in simulation
({\color{red}\texttt{which\ literatures}}), \citet{ripley2009stochastic}
has exhaustively discussed the topic. In addition, many literatures
({\color{red}\texttt{which\ literatures}}) are available on studies
which has implemented simulated data in order to investigate new
estimation methods and prediction strategy
\citep[see:][]{cook2015simultaneous, cook2013envelopes, helland2012near}.
However, most of the simulations in these studies were developed to
address their specific problem. A systematic tool for simulating linear
model data with single response, which could serve as a general tool for
all such comparisons, was presented in \citet{saebo2015simrel} and as
r-package \texttt{simrel}. This paper extends \texttt{simrel} in order
to simulate linear model data with multivariate response with an
r-package \texttt{simrel-m}.

\section{Statistical Model}\label{statistical-model}

Let us consider a random regression model in
equation\textasciitilde{}\eqref{eq:rand-reg-model} as our point of
departure.

\begin{equation}
  \begin{bmatrix}\mathbf{Y}\\ \mathbf{X}\end{bmatrix} \sim N
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_Y \\
      \boldsymbol{\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{YY} & \boldsymbol{\Sigma}_{YX} \\
      \boldsymbol{\Sigma}_{XY} & \boldsymbol{\Sigma}_{XX}
    \end{bmatrix}
  \right)
  \label{eq:rand-reg-model}
\end{equation}

where, \(\mathbf{Y}\) is a response matrix with \(m\) response variables
\(y_1, y_2, \ldots y_m\) with mean vector of \(\boldsymbol{\mu}_Y\) and
\(\mathbf{X}\) is vector of \(p\) predictor variables. Further,

\begin{longtable}[]{@{}rl@{}}
\toprule
\(\boldsymbol{\Sigma}_{YY}\) & is variance-covariance matrix of response
\(\mathbf{Y}\)\tabularnewline
\(\boldsymbol{\Sigma}_{XX}\) & is variance-covariance matrix of
predictor variables \(\mathbf{X}\)\tabularnewline
\(\boldsymbol{\Sigma}_{XY}\) & is matrix of covariance between
\(\mathbf{X}\) and \(\mathbf{Y}\)\tabularnewline
\(\boldsymbol{\mu}_Y\) and \(\boldsymbol{\mu}_X\) & are mean vectors of
response \(\mathbf{Y}\) and predictor \(\mathbf{X}\)
respective.\tabularnewline
\bottomrule
\end{longtable}

A linear relationship between \(\mathbf{X}\) and \(\mathbf{Y}\) for
model\textasciitilde{}\eqref{eq:rand-reg-model} can be imagined as,

\begin{equation}
\mathbf{Y} = \boldsymbol{\mu}_Y + \boldsymbol{\beta}^t (\mathbf{X} - \boldsymbol{\mu}_X) + \boldsymbol{\varepsilon}
  \label{eq:linear-model}
\end{equation}

where, \(\boldsymbol{\beta}^t\) is regression coefficient and
\(\boldsymbol{\varepsilon}\) is error term such that
\(\boldsymbol{\varepsilon} \sim N\left(0, \boldsymbol{\Sigma}_{Y|X}\right)\).
The properties of the linear model in
equation\textasciitilde{}\eqref{eq:linear-model} can be expressed in terms
of covariance matrices from
equation\textasciitilde{}\eqref{eq:rand-reg-model}.

\begin{description}
\tightlist
\item[Regression Coefficients]
\[ \boldsymbol{\beta} = \boldsymbol{\Sigma}_{XY} \boldsymbol{\Sigma}_{XX}^{-1}\]
\item[Coefficient of Determination \(\boldsymbol{\rho}_Y^2\)]
The diagonal elements of coefficient of determination matrix
\(\boldsymbol{\rho}_Y^2\) gives the amount of variation that
\(\boldsymbol{X}\) has explained about \(\boldsymbol{Y}\) in
equation\textasciitilde{}\eqref{eq:linear-model}.
\[\boldsymbol{\rho}_Y^2 = \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\boldsymbol{\Sigma}_{YY}^{-1}\]
\item[Error variance]
The minimum error \(\boldsymbol{\Sigma}_{Y|X}\) of the model is,
\[\boldsymbol{\Sigma}_{Y|X} = \boldsymbol{\Sigma}_{YY} - \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\]
\end{description}

Let us define a transformation of \(\boldsymbol{X}\) and
\(\boldsymbol{Y}\) as, \(\mathbf{Z} = \mathbf{RX}\) and
\(\mathbf{W} = \mathbf{QY}\). Here, \(\mathbf{R}_{p\times p}\) and
\(\mathbf{Q}_{m\times m}\) are rotation matrices which rotates
\(\mathbf{X}\) and \(\mathbf{Y}\) giving \(\mathbf{Z}\) and
\(\mathbf{W}\) respectively. The random regression model in
equation\textasciitilde{}\eqref{eq:rand-reg-model} can be expressed with
these transformed variables as,

\begin{align}
  \begin{bmatrix}\mathbf{W} \\ 
  \boldsymbol{Z}\end{bmatrix}  & \sim N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_W \\ \boldsymbol{\mu}_Z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{WW} & \boldsymbol{\Sigma}_{WZ} \\
      \boldsymbol{\Sigma}_{ZW} & \boldsymbol{\Sigma}_{ZZ}
    \end{bmatrix} \right) \nonumber \\
  & = N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_Y \\
      \boldsymbol{R\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{YY}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{YX}\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{XY}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{XX}\mathbf{R}^t
    \end{bmatrix}
  \right)
  \label{eq:model3}
\end{align}

In addition, a linear model relating \(\mathbf{W}\) and \(\mathbf{Z}\)
can be written as,

\begin{equation}
\mathbf{W} =    \boldsymbol{\mu}_W + \boldsymbol{\alpha}^t \left(\mathbf{Z} - \boldsymbol{\mu}_Z\right) + \boldsymbol{\tau}
\label{eq:latent-model}
\end{equation}

where, \(\boldsymbol{\alpha}\) is regression coefficient for the
transformed model and
\(\boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{W|Z}\right)\).
Further, if both \(\mathbf{Q}\) and \(\mathbf{R}\) are orthonormal
matrix such that \(\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_q\) and
\(\mathbf{R}^t\mathbf{R} = \mathbf{I}_p\), the inverse transformation
can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{XX} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{ZZ} = \mathbf{R} \boldsymbol{\Sigma}_{XX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{XY} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{ZW} = \mathbf{R} \boldsymbol{\Sigma}_{XY} \mathbf{Q}^t \\
    \boldsymbol{\Sigma}_{YX} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{WZ} = \mathbf{Q} \boldsymbol{\Sigma}_{YX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{YY} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{WW} = \mathbf{Q} \boldsymbol{\Sigma}_{YY} \mathbf{Q}^t
  \end{matrix}
  \label{eq:cov-yx-wz}
\end{equation}

Here, we can find a direct connection between different population
properties between \eqref{eq:linear-model} and \eqref{eq:latent-model}.

\begin{description}
\tightlist
\item[Regression Coefficients]
\[
  \begin{aligned}
  \boldsymbol{\alpha} &= \boldsymbol{\Sigma}_{WZ} \boldsymbol{\Sigma}_{ZZ}^{-1}
  &&= \boldsymbol{Q\Sigma}_{YZ}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{XX}\mathbf{R}^t\right]^{-1} \\
  &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\right]\mathbf{R}^t
  &&= \mathbf{Q}\boldsymbol{\beta}\mathbf{R}^t
  \end{aligned}
  \]
\item[Error Variance]
Further, the noise variance of transformed
model\textasciitilde{}\eqref{eq:latent-model} is, \[
  \begin{aligned}
\boldsymbol{\Sigma}_{W|Z}
&= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t -
  \boldsymbol{Q \Sigma}_{YX}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{XX}\boldsymbol{R}^t\right]^{-1}
  \boldsymbol{R\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
&= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t - 
  \boldsymbol{Q \Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
&= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YY} -
  \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\right]\mathbf{Q}^{t} \nonumber \\
&= \mathbf{Q} \boldsymbol{\Sigma}_{Y|X}\mathbf{Q}^t
  \end{aligned}
  \]
\item[Population Coefficient of Determination]
The population coefficient of determination for
model\textasciitilde{}\eqref{eq:latent-model} is, \[
  \begin{aligned}
\boldsymbol{\rho}^2_W &= \boldsymbol{\Sigma}_{WZ} 
\boldsymbol{\Sigma}_{ZZ}^{-1} \boldsymbol{\Sigma}_{ZW} 
\boldsymbol{\Sigma}_{WW}^{-1} \\
  &=\mathbf{Q}^t
  \boldsymbol{\Sigma}_{YX}\mathbf{R}^t \left(\mathbf{R}\boldsymbol{\Sigma}_{XX}\mathbf{R}^t\right)^{-1}
  \mathbf{R}\boldsymbol{\Sigma}_{XY}\mathbf{Q}^t \left(\mathbf{Q} \boldsymbol{\Sigma}_{YY}^{-1} \mathbf{Q}^t\right) \nonumber \\
  &=\mathbf{Q}^t\left[\boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}\boldsymbol{\Sigma}_{XY}\boldsymbol{\Sigma}_{YY}^{-1}\right]\mathbf{Q} \\
  &= \mathbf{Q}\boldsymbol{\rho}_{Y}^2 \mathbf{Q}^t
  \end{aligned}
  \]
\end{description}

From eigenvalue decomposition principle, if
\(\boldsymbol{\Sigma}_{XX} = \mathbf{R}\boldsymbol{\Lambda}\mathbf{R}^t\)
and
\(\boldsymbol{\Sigma}_{YY} = \mathbf{Q}\boldsymbol{\Omega}\mathbf{Q}^t\)
then \(\mathbf{Z}\) and \(\mathbf{W}\) can be principal components of
\(\mathbf{X}\) and \(\mathbf{Y}\) respectively. Here,
\(\boldsymbol{\Sigma}\) and \(\boldsymbol{\Omega}\) are diagonal matrix
of eigenvalues corresponding to \(\mathbf{X}\) and \(\mathbf{Y}\)
respectively.

\section{Relevant Components}\label{relevant-components}

Let us consider a single response linear model with \(p\) predictors.

\[\mathbf{y} = \mu_y + \beta^t\left(\mathbf{X} - \mu_x\right) + \epsilon\]

where, \(\epsilon \sim N(0, \sigma^2)\) and \(\mathbf{X}\) and
\(\epsilon\) are random and independent. Following the principal of
relevant space and irrelevant space which are discussed extensively in
\citet{helland1994comparison}, \citet{Helland_2000},
\citet{helland2012near}, \citet{cook2013envelopes},
\citet{saebo2015simrel} and \citet{helland2017}, we can assume that
there exists a subspace of complete variable space which is relevant for
\(\mathbf{y}\). An orthogonal space to this space does not contain any
information about \(\mathbf{y}\) and are irrelevant. Here, the
\(y-\)relevant subspace of \(\mathbf{X}\) is spanned by a subset of
eigenvectors \(\mathbf{(e)}\) of covariance matrix of \(\mathbf{X}\),
i.e. \(\boldsymbol{\Sigma}_{XX})\).

This concept can be extended for \(m\) response so that the subspace of
\(\mathbf{X}\) is relevant for a subspace of \(\mathbf{Y}\). This
corresponds to the concept of simultaneous envelope \citep{Cook_2014}
where relevant (material) and irrelevant (immaterial) space were
discussed for both response and predictors.

\section{Model Parameterization}\label{model-parameterization}

In order to construct a covariance matrix of \(\mathbf{Z}\) and
\(\mathbf{W}\) for model in equation\textasciitilde{}\eqref{eq:model3}, we
need to identify \(1/2 (p+m)(p+m+1)\) unknowns. For the purpose of this
simulation, we implement some assumption to re-parameterize and simplify
the model parameters. This enables us to construct diverse nature of
model from few key parameters.

\begin{description}
\tightlist
\item[\textbf{Parameterization of \(\boldsymbol{\Sigma}_{ZZ}\):}]
Since \(\mathbf{X}\)'s are principal components of \(\mathbf{X}\), the
\(\boldsymbol{\Sigma}_{ZZ}\) can be a diagonal matrix with eigenvalues
\(\lambda_1, \ldots, \lambda_p\) of predictors \(\mathbf{X}\). Further,
we adopt following approximate parametric representation of these
eigenvalues,
\[\lambda_j = e^{-\gamma(i - 1)}, \gamma >0 \text{ and } j = 1, 2, \ldots, p\]
Here as \(\gamma\) increases, the decline of eigenvalues becomes steeper
and hence a single parameter \(\gamma\) can be used for
\(\boldsymbol{\Sigma}_{ZZ}\).
\item[\textbf{Parameterization of \(\boldsymbol{\Sigma}_{WW}\):}]
Here, we assume that \(\mathbf{W}\)'s are independent and thus their
covariance matrix is considered to be Identity \(\mathbf{I}_m\).
\item[\textbf{Parameterization of \(\boldsymbol{\Sigma}_{ZW}\):}]
After parameterization of \(\boldsymbol{\Sigma}_{ZZ}\) and
\(\boldsymbol{\Sigma}_{WW}\), we are left with \(m \times p\) number of
unknowns corresponding to \(\boldsymbol{\Sigma}_{ZW}\). The elements in
this covariance matrix depends on position of x-component that are
relevant for \(\mathbf{Y}\). In order to re-parameterize this covariance
matrix, it is necessary to discuss about the position of relevant
components in details.
\end{description}

\subsection{Position of relevant
components}\label{position-of-relevant-components}

Let only \(k_1\) components are relevant for \(\mathbf{w}_1\), \(k_2\)
components are relevant for \(\mathbf{w}_2\) and so on. Let the position
of these components are given by the set
\(\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m\) respectively.
Further, the covariance between \(\mathbf{w}_j\) and \(\mathbf{z}_i\) is
non-zero only if \(\mathbf{z}_i\) is relevant for \(\mathbf{w}_j\). If
\(\sigma_{ij}\) be the covariance between \(\mathbf{w}_j\) and
\(\mathbf{z_i}\) then \(\sigma_{ij} \ne 0\) if \(i \in \mathcal{P}_j\)
where \(i = 1, \ldots, p\) and \(j = 1, \ldots, m\) and
\(\sigma_{ij} = 0\) otherwise.

In addition, the corresponding regression coefficient for
\(\mathbf{w}_j\) is,

\[
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i}\mathbf{t}_{ij},\qquad j = 1, 2, \ldots m
\]

where, \(\mathbf{t}_{ij}\) is a matrix with column vectors of 1's and
0's such that \(\mathbf{t}_{ij} = 1\) if the position relevant
components for \(\mathbf{w}_j\) in set \(\mathcal{P}_j\) and 0
otherwise.

The position of relevant components have heavy impact on prediction.
\citet{helland1994comparison} have shown that if relevant components
have large variance, prediction of \(\mathbf{Y}\) from \(\mathbf{X}\) is
relatively easy and if the variance of relevant components is small, the
prediction becomes difficult given that coefficient of determination and
other model parameters held constant. For example, if first and second
components of \(\mathbf{X}\) are relevant for \(\mathbf{Y}_1\) and fifth
and sixth componets are relevant for \(\mathbf{Y}_2\), it is relatively
easy to predict \(\mathbf{Y}_1\) than \(\mathbf{Y}_2\). Since, the first
and second principal components have larger variance than fifth and
sixth components.

Although the covariance matrix depends only on few relevant components,
we can not choose these covariances freely since we also need to satisfy
following two conditions:

\begin{itemize}
\tightlist
\item
  The covariance matrix must be positive definite
\item
  The covariance \(\sigma_{ij}\) must satisfy user defined coefficient
  of determination
\end{itemize}

We have the relation,
\[\boldsymbol{\rho}_W^2 = \boldsymbol{\Sigma}_{ZW}^t\boldsymbol{\Sigma}_{ZZ}^{-1}\boldsymbol{\Sigma}_{ZW}\boldsymbol{\Sigma}_{WW}^I\]
Applying our assumption for simulation,
\(\boldsymbol{\Sigma_{WW}} = \mathbf{I}_m\) and
\(\boldsymbol{\Sigma}_{ZZ} = \Lambda\), we obtain,

\[
\begin{aligned}
\boldsymbol{\rho}_W^2 &= \boldsymbol{\Sigma}_{ZW}^t \Lambda^{-1} \boldsymbol{\Sigma}_{ZW} \mathbf{I}_m \\
&= \begin{bmatrix}
\sum_{i = 1}^p \sigma_{i1}^2/\lambda_i          & \ldots & \sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i \\
\vdots                                          & \ddots & \vdots \\
\sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i & \ldots & \sum_{i = 1}^p \sigma_{im}^2/\lambda_i
\end{bmatrix}
\end{aligned}
\]

Furthermore, we assume that there are no overlapping relevant components
for any two \(\mathbf{W}\), i.e,
\(n\left(\mathcal{P}_j \cap \mathcal{P}_{j*}\right) = 0\) or
\(\sigma_{ij}\sigma_{ij*} = 0\) for \(j\ne j*\). The additional unknown
parameters in diagonal should agree with user specified coefficient of
determination for \(\mathbf{W}_j\). i.e, \(\rho_{wj}^2\) is,

\[
\rho_{wj}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i}
\]

Here, only the relevant components have non-zero covariances with
\(\mathbf{w}_j\), so,

\[
\rho_{wj}^2 = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}^2}{\lambda_i}
\]

For some user defined \(\rho_{jw}^2\), \(\sigma_{ij}^2\) determined as
follows,

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Sample \(k_j\) values from uniform distribution \(\mathcal{U}(-1, 1)\)
  distribution. Let them be,
  \(\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}\).
\item
  Define,
  \[\sigma_{ij} = \text{Sign}\left(\mathcal{S}_i\right)\sqrt{\frac{\rho_{wj}^2\left|\mathcal{S}_i\right|}{\sum_{k\in \mathcal{P}_j}\left|\mathcal{S}_k\right|} \lambda_i}\]
  for \(i \in \mathcal{P}_j\) and \(j = 1, \ldots, m\)
\end{enumerate}

\subsection{Data Simulation}\label{data-simulation}

After the construction of \(\boldsymbol{\Xi}_{WZ}\), \(n\) samples are
generated from standard normal distribution
of\(\left(\mathbf{W}, \mathbf{Z} \right)\) considering their mean to be
zero, i.e.\(\boldsymbol{\mu}_W = 0\) and\(\boldsymbol{\mu}_Z=0\).
Since\(\boldsymbol{\Xi}_{WZ}\) is positive
definite,\(\boldsymbol{\Xi}_{WZ}^{1/2}\) obtained from its Cholesky
decomposition, can serve as one of its square root. The simulation
process constitute of following steps,

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  A matrix \(\mathbf{U}_{n\times (p + q)}\) is sampled from standard
  normal distribution
\item
  Compute \(\mathbf{G} = \boldsymbol{U\Xi}_{WZ}^{1/2}\)
\end{enumerate}

Here, first \(m\) columns of \(\mathbf{G}\) will serve as \(\mathbf{W}\)
and remaining \(p\) columns will serve as \(\mathbf{Z}\). Further, each
row of \(\mathbf{G}\) will be a vector sampled independently from joint
normal distribution of \(\left(\mathbf{W}, \mathbf{Z}\right)\). The
final step to generate \(\mathbf{X}\) and \(\mathbf{Y}\) from
\(\mathbf{Z}\) and \(\mathbf{W}\) requires corresponding rotation
matrices which is discusses on following section.

\subsection{Rotation of predictor space}\label{rotation-predictor-space}

Simulation of predictor variables from principal components requires a
construction of a rotation matrix \(\mathbf{R}\) that defines a new
basis for the same space as is spanned by the principle components. As
any rotation matrix can be considered as \(\mathbf{R}\), an eigenvalue
matrix from eigenvalue decomposition of \(\boldsymbol{\Sigma}_{XX}\) can
be a candidate. Since simulation is a reverse engineering, the
underlying covariance structure for the predictors are unknown. So, the
method is free to construct a real valued orthogonal matrix that can
serve for the purpose.

Among several methods
\citep{anderson1987generation, heiberger1978algorithm} to generate
random orthogonal matrix the same method as is used in
\citet{saebo2015simrel} is implemented here. The \(\mathcal{Q}\) matrix
obtained from QR-decomposition of a matrix filled with standard normal
variates can serve as the rotation matrix \(\mathbf{R}\).

The rotation can be a) unrestricted and b) restricted. The former one
rotates all \(p\) predictors making them some what relevant for the all
response conponents and consequently all responses. However, only
\(q_i \le p\) predictors are relevant for for \(i^\text{th}\) response
component, the resticted rotation is implemented in \texttt{simrel-M}.
This also ensure that \(p-q_i\) predictors does not contribute anything
on response component \(i\) and consequently the simulated data can also
be used for testing variable selection methods.

\subsection{Rotation of response space}\label{rotation-response-space}

\texttt{Simrel-M} has considered an exclusive relevant predictor space
for each response components, i.e.~a set of predictor variables only
influence one response component. However, it allows user to simulate
more response variable than response components. In this case, noise are
added during the orthogonal rotation of response components. For
example, if user wants to simulation 5 response variation from 3
response components. Two standard normal vectors are combined with
response components and rotated simultaneously. The rotation can be both
restricted and unrestricted as discussed in previous section. The
restricted rotation is carried out combining response vectors along with
noise vector in a block-wise manner according to the users choice.
Illustration in fig-\ldots{}

Suppose, in our previous example, if response components are combined as
-- \(\mathbf{W}_1, \mathbf{W}_4\), \(\mathbf{W}_2\) and
\(\mathbf{W}_3, \mathbf{W}_5\). Here, any predictor variable is only
relevant for \(\mathbf{W}_1, \mathbf{W}_2\) and \(\mathbf{W}_3\) while
\(\mathbf{W}_4\) and \(\mathbf{W}_5\) are noise. The resulting response
variables are \(\mathbf{Y}_1 \ldots \mathbf{Y}_5\) where, the first and
fourth response variable spans the same space as by the first response
components \(\mathbf{W}_1\) and noise component \(\mathbf{W}_4\) and so
on. Thus, the predictors and predictor space relevant for response
component \(\mathbf{W}_1\) is also relevant for response
\(\mathbf{Y}_1\) and \(\mathbf{Y}_4\).

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}


\renewcommand\refname{References}
\bibliography{packages.bib,ref-db.bib}



\end{document}
