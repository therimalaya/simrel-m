\documentclass[12pt,A4paper,authoryear]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
%% Other Customizations
\usepackage[hyphens]{url}
\usepackage{lineno} % add
\usepackage{caption}

%% Use Landscape Pages
\usepackage{lscape}

\usepackage{setspace}
\setstretch{1.25}

\usepackage{mathpazo}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\biboptions{sort&compress} % For natbib
\usepackage{booktabs} % book-quality tables

% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \def\@oddfoot{\it \hfill\today}%
  \let\@evenfoot\@oddfoot}
\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath}

%% Multiple figure side-by-side
\usepackage{subfig}

\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[utf8]{inputenc}
\else % if luatex or xelatex
\usepackage{fontspec}
\ifxetex
\usepackage{xltxtra,xunicode}
\fi
\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
\newcommand{\euro}{€}
\setmonofont{Source Code Pro}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable}
\ifxetex
\usepackage[setpagesize=false, % page size defined by xetex
unicode=false, % unicode breaks when used with xetex
xetex]{hyperref}
\else
\usepackage[unicode=true]{hyperref}
\fi

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header


% \usepackage[nomarkers]{endfloat}

%% My customizations %%%%%%%%
%% Loading Packages
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{tabularx}
\usepackage[dvipsnames]{xcolor}
\usepackage{pgf, tikz}
\usepackage{amsthm}

%% Custom macros
\newtheorem{mydef}{Definition}
\newcommand{\bs}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
\newcommand{\seq}[3][1]{\ensuremath{#2_{#1},\ldots,\,#2_{#3}}}
\newcommand{\note}[1]{\marginpar{\scriptsize\tt{\color{RoyalBlue}#1}}}
\newcommand{\edit}[1]{{\color{OrangeRed} #1}}

%% Custom Lengths
\setlength{\parindent}{0pt}
\setlength{\parskip}{9pt}

%% Define Colors
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{YellowOrange}
\colorlet{myurlcolor}{Aquamarine}

%% Hyperlink Setup
\AtBeginDocument{%
  \hypersetup{breaklinks=true,
    bookmarks=true,
    pdfauthor={},
    pdftitle={simrel-m: A versatile tool for simulating multi-response linear model data},
    colorlinks=true,
    urlcolor=myurlcolor!\myshade!black,
    linkcolor=mylinkcolor!\myshade!black,
    citecolor=mycitecolor!\myshade!black,
    pdfborder={0 0 0}}
}

\urlstyle{same}  % don't use monospace font for urls

%% End my customizations %%%%%%%%%

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\begin{document}
\begin{frontmatter}

  \title{\texttt{simrel-m}: A versatile tool for simulating multi-response linear
model data}
    \author[]{Raju Rimal}
  
  
    \author[]{Trygve Almøy}
  
  
    \author[Norwegian University of Life Sciences]{Solve Sæbø\corref{c1}}
  
   \cortext[c1]{Dep. of Chemistry and Food Science, NMBU, Ås
(\href{http://nmbu.no}{nmbu.no})}
    
  \begin{abstract}
    \small
    Data science is generating enormous amounts of data, and new and
    advanced analytical methods are constantly being developed to cope with
    the challenge of extracting information from such ``big-data''.
    Researchers often use simulated data to assess and document the
    properties of these new methods, and in this paper we present
    \texttt{simrel-m}, which is a versatile and transparent tool for
    simulating linear model data with extensive range of adjustable
    properties. The method is based on the concept of relevant components
    \citet{helland1994comparison}, which is equivalent to the envelope model
    \citet{cook2013envelopes}. It is a multi-response extension of
    \texttt{simrel} \citet{saebo2015simrel}, and as \texttt{simrel} the new
    approach is essentially based on random rotations of latent relevant
    components to obtain a predictor matrix \(\mathbf{X}\), but in addition
    we introduce random rotations of latent components spanning a response
    space in order to obtain a multivariate response matrix \(\mathbf{Y}\).
    The properties of the linear relation between \(\mathbf{X}\) and
    \(\mathbf{Y}\) are defined by a small set of input parameters which
    allow versatile and adjustable simulations. Sub-space rotations also
    allow for generating data suitable for testing variable selection
    methods in multi-response settings. The method is implemented as an
    R-package which serves as an extension of the existing \texttt{simrel}
    packages \citet{saebo2015simrel}.
  \end{abstract}
   \begin{keyword} \footnotesize \texttt{simrel-2.0}, \texttt{simrel} package in r, data
simulation, linear model, \texttt{simrel-m} \sep \end{keyword}
\end{frontmatter}

\section{Introduction}\label{introduction}

Technological advancement has opened a door for complex and
sophisticated scientific experiments that was not possible before. Due
to this change, enormous amounts of raw data are generated which
contains massive information but difficult to excavate. Finding
information and performing scientific research on these raw data has now
become another problem. In order to tackle this situation new methods
are being developed. However, before implementing any method, it is
essential to test its performance. Often, researchers use simulated data
for the purpose which itself is a time-consuming process. The main focus
of this paper is to present a simulation method, along with an r-package
called \texttt{simrel-m}, that is versatile in nature and yet simple to
use.

The simulation method we are presenting here is based on the principal
of relevant space for prediction \citep{helland1994comparison} which
assumes that there exists a subspace in the complete space of response
variables that is spanned by a subset of eigenvectors of predictor
variables. The r-package based on this method lets user specify various
population properties such as which components of predictors
\((\mathbf{x})\) are relevant for a latent subspace of the responses
\(\mathbf{y}\) and collinearity structure of \(\mathbf{x}\). This
enables the possibility to construct data for evaluating estimation
methods and methods developed for variable selection.

Among several publications on simulation, \citet{ripley2009stochastic}
and \citet{gamerman2006markov} has exhaustively discussed the topic. In
addition, many publications have implemented simulated data in order to
investigate new estimation methods and prediction strategy
\citep[see:][]{cook2015simultaneous, cook2013envelopes, helland2012near}.
However, most of the simulations in these studies were developed to
address their specific problem. A systematic tool for simulating linear
model data with single response, which could serve as a general tool for
all such comparisons, was presented in \citet{saebo2015simrel} and as
the r-package \texttt{simrel}. This paper extends \texttt{simrel} in
order to simulate linear model data with multivariate response with an
r-package \texttt{simrel-m}.

\section{Statistical Model}\label{statistical-model}

Let us consider a model in
equation\textasciitilde{}\eqref{eq:rand-reg-model} as our point of
departure.

\begin{equation}
  \begin{bmatrix}\mathbf{y}\\ \mathbf{x}\end{bmatrix} \sim N
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
      \boldsymbol{\Sigma}_{xy} & \boldsymbol{\Sigma}_{xx}
    \end{bmatrix}
  \right)
  \label{eq:rand-reg-model}
\end{equation}

where, \(\mathbf{y}\) is a response vector with \(m\) response variables
\(y_1, y_2, \ldots y_m\) with mean vector of \(\boldsymbol{\mu}_y\) and
\(\mathbf{x}\) is vector of \(p\) predictor variables with mean vector
\(\boldsymbol{\mu}_x\). Further,

\begin{longtable}[]{@{}ll@{}}
\toprule
\(\boldsymbol{\Sigma}_{yy}\) & is variance-covariance matrix of
\(\mathbf{y}\)\tabularnewline
\(\boldsymbol{\Sigma}_{xx}\) & is variance-covariance matrix of
variables \(\mathbf{x}\)\tabularnewline
\(\boldsymbol{\Sigma}_{xy}\) & is matrix of covariance between
\(\mathbf{x}\) and \(\mathbf{y}\)\tabularnewline
\bottomrule
\end{longtable}

\addtocounter{table}{-1}

For model\textasciitilde{}\eqref{eq:rand-reg-model}, standard theory in
multivariate statistics may be used to show that \(\mathbf{y}\)
conditioned on \(\mathbf{x}\) corresponds to the linear model,

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\beta}^t (\mathbf{x} - \boldsymbol{\mu}_x) + \boldsymbol{\varepsilon}
  \label{eq:linear-model}
\end{equation}

where, \(\boldsymbol{\beta}^t\) is a matrix of regression coefficient
and \(\boldsymbol{\varepsilon}\) is error term such that
\(\boldsymbol{\varepsilon} \sim N\left(0, \boldsymbol{\Sigma}_{y|x}\right)\).
The properties of the linear model in
equation\textasciitilde{}\eqref{eq:linear-model} can be expressed in terms
of covariance matrices from
equation\textasciitilde{}\eqref{eq:rand-reg-model}.

\begin{description}
\tightlist
\item[Regression Coefficients]
\[ \boldsymbol{\beta} = \boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\]
\item[Coefficient of Determination \(\boldsymbol{\rho}_y^2\)]
The diagonal elements of coefficient of determination matrix
\(\boldsymbol{\rho}_y^2\) gives the amount of variation that
\(\mathbf{X}\) has explained about each \(\mathbf{Y}\).
\[\boldsymbol{\rho}_y^2 = \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}\]
\item[Conditional variance]
The conditional variance of \(\mathbf{y}\) given \(\mathbf{x}\) is,
\[\boldsymbol{\Sigma}_{y|x} = \boldsymbol{\Sigma}_{yy} - \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}.\]
The diagonal elements of this matrix equals the theoretical least square
error of prediction for each of the response variables.
\end{description}

Let us define a transformation of \(\mathbf{X}\) and \(\mathbf{Y}\) as,
\(\mathbf{z} = \mathbf{Rx}\) and \(\mathbf{w} = \mathbf{Qy}\). Here,
\(\mathbf{R}_{p\times p}\) and \(\mathbf{Q}_{m\times m}\) are rotation
matrices which rotates \(\mathbf{x}\) and \(\mathbf{y}\) giving
\(\mathbf{z}\) and \(\mathbf{w}\) respectively. The model in
equation\textasciitilde{}\eqref{eq:rand-reg-model} can be expressed with
these transformed variables as,

\begin{align}
  \begin{bmatrix}\mathbf{w} \\ 
  \mathbf{z}\end{bmatrix}  & \sim N \left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right) \\
  &= N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_w \\ \boldsymbol{\mu}_z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{ww} & \boldsymbol{\Sigma}_{wz} \\
      \boldsymbol{\Sigma}_{zw} & \boldsymbol{\Sigma}_{zz}
    \end{bmatrix} \right) \nonumber \\
  &= N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_y \\
      \boldsymbol{R\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{yy}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{yx}\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{xy}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{xx}\mathbf{R}^t
    \end{bmatrix}
  \right)
  \label{eq:model3}
\end{align}

In addition, a linear model relating \(\mathbf{w}\) and \(\mathbf{z}\)
can be written as,

\begin{equation}
\mathbf{w} =    \boldsymbol{\mu}_w + \boldsymbol{\alpha}^t \left(\mathbf{z} - \boldsymbol{\mu}_z\right) + \boldsymbol{\tau}
\label{eq:latent-model}
\end{equation}

where, \(\boldsymbol{\alpha}\) is regression coefficient for the
transformed model and
\(\boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{w|z}\right)\).
Further, if both \(\mathbf{Q}\) and \(\mathbf{R}\) are orthonormal
matrix such that \(\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_q\) and
\(\mathbf{R}^t\mathbf{R} = \mathbf{I}_p\), the inverse transformation
can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{yy} = \mathbf{Q}^t \boldsymbol{\Sigma}_{ww} \mathbf{Q} &
    \boldsymbol{\Sigma}_{yx} = \mathbf{Q}^t \boldsymbol{\Sigma}_{wz} \mathbf{R} \\
    \boldsymbol{\Sigma}_{xy} = \mathbf{R}^t \boldsymbol{\Sigma}_{zw} \mathbf{Q} &
    \boldsymbol{\Sigma}_{xx} = \mathbf{R}^t \boldsymbol{\Sigma}_{zz} \mathbf{R}
  \end{matrix}
  \label{eq:cov-yx-wz}
\end{equation}

Here, we can find a direct connection between different population
properties between \eqref{eq:linear-model} and \eqref{eq:latent-model}.

\begin{description}
\tightlist
\item[Regression Coefficients]
\[
  \begin{aligned}
  \boldsymbol{\alpha} &= \boldsymbol{\Sigma}_{wz} \boldsymbol{\Sigma}_{zz}^{-1}
  &&= \boldsymbol{Q\Sigma}_{YZ}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{xx}\mathbf{R}^t\right]^{-1} \\
  &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\right]\mathbf{R}^t
  &&= \mathbf{Q}\boldsymbol{\beta}\mathbf{R}^t
  \end{aligned}
  \]
\item[Error Variance]
Further, the noise variance of transformed
model\textasciitilde{}\eqref{eq:latent-model} is, \[
  \begin{aligned}
\boldsymbol{\Sigma}_{w|z}
&= \boldsymbol{Q\Sigma}_{yy}\mathbf{Q}^t -
  \boldsymbol{Q \Sigma}_{yx}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{xx}\boldsymbol{R}^t\right]^{-1}
  \boldsymbol{R\Sigma}_{xy}\mathbf{Q}^t \nonumber \\
&= \boldsymbol{Q\Sigma}_{yy}\mathbf{Q}^t - 
  \boldsymbol{Q \Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\mathbf{Q}^t \nonumber \\
&= \mathbf{Q}\left[\boldsymbol{\Sigma}_{yy} -
  \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\right]\mathbf{Q}^{t} \nonumber \\
&= \mathbf{Q} \boldsymbol{\Sigma}_{y|x}\mathbf{Q}^t
  \end{aligned}
  \]
\item[Coefficient of Determination]
The coefficient of determination for
model\textasciitilde{}\eqref{eq:latent-model} is, \[
  \begin{aligned}
\boldsymbol{\rho}^2_w &= \boldsymbol{\Sigma}_{wz} 
\boldsymbol{\Sigma}_{zz}^{-1} \boldsymbol{\Sigma}_{zw} 
\boldsymbol{\Sigma}_{ww}^{-1} \\
  &=\mathbf{Q}^t
  \boldsymbol{\Sigma}_{yx}\mathbf{R}^t \left(\mathbf{R}\boldsymbol{\Sigma}_{xx}\mathbf{R}^t\right)^{-1}
  \mathbf{R}\boldsymbol{\Sigma}_{xy}\mathbf{Q}^t \left(\mathbf{Q} \boldsymbol{\Sigma}_{yy}^{-1} \mathbf{Q}^t\right) \nonumber \\
  &=\mathbf{Q}^t\left[\boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}\right]\mathbf{Q} \\
  &= \mathbf{Q}\boldsymbol{\rho}_{Y}^2 \mathbf{Q}^t
  \end{aligned}
  \]
\end{description}

From eigenvalue decomposition principal, if
\(\boldsymbol{\Sigma}_{xx} = \mathbf{R}\boldsymbol{\Lambda}\mathbf{R}^t\)
and
\(\boldsymbol{\Sigma}_{yy} = \mathbf{Q}\boldsymbol{\Omega}\mathbf{Q}^t\)
then \(\mathbf{z}\) and \(\mathbf{w}\) can be interpreted as principal
components of \(\mathbf{x}\) and \(\mathbf{y}\) respectively. In this
paper, these principal components will be termed as \emph{predictor
components} and \emph{response components} respectively. Here,
\(\boldsymbol{\Sigma}\) and \(\boldsymbol{\Omega}\) are diagonal
matrices of eigenvalues of \(\boldsymbol{\Sigma}_{xx}\) and
\(\boldsymbol{\Sigma}_{yy}\) respectively.

\section{Relevant Components}\label{relevant-components}

Let us consider a single response linear model with \(p\) predictors.

\[y = \mu_y + \boldsymbol{\beta}^t\left(\mathbf{x} - \mu_x\right) + \epsilon\]

where, \(\epsilon \sim N(0, \sigma^2)\) and \(\mathbf{x}\) are random
and independent. Following the principal of relevant space and
irrelevant space which are discussed extensively in
\citet{helland1994comparison}, \citet{Helland_2000},
\citet{helland2012near}, \citet{cook2013envelopes},
\citet{saebo2015simrel} and \citet{helland2017}, we can assume that
there exists a subspace of the full predictor space which is relevant
for \(\mathbf{y}\). An orthogonal space to this space does not contain
any information about \(\mathbf{y}\) and is considered as irrelevant.
Here, the \(y-\)relevant subspace of \(\mathbf{x}\) is spanned by a
subset of eigenvectors of covariance matrix of \(\mathbf{x}\), i.e.
\(\boldsymbol{\Sigma}_{xx}\).

This concept can be extended to \(m\) responses so that the subspace of
\(\mathbf{x}\) is relevant for a subspace of \(\mathbf{y}\). This
corresponds to the concept of simultaneous envelope
\citep{cook2015simultaneous} where relevant (material) and irrelevant
(immaterial) space were discussed for both response and predictors.

\subsection{Model Parameterization}\label{model-parameterization}

In order to construct a fully specified covariance matrix of
\(\mathbf{z}\) and \(\mathbf{w}\) for model in
equation\textasciitilde{}\eqref{eq:model3}, we need to identify
\(1/2 (p+m)(p+m+1)\) unknown parameters. However, \(\Sigma_{zz}\) is a
diagonal matrix of eigenvalues of \(\Sigma_{xx}\), the number of
parameter reduced by \(1/2p(p-1)\) parameters. In addition, the
covariances of \(\mathbf{z}\) that are not relevant for \(\mathbf{w}\)
will be zero. This further reduces the unknown parameters. For the
purpose of this simulation, we implement some assumption to
re-parameterize and simplify the model parameters. This enables us to
construct a wide range of model properties from only few key parameters.

\begin{description}
\tightlist
\item[\textbf{Parameterization of \(\boldsymbol{\Sigma}_{zz}\)}]
If we consider the rotation matrix \(\mathbf{R}\) corresponds to the
eigenvectors of \(\boldsymbol{\Sigma}_{xx}\), then \(\mathbf{z}\)
becomes the set of principal components of \(\mathbf{x}\). In that case
\(\boldsymbol{\Sigma}_{zz}\) is a diagonal matrix with eigenvalues
\(\lambda_1, \ldots, \lambda_p\). Further, we adopt the following
parametric representation of these eigenvalues,
\[\lambda_j = e^{-\gamma(j - 1)}, \gamma >0 \text{ and } j = 1, 2, \ldots, p\]
Here as \(\gamma\) increases, the decline of eigenvalues becomes
steeper, hence the parameter \(\gamma\) controls the level of
multicollinearity in \(\mathbf{x}\).
\item[\textbf{Parameterization of \(\boldsymbol{\Sigma}_{ww}\)}]
Here, we assume that \(\mathbf{w}\)'s are independent unconditionally
equally mutinormal distributed with variance 1, hence
\(\boldsymbol{\Sigma}_{ww} = \mathbf{I}_m\).
\item[\textbf{Parameterization of \(\boldsymbol{\Sigma}_{zw}\)}]
After parameterization of \(\boldsymbol{\Sigma}_{zz}\) and
\(\boldsymbol{\Sigma}_{ww}\), we are left with \(m \times p\) number of
unknowns corresponding to \(\boldsymbol{\Sigma}_{zw}\). Some of the
elements of \(\boldsymbol{\Sigma}_{zw}\) may be equal to zero, which
implies that the given \(\mathbf{z}\) is irrelevant for the given
variable \(\mathbf{w}\). The non-zero elements define which of the
\(\mathbf{z}\) are relevant for the \(\mathbf{w}\). We typically refer
to the indices of these \(\mathbf{z}\) variables as the position of
relevant components. In order to re-parameterize this covariance matrix,
it is necessary to discuss the position of relevant components in
details.
\end{description}

\subsubsection{Position of relevant
components}\label{position-of-relevant-components}

Let \(k_1\) components be relevant for \(\mathbf{w}_1\), \(k_2\)
components be relevant for \(\mathbf{w}_2\) and so on. Let the positions
of these components be given by the index sets
\(\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m\) respectively.
Further, the covariance between \(\mathbf{w}_j\) and \(\mathbf{z}_i\) is
non-zero only if \(\mathbf{z}_i\) is relevant for \(\mathbf{w}_j\). If
\(\sigma_{ij}\) is the covariance between \(\mathbf{w}_j\) and
\(\mathbf{z_i}\) then \(\sigma_{ij} \ne 0\) if \(i \in \mathcal{P}_j\)
where \(i = 1, \ldots, p\) and \(j = 1, \ldots, m\) and
\(\sigma_{ij} = 0\) otherwise.

In addition, the true regression coefficients for \(w_j\)
(equation\textasciitilde{}\eqref{eq:latent-model}) is given by:

\[
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i},\qquad j = 1, 2, \ldots m
\]

The position of relevant components have heavy impact on prediction.
\citet{helland1994comparison} have shown that if relevant components
have large eigenvalues (variances), which here implies small index
values in \(\mathcal{P}_j\), prediction of \(\mathbf{y}\) from
\(\mathbf{x}\) is relatively easy and if the eigenvalues (variances) of
relevant components are small, the prediction becomes difficult given
that the coefficient of determination and other model parameters are
held constant. For example, if the first and second components,
\(\mathbf{z}_1\) and \(\mathbf{z}_2\), are relevant for \(\mathbf{w}_1\)
and fifth and sixth components, \(\mathbf{z}_5\) and \(\mathbf{z}_6\),
are relevant for \(\mathbf{w}_2\), it is relatively easier to predict
\(\mathbf{w}_1\) than \(\mathbf{w}_2\), other properties being similar.
This is so, because the first and second principal components have
larger variances than the fifth and sixth components.

Although the covariance matrix may depends on few relevant components,
we can not choose these covariances freely since we also need to satisfy
following two conditions:

\begin{itemize}
\tightlist
\item
  The covariance matrices \(\Sigma_{zz}\), \(\Sigma_{ww}\) and
  \(\Sigma\) must be positive definite
\item
  The covariance \(\sigma_{ij}\) must satisfy user defined coefficient
  of determination
\end{itemize}

We have the relation,
\[\boldsymbol{\rho}_w^2 = \boldsymbol{\Sigma}_{zw}^t\boldsymbol{\Sigma}_{zz}^{-1}\boldsymbol{\Sigma}_{zw}\boldsymbol{\Sigma}_{ww}^{-1}\]
Applying our above given assumptions that,
\(\boldsymbol{\Sigma_{ww}} = \mathbf{I}_m\) and
\(\boldsymbol{\Sigma}_{zz} = \Lambda\), we obtain,

\[
\begin{aligned}
\boldsymbol{\rho}_w^2 &= \boldsymbol{\Sigma}_{zw}^t \Lambda^{-1} \boldsymbol{\Sigma}_{zw} \mathbf{I}_m \\
&= \begin{bmatrix}
\sum_{i = 1}^p \sigma_{i1}^2/\lambda_i          & \ldots & \sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i \\
\vdots                                          & \ddots & \vdots \\
\sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i & \ldots & \sum_{i = 1}^p \sigma_{im}^2/\lambda_i
\end{bmatrix}
\end{aligned}
\]

Furthermore, we assume that there are no overlapping relevant components
for any two \(\mathbf{w}\), i.e,
\(n\left(\mathcal{P}_j \cap \mathcal{P}_{j*}\right) = 0\) or
\(\sigma_{ij}\sigma_{ij*} = 0\) for \(j\ne j*\). The additional unknown
parameters in diagonal of \(\boldsymbol{\rho}_w^2\) should agree with
user specified coefficient of determination for \(\mathbf{w}_j\). i.e,
\(\rho_{wj}^2\) is,

\[
\rho_{wj}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i}
\]

Here, only the relevant components have non-zero covariances with
\(\mathbf{w}_j\), so,

\[
\rho_{wj}^2 = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}^2}{\lambda_i}
\]

For some user defined \(\rho_{jw}^2\), \(\sigma_{ij}^2\) is determined
as follows,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample \(k_j\) values from a uniform distribution
  \(\mathcal{U}(-1, 1)\) distribution. Let them be,
  \(\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}\).
\item
  Define,
  \[\sigma_{ij} = \text{Sign}\left(\mathcal{S}_i\right)\sqrt{\frac{\rho_{wj}^2\left|\mathcal{S}_i\right|}{\sum_{k\in \mathcal{P}_j}\left|\mathcal{S}_k\right|} \lambda_i}\]
  for \(i \in \mathcal{P}_j\) and \(j = 1, \ldots, m\)
\end{enumerate}

\subsubsection{Data Simulation}\label{data-simulation}

From the above given parameterizations and the user defined choices of
model parameters, a fully defined and known covariance matrix
\(\boldsymbol{\Sigma}\) of \((\mathbf{w, z})\) is given. For the
simulation of a single observation of \((\mathbf{w, z})\) let us define
\(\mathbf{g} = \boldsymbol{\Sigma}^{-1/2}\mathbf{u}\) such that
\(\text{cov}(\mathbf{g}) = \boldsymbol{\Sigma}\). Here
\(\boldsymbol{\Sigma}^{-1/2}\) is obtained from Choleskey decomposition
and serves as one of the square root of positive definite matrix
\(\boldsymbol{\Sigma}\) and \(\mathbf{u}\) is simulated from standard
normal distribution and has covariance \(\text{cov}(u) = \mathbf{I}\).

Similarly, in order to simulate \(n\) observations, we define
\(\underset{n \times (m + p)}{\mathbf{G}} = \mathbf{U}\boldsymbol{\Sigma}^{-1/2}\)
such that \(\text{cov}(\mathbf{G}) = \boldsymbol{\Sigma}\). Here the
first \(m\) columns of \(\mathbf{G}\) will serve as \(\mathbf{W}\) and
remaining \(p\) columns will serve as \(\mathbf{Z}\). Further, each row
of \(\mathbf{G}\) will be a vector sampled independently from joint
normal distribution of \(\left(\mathbf{w}, \mathbf{z}\right)\). Finally,
these simulated matrices \(\mathbf{W}\) and \(\mathbf{Z}\) are
orthogonally rotated in order to obtain \(\mathbf{Y}\) and
\(\mathbf{X}\) respectively. Following section discuss about these
rotation matrices in details.

\subsection{Rotation of predictor
space}\label{rotation-of-predictor-space}

In order to make comments on predictor space, let us consider an example
where a regression model with \(p = 10\) predictors \((\mathbf{x})\) and
\(m = 4\) responses \((\mathbf{y})\). Let's assume that only three
principal components \((w_1, w_2\) and \(w_3)\) are needed to describe
all four response variables. Further, let the index sets
\(\mathcal{P}_1 = \{1, 2\}, \mathcal{P}_2 = \{3, 4\}\) and
\(\mathcal{P}_3 = \{5, 6\}\) define the position of the principal
components of \(\mathbf{x}\) that are relevant for \(w_1, w_2\) and
\(w_3\) respectively. Let \(\mathcal{S}_1\), \(\mathcal{S}_2\) and
\(\mathcal{S}_3\) be the orthogonal spaces spanned by each set of
principal components. These spaces together span
\(\mathcal{S}_k = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3\)
which is the minimum relevant space and equivalent to the x-envelope as
discussed by \citet{cook2013envelopes}.

Moreover, let \(q_1 = 3, q_2 = 3\) and \(q_3 = 2\) be the number of
predictor variables we want to be relevant for \(w_1, w_2\) and \(w_3\)
respectively. Then \(q_1 = 3\) predictors may be obtained by rotating
the principal components in \(\mathcal{P}_1\) along with one more
irrelevant principal component. Similarly, \(q_2 = 3\) predictors,
relevant for \(w_2\), can be obtained by rotating principal components
in \(\mathcal{P}_2\) along with one more irrelevant component and
\(q_3 = 2\) predictors, relevant for \(w_3\), can be obtained by
rotating principal components in \(\mathcal{P}_3\) without any
additional irrelevant component. Let the space spanned by the
\(q_1, q_2\) and \(q_3\) number of predictors be \(\mathcal{S}_{q_1}\),
\(\mathcal{S}_{q_2}\) and \(\mathcal{S}_{q_3}\). Together they span a
space
\(\mathcal{S}_q = \mathcal{S}_{q_1} \oplus \mathcal{S}_{q_2} \oplus \mathcal{S}_{q_3}\).
This space is bigger than \(\mathcal{S}_k\). Here, \(\mathcal{S}_k\) is
orthogonal to \(\mathcal{S}_{p - k}\) and \(\mathcal{S}_q\) is
orthogonal to \(\mathcal{S}_{p - q}\). Generally speaking, here we are
splitting complete variable space \(\mathcal{S}_p\) into two orthogonal
space -- \(\mathcal{S}_k\) relevant for \(\mathbf{y}\) and
\(\mathcal{S}_{p - k}\) irrelevant for \(\mathbf{y}\).

In the previous section, we discussed about constructing covariance
matrix of latent structure.
Figure\textasciitilde{}\ref{fig:cov-plot-print} (left) shows a similar
structure resembling the example here. The three colors represents their
relevance with the three latent response components \((w_1, w_2\) and
\(w_3)\). Here we can see that \(z_{1}\) and \(z_{2}\) (first and second
principal components of \(\mathbf{x}\)) have non-zero covariance with
\(w_1\) (first latent component of response \(\mathbf{y}\)). In the
similar manner other non-zero covariances are self-explanatory.

\begin{figure}
\subfloat[Relevant Position\label{fig:cov-plot-print1}]{\includegraphics[width=0.33\linewidth]{Simrel-M_files/figure-latex/cov-plot-print-1} }\subfloat[Rotation Matrix\label{fig:cov-plot-print2}]{\includegraphics[width=0.33\linewidth]{Simrel-M_files/figure-latex/cov-plot-print-2} }\subfloat[Relevant Predictors\label{fig:cov-plot-print3}]{\includegraphics[width=0.33\linewidth]{Simrel-M_files/figure-latex/cov-plot-print-3} }\caption{Simulation of predictor and response variables after orthogonal transformation of principal components by a rotation matrix}\label{fig:cov-plot-print}
\end{figure}

In order to simulate predictor variables \((\mathbf{x})\), we construct
matrix \(\mathbf{R}\) which then is used for orthogonal rotation of
principal components \(\mathbf{z}\). This defines a new basis for the
same space as is spanned by the principal components. In principal,
there are many possible options for a rotation matrix. Among them, the
eigenvector matrix of \(\boldsymbol{\Sigma}_{xx}\) can be a candidate.
However, in this reverse engineering both rotation matrices
\(\mathbf{R}\) and \(\mathbf{Q}\) along with the covariance matrices
\(\boldsymbol{\Sigma}_{xx}\) are unknown. So, we are free to choose any
\(\mathbf{R}\) that satisfied the properties of a real valued rotation
matrix, i.e \(\mathbf{R}^{-1} = \mathbf{R}^t\) so that \(\mathbf{R}\) is
orthonormal and its determinant becomes \(\pm 1\). Here the rotation
matrix \(\mathbf{R}\) should be block diagonal as in
figure\textasciitilde{}\ref{fig:cov-plot-print} (middle) in order to
rotate spaces \(\mathcal{S}_1, \mathcal{S}_2 \ldots\) separately.
Figure\textasciitilde{}\ref{fig:simulated-data} (left) shows the
simulated principal components \(\mathbf{z}\) that we are following in
our example where we can see that the principal component \(z_{1}\) and
\(z_{2}\) relevant for \(w_1\) is getting rotated together with an
irrelevant component \(z_{8}\). The resultant predictors
(Figure\textasciitilde{}\ref{fig:simulated-data}, right)
\(x_{1}, x_{2}\) and \(x_{8}\) will also be relevant for \(w_1\). In the
figure, we can see that principal components \(x_{7}, x_{8}, x_{9}\) and
\(x_{10}\) are not relevant for any responses before rotation however
\(x_{8}, x_{10}\) predictors becomes relevant after rotation keeping
\(x_{7}\) and \(x_{9}\) still irrelevant.

\begin{figure}
\includegraphics[width=1\linewidth]{Simrel-M_files/figure-latex/simulated-data-1} \caption{Simulated Data before (left) and after (right) rotation}\label{fig:simulated-data}
\end{figure}

Among several methods
\citep{anderson1987generation, heiberger1978algorithm} for generating
random orthogonal matrix, in this paper we are using orthogonal matrix
\(\mathcal{Q}\) obtained from QR-decomposition of a matrix filled with
standard normal variates. The rotation here can be a) restricted and b)
unrestricted. The latter rotates all principal components \(\mathbf{z}\)
together and makes all predictor variables somewhat relevant for all
response variables. However, the former one performs a block-wise
rotation so that it rotates certain selected principal components
together. This gives control for specifying certain predictors as
relevant for selected responses, which was discussed in our example
above. This also allows us to simulate irrelevant predictors such as
\(x_{7}\) and \(x_{9}\) which can be detected during variables selection
procedures.

\hypertarget{rotation-of-response-space}{\subsection{Rotation of
response space}\label{rotation-of-response-space}}

The previous example has four response variables with only three
informative principal components \(w_1, w_2\) and \(w_3\). During the
rotation of covariance matrix \(\boldsymbol{\Sigma}\), the response
space is also rotated separately along with the predictor space.
Figure\textasciitilde{}\ref{fig:cov-plot-print} shows that the
informative response component \(w_3\) is rotated together with
uninformative response component \(w_4\) so that the predictors which
were relevant for \(w_3\) will be relevant for response variables
\(y_3\) and \(y_4\). Similarly, response components \(w_1\) and \(w_2\)
are rotated separately so that predictors relevant for \(w_1\) and
\(w_2\) will also be relevant for \(y_1\) and \(y_2\) respectively which
we can see in Figure\textasciitilde{}\ref{fig:simulated-data}. In the
r-package \emph{simrel-m}, the combining of the response components is
specified by a parameter \texttt{ypos}.

\hypertarget{implementation}{\section{Implementation}\label{implementation}}

This section demonstrates an application of \texttt{simrel-m} in order
to compare different estimation methods on the basis of prediction
error. For the comparison, we have considered four well established
estimation methods.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Ordinary Least Squares (OLS),
\item
  Principal Component Regression (PCR),
\item
  Partial Least Squares predicting individual response variable
  separately (PLS1) and
\item
  Partial Least Squares predicting all response variables together
  (PLS).
\end{enumerate}

We have also considered four relatively new estimation methods

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Canonically Powered Partial Least Squares regression (CPPLS)
  \citep{indahl2009canonical},
\item
  Canonical Partial Least Squares regression (CPLS)
  \citep{indahl2009canonical},
\item
  Envelope estimation in predictor space (xenv)
  \citep{cook2010envelope},
\item
  Envelope estimation in response space (yenv)
  \citep{cook2015foundations} and
\item
  Simultaneous estimation of x- and y-envelope (senv)
  \citep{cook2015simultaneous}
\end{enumerate}

From the possible combinations of two levels of coefficient of
determination \((R^2)\) and two levels of \texttt{gamma} (factor that
controls the multicollinearity in predictor variable), four simulation
designs (design 1, design 2, design 3, design 4) are prepared.
Replicating each design 20 times, 80 datasets with five response
variables \((m=5)\) and 16 predictor variables \((p = 16)\) are
simulated using the method discussed in this paper. It is also assumed
that three principle components of response variables (\(w_1, w_2\) and
\(w_3\)) completely describes the variation present in five response
variables (\(y_1 \ldots y_5\)). The four designs are presented in the
Table\textasciitilde{}\ref{tab:parameter-settings}. All datasets
contains 100 sample observations and out of 16 predictor variables,
three disjoint set of five predictor variables are relevant for response
components \(w_1, w_2\) and \(w_3\). Further, predictor components
\(z_1\) and \(z_6\) are relevant for response component \(w_1\),
predictor components \(z_2\) and \(z_5\) are relevant for response
component \(w_2\) and predictor component \(z_3\) is relevant for
response component \(w_3\). In addition, following the discussion about
\protect\hyperlink{rotation-of-response-space}{rotation of response
space}, \(w_1\) is rotated together with \(w_4\) and \(w_2\) is rotated
together with \(w_5\).

\begin{longtable}[]{@{}rrrrr@{}}
\caption{\label{tab:parameter-settings} Parameter setting of simulated data
for model comparison}\tabularnewline
\toprule
\begin{minipage}[b]{0.36\columnwidth}\raggedleft\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design1\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design2\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design3\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design4\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.36\columnwidth}\raggedleft\strut
~\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design1\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design2\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design3\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Design4\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.36\columnwidth}\raggedleft\strut
\textbf{Decay of eigenvalue \((\gamma)\)}\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.2\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.8\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.2\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.8\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\raggedleft\strut
\textbf{Coef. of Determination \((\rho^2)\)}\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.8, 0.8, 0.4\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.8, 0.8, 0.4\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.4, 0.4, 0.4\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
0.4, 0.4, 0.4\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

For each method, an estimate of test prediction error is computed as,

\[\underset{m \times m}{\boldsymbol{\alpha}} = 
\left(
\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}
\right) ^t \boldsymbol{\Sigma}_{xx}
\left(
\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}
\right) + \boldsymbol{\Sigma}_{Y|X}\]

where, \(\hat{\boldsymbol{\beta}}\) is an estimate of true regression
coefficient \(\boldsymbol{\beta}\) and \(\boldsymbol{\Sigma}_{xx}\) is
the true covariance structure of predictor variable obtained from
\texttt{simrel-m}. Also, \(\boldsymbol{\sigma}^2\) is the true minimum
error of the model. Here \(\hat{\beta}\) vary accross different
estimation methods while the remaining terms are same for each dataset
design. Further, an overall prediction error of all responses is
measured by the trace of \(\boldsymbol{\alpha}\).

The minimimum prediction error (measured as discussed above) for nine
estimation methods averaged over 20 replications of four designs are in
Table\textasciitilde{}\ref{tab:min-error}. The table also shows that the
number of components a method has used in order to obtain the minimum of
average prediciton error.

\begin{longtable}[]{@{}lcccc@{}}
\caption{\label{tab:min-error} Minimum average prediction error (number of
components corresponding to minimum prediction error, minimum prediction
error)}\tabularnewline
\toprule
\begin{minipage}[b]{0.09\columnwidth}\raggedright\strut
Model\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 1\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 2\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 3\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 4\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.09\columnwidth}\raggedright\strut
Model\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 1\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 2\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 3\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering\strut
Design: 4\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{CPLS}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 3.24)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(4,\ 3.22)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.09)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.05)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{CPPLS}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 3.21)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 3.17)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.11)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\textbf{\emph{\texttt{(3,\ 4.04)}}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{OLS}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(1,\ 3.6)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(1,\ 3.58)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(1,\ 4.57)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(1,\ 4.5)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{PCR}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(7,\ 3.28)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(6,\ 3.19)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\textbf{\emph{\texttt{(6,\ 4.08)}}}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(6,\ 4.04)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{PLS}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(5,\ 3.29)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(6,\ 3.19)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.11)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(6,\ 4.06)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{PLS1}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(2,\ 3.32)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(5,\ 3.2)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(1,\ 4.16)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(5,\ 4.07)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{Senv}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\textbf{\emph{\texttt{(4,\ 3.17)}}}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\textbf{\emph{\texttt{(5,\ 3.14)}}}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.35)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(5,\ 4.28)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{Xenv}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(5,\ 3.23)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(6,\ 3.2)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(5,\ 4.1)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(6,\ 4.11)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.09\columnwidth}\raggedright\strut
\emph{Yenv}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 3.24)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 3.23)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.29)}\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering\strut
\texttt{(3,\ 4.24)}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Table\textasciitilde{}\ref{tab:min-error} shows that simulteneous
envelope has prediction error of 3.17 and 3.14 in
design\textasciitilde{}1 (with 4 components) and
design\textasciitilde{}2 (with 5 components) respectively which is
smaller than other methods. However the method was not able to show the
same performance in design\textasciitilde{}3 and
design\textasciitilde{}4. PCR model has least prediction error (4.08)
from 6 components in design 3 and Cannonically Powered PLS has minimum
prediction error (4.04) from 3 components in design 4. In design 3, we
can also see that the Canonical PLS method has second best performance
with only three components. The number of components vary accross
different replicated dataset but the component corresponding to minimum
prediction error is discussed here. A detail picture of prediction error
for each estimation method obtained for each additional component is
shown in Figure\textasciitilde{}\ref{fig:Average-Prediction-Plot}.
Although design 2 and design 4 has higher level of multicollinearity,
the performance of the estimation methods is indifferent to its effect.
Since all the methods, except OLS, are based on shrinking of estimates,
they are less influenced by multicollinearity problem.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Simrel-M_files/figure-latex/Average-Prediction-Plot-1} \caption{Minimum of Average Prediction Error}\label{fig:Average-Prediction-Plot}
\end{figure}

Above analysis has answered some questions such as how methods works
when there exist a true reduced dimension in response space but also
arised question like why they perform differently. For example, what is
the reason for the decreasing performance of the simultaneous envelope
method as the \(\rho^2\) values are reduced? Does this depend on the
dimensions and spheric shape of the \(\mathbf{y}\) envelopes? Since, the
example is intended as a demonstration of how \texttt{simrel-m} can be
used in scientific study, and a more elaborative study would be
necessary in order to answer such questions, but for this purpose
\texttt{simrel-m} would be a powerful tool.

\section{Web Interface}\label{web-interface}

In order to give an alternative interface for \texttt{simrel-m}, we have
created a shiny app which allows users to input the simulation
parameters through different input fields.
Figure\textasciitilde{}\ref{fig:AppSimulatr} shows a screenshot for the
application. The application contains three main sections through which
the user can interact with this simulation approach. A random seed can
be selected using section in
Figure\textasciitilde{}\ref{fig:AppSimulatr} (a) so that a particular
set of data can be simulated.
Figure\textasciitilde{}\ref{fig:AppSimulatr} (b) has all the input panel
where parameter for simulation can be entered. Here the user also has
the option to simulate univariate (uses \texttt{simrel} package in
CRAN), bivariate (not yet available in CRAN) and multivariate simulation
(\texttt{simrel-m}). In addition, a simulated R-object is comprised by
the simulated data can be download as \texttt{Rdata} format (section (e)
in Figure\textasciitilde{}\ref{fig:AppSimulatr}). The object constitute
of the simulated data along with other properties such as coefficient of
determination for each response, true regression coefficients and
rotation matrices.

All \texttt{simrel-m} parameters can be entered using simple user
interface where a vectors are separated with comma(,) and lists are
separated with semicolon(;). For instance, the relevant position
discussed in \protect\hyperlink{implementation}{implementation} section
of this paper can be entered as \texttt{1,\ 6;\ 2,\ 5;\ 3,\ 4} which is
equivalent to R syntax \texttt{list(c(1,\ 6),\ c(2,\ 5),\ c(3,\ 4))}.

\begin{figure}[!htb]

{\centering \includegraphics[width=0.95\linewidth]{/Users/rajurim/Dropbox/Papers/01-simrel-m/images/screenshots/AppSimulatr} 

}

\caption{Application interface of `simulatr`. (a) Seed and simulation button (b) Parameter control panel (c) Properties of simulated data (d) Additional analysis (e) Download option of simulated data}\label{fig:AppSimulatr}
\end{figure}

The application not only allows users to simulate data but also gives
some insight on simulated data. The example used in the screenshot has
simulated 200 training and 50 test samples with 15 predictor and 4
response variables. There are two latent variables (response component)
that completely spans the informative response space. Five predictor
variables are relevant for the first response component and explains 80
percent of the variation. In addition, the first and second predictor
components spans the same space spanned by these five relevant
predictors. Similarly, another set of four predictor variables are
relevant for second response component and explains 70 percent of the
variation. Here, third, fourth and sixth predictor components spans the
same space spanned by these four relevant predictors. Further, the first
response component is rotated together with a normally distributed
random vector to obtain first and third response variable and second
resposne components is rotated together with another normally
distributed random vector to obtain second and fourth response variable.

Section (c) in Figure\textasciitilde{}\ref{fig:AppSimulatr} contains
three plots -- a) regression coefficients b) relevant components and c)
estimated relevant components. In the first plot we can see that
predictor variables (1, 2, 8, 9 and 13) are relevant for first and third
response variable and predictor variables (3, 4, 6 and 15) are relevant
for second and fourth response variable. The second plot shows
covariance between response components and the predictor components
along with the corresponding eigenvalues in the background (bar plot).
As in our parameter setting, the plot shows that first and second
predictor components have non-zero covariance with first response
component and third, fourth and sixth predictor components have non-zero
covariance with second response component. The third plot is the
estimated covariance between predictor components with response
variable, for the simulated data. Since the first and third response
components are rotated together, in the plot, the covariance between
predictor components and first and third response variable is following
similar pattern. This also suggests that the predictor components which
were relevant for first response components gets relevant for first and
third response variables after rotation.

Along with these main sections, section (d) in the figure contains
additional analysis performed with the simulated data such as its
estimation with different methods. This section, which is still under
development, is intended for educational purposes to show how changing
the data properties influences the performances of different estimation
and prediction methods.

Many scientific studies
\citep{helland2012near, saebo2008lpls, cook2015simultaneous} are using
simulated data in order to compare their findings with others or assess
its properties. In many of these situations, a user-friendly and
versitle simulation tool like \texttt{simrel-m} can play an important
role. \citet{gangsei2016theoretical}; \citet{gangsei2016linear}; and
\citet{saebo2015simrel} are some examples where the univariate and
bivariate form of \texttt{simrel} have been used for their study.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}


\renewcommand\refname{References}
\bibliography{packages.bib,ref-db.bib}



\end{document}
