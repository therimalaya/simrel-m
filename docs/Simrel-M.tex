\documentclass[12pt,A4paper,authoryear]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
%% Other Customizations
\usepackage[hyphens]{url}
\usepackage{lineno} % add
\usepackage{caption}


\usepackage{setspace}
\setstretch{1.25}

\usepackage{mathpazo}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\biboptions{sort&compress} % For natbib
\usepackage{booktabs} % book-quality tables

% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \def\@oddfoot{\it \hfill\today}%
  \let\@evenfoot\@oddfoot}
\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath}

\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[utf8]{inputenc}
\else % if luatex or xelatex
\usepackage{fontspec}
\ifxetex
\usepackage{xltxtra,xunicode}
\fi
\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
\newcommand{\euro}{€}
\setmonofont{Source Code Pro}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable}
\ifxetex
\usepackage[setpagesize=false, % page size defined by xetex
unicode=false, % unicode breaks when used with xetex
xetex]{hyperref}
\else
\usepackage[unicode=true]{hyperref}
\fi

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header


% \usepackage[nomarkers]{endfloat}

%% My customizations %%%%%%%%
%% Loading Packages
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{tabularx}
\usepackage[dvipsnames]{xcolor}
\usepackage{pgf, tikz}
\usepackage{amsthm}

%% Custom macros
\newtheorem{mydef}{Definition}
\newcommand{\bs}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
\newcommand{\seq}[3][1]{\ensuremath{#2_{#1},\ldots,\,#2_{#3}}}
\newcommand{\note}[1]{\marginpar{\scriptsize\tt{\color{RoyalBlue}#1}}}
\newcommand{\edit}[1]{{\color{OrangeRed} #1}}

%% Custom Lengths
\setlength{\parindent}{0pt}
\setlength{\parskip}{9pt}

%% Define Colors
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{YellowOrange}
\colorlet{myurlcolor}{Aquamarine}

%% Hyperlink Setup
\AtBeginDocument{%
  \hypersetup{breaklinks=true,
    bookmarks=true,
    pdfauthor={},
    pdftitle={simrel-m: A versatile tool for simulating multi-response linear model data},
    colorlinks=true,
    urlcolor=myurlcolor!\myshade!black,
    linkcolor=mylinkcolor!\myshade!black,
    citecolor=mycitecolor!\myshade!black,
    pdfborder={0 0 0}}
}

\urlstyle{same}  % don't use monospace font for urls

%% End my customizations %%%%%%%%%

\begin{document}
\begin{frontmatter}

  \title{\texttt{simrel-m}: A versatile tool for simulating multi-response linear
model data}
    \author[]{Raju Rimal}
  
  
    \author[]{Trygve Almøy}
  
  
    \author[Norwegian University of Life Sciences]{Solve Sæbø\corref{c1}}
  
   \cortext[c1]{Dep. of Chemistry and Food Science, NMBU, Ås
(\href{http://nmbu.no}{nmbu.no})}
    
  \begin{abstract}
    \small
    Data science is generating enormous amounts of data, and new and
    advanced analytical methods are constantly being developed to cope with
    the challenge of extracting information from such ``big-data''.
    Researchers often use simulated data to assess and document the
    properties of these new methods, and in this paper we present
    \texttt{simrel-m}, which is a versatile and transparent tool for
    simulating linear model data with extensive range of adjustable
    properties. The method is based on the concept of relevant components
    \citet{helland1994comparison}, which is equivalent to the envelope model
    \citet{cook2013envelopes}. It is a multi-response extension of
    \texttt{simrel} \citet{saebo2015simrel}, and as \texttt{simrel} the new
    approach is essentially based on random rotations of latent relevant
    components to obtain a predictor matrix \(\mathbf{X}\), but in addition
    we introduce random rotations of latent components spanning a response
    space in order to obtain a multivariate response matrix \(\mathbf{Y}\).
    The properties of the linear relation between \(\mathbf{X}\) and
    \(\mathbf{Y}\) are defined by a small set of input parameters which
    allow versatile and adjustable simulations. Sub-space rotations also
    allow for generating data suitable for testing variable selection
    methods in multi-response settings. The method is implemented as an
    R-package which serves as an extension of the existing \texttt{simrel}
    packages \citet{saebo2015simrel}.
  \end{abstract}
   \begin{keyword} \footnotesize \texttt{simrel-2.0}, \texttt{simrel} package in r, data
simulation, linear model, \texttt{simrel-m} \sep \end{keyword}
\end{frontmatter}

\section{Introduction}\label{introduction}

Technological advancement has opened a door for complex and
sophisticated scientific experiments that was not possible before. Due
to this change, enormous amounts of raw data are generated which
contains massive information but difficult to excavate. Finding
information and performing scientific research on these raw data has now
become another problem. In order to tackle this situation new methods
are being developed. However, before implementing any method, it is
essential to test its performance. Often, researchers use simulated data
for the purpose which itself is a time-consuming process. The main focus
of this paper is to present a simulation method, along with an r-package
called \texttt{simrel-m}, that is versatile in nature and yet simple to
use.

The simulation method we are discussing here is based on principal of
relevant space for prediction \citep{helland1994comparison} which
assumes that there exists a subspace in the complete space of response
variables that is spanned by a subset of eigenvectors of predictor
variables. The r-package based on this method lets user to specify
various population properties such as which components of predictors
\((\mathbf{X})\) are relevant for a component of responses
\(\mathbf{Y}\) and how the eigenvalues of \(\mathbf{X}\) decreases. This
enables the possibility to construct data for evaluating estimation
methods and methods developed for variable selection.

Among several literatures in simulation
({\color{red}\texttt{which\ literatures}}), \citet{ripley2009stochastic}
has exhaustively discussed the topic. In addition, many literatures
({\color{red}\texttt{which\ literatures}}) are available on studies
which has implemented simulated data in order to investigate new
estimation methods and prediction strategy
\citep[see:][]{cook2015simultaneous, cook2013envelopes, helland2012near}.
However, most of the simulations in these studies is developed to
address their specific problem. A systematic tool for simulating linear
model data with single response, which could serve as a general tool for
all such comparisons, was presented in \citet{saebo2015simrel} and as
r-package \texttt{simrel}. This paper extends \texttt{simrel} in order
to simulate linear model data with multivariate response with an
r-package \texttt{simrel-m}.

The r-package \texttt{simrel-m} uses model parameterization which is
based on the concept of relevant components
\citet{helland1994comparison} where it is assumed that a subspace of
response \(\mathbf{Y}\) is spanned by a subset of eigenvectors
corresponding to predictor space. A response space can be thought to
have two mutually orthogonal space -- relevant and irrelevant. Here the
space of response matrix for which the predictors are relevant is termed
as response components, and we assume that each response component is
spanned by an exclusive subset of predictor variables. In this way we
can construct a set of predictor variables which has non-zero regression
coefficients. This also enables user to have uninformative predictors
which can be detected during variable selection procedure. In addition,
user can control signal-to-noise ratio for each response components with
a vector of population coefficient of determination
\(\boldsymbol{\rho}^2\). Further, the collinearity between predictor
variables can also be adjusted by a factor \(\gamma\) which controls
decay factor of eigenvalue of \(\mathbf{X}\) matrix.
\citet{helland1994comparison} showed that if the direction of large
variability (i.e., component corresponding to large eigenvalues) are
also relevant relevant predictor space, prediction is relatively easy.
In contrast, if the relevant predictors are on the direction of low
variability, prediction becomes difficult.
Table\textasciitilde{}\ref{tab:parameters} shows all the parameters that
a user can specify in \texttt{simrel-m}.

\begin{longtable}[]{@{}cl@{}}
\caption{\label{tab:parameters} Parameters for simulation used in this
study}\tabularnewline
\toprule
\begin{minipage}[b]{0.19\columnwidth}\centering\strut
Parameters\strut
\end{minipage} & \begin{minipage}[b]{0.75\columnwidth}\raggedright\strut
Description\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.19\columnwidth}\centering\strut
Parameters\strut
\end{minipage} & \begin{minipage}[b]{0.75\columnwidth}\raggedright\strut
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(n\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
number of observations\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(p\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
number of predictor variables\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(q\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
numbers of relevant predictors for each latent component of response
variables\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(l\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
number of informative latent component of response variables (response
components)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(m\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
number of response variables\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(\gamma\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
degree of collinearity (factor that control the decrease of eigenvalue
of \(\mathbf{X}\))\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(\mathcal{P}\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
position index of relevant predictor components for each response
components\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(\mathcal{S}\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
position index of response components to combine relevant and irrelevant
response variable\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\centering\strut
\(\boldsymbol{\rho}^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.75\columnwidth}\raggedright\strut
population coefficient determination for each response components\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Based on random regression model in
equation\textasciitilde{}\eqref{eq:model1}, we discuss some of these
parameters in details.

Out of \(p\) predictor variables only \(q\) of them are relevant and has
non-zero regression coefficients. If
\(\boldsymbol{e}_i, \; i = 1, 2, \ldots, p\) be the eigenvectors
corresponding to \(\mathbf{X}\), then, for some vector of
\(\boldsymbol{\eta}_j, j = 1, \ldots, m\) for \(m\) response variables,

\begin{align}
\mathbf{B} = \left(\beta_{ij}\right) &= 
  \begin{bmatrix} 
    \boldsymbol{\eta}_{11} & \ldots & \boldsymbol{\eta}_{1p} \\
    \vdots                 & \ddots & \vdots \\
    \boldsymbol{\eta}_{m1} & \ldots & \boldsymbol{\eta}_{mp}
  \end{bmatrix} 
  \begin{bmatrix} 
    \mathbf{e}_{11} & \vdots & \mathbf{e}_{1p} \\
    \vdots          & \ddots & \vdots \\
    \mathbf{e}_{p1} & \vdots & \mathbf{e}_{pp}
  \end{bmatrix} \\ &= 
  \begin{bmatrix}\boldsymbol{\eta}_{1} \\ \vdots \\ \boldsymbol{\eta}_{m}\end{bmatrix}_{m \times p}
  \begin{bmatrix}\mathbf{e}_{1} & \ldots & \mathbf{e}_{p}\end{bmatrix}_{p \times p} 
\label{eq:beta-eigenvector-relation}
\end{align}

The number of terms in
equation\textasciitilde{}\eqref{eq:beta-eigenvector-relation} may be
reduced by two mechanisms:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Some elements in \(\boldsymbol{\eta}_j, j = 1, \ldots m\) are zero
\item
  There are coinciding eigenvalues of \(\boldsymbol{\Sigma}_{xx}\) such
  that it is enough to have one eigenvector in
  equation\textasciitilde{}\eqref{eq:beta-eigenvector-relation}.
\end{enumerate}

Let there are \(k\) number predictor components that are relevant for
any of the response. The \(\mathcal{P}\) contains the indices of these
position for each response components. Here the order of components of
predictor is defined by a decreasing set of eigenvalues such that
\(\lambda_1 \ge \ldots \ge \lambda_p > 0\). In \texttt{simrel-m}
package, these set of position is referred as \texttt{relpos}. For
example, if \(\mathcal{P} = {{1, 2}, {3, 5}, {4}}\) then we can say that
there are 3 informative space in response such that components 1 and 2
of predictor variable is relevant for first response component;
component 3 and 5 of predictor are relevant for second response
component and fourth predictor component is relevant for third response
component. In addition, \(\lambda_1 \ge \ldots \ge \lambda_5\) are
relevant for some response.

In \texttt{simrel-m}, these 3 response components are combined with
non-informative vectors for desired number of response variables. This
is referred as \(\mathcal{S}\) and \texttt{ypos} in \texttt{simrel-m}
package. If \(\mathcal{S} = {{1, 4}, {2}, {3, 5}}\) then we can say that
there are 5 response variables which has 3 dimensional informative
space. Since response components 1, 2 and 3 are informative, from the
indices of \(\mathcal{S}\), first response component is combined with
non-informative fourth component and third informative component is
combined with fifth non-informative component. In this way, we will
obtain a set of 5 response variables for which predictor component 1 and
2 will be relevant for response 1 and 4; predictor component 3 and 5
will be relevant for response 2 and predictor component 4 will be
relevant for response 3 and 5.

For simplification, an assumption is made that all \(p\) eigenvalues of
\(\boldsymbol{\Sigma}_{XX}\) decrease exponentially as
\(e^{-\gamma (i - 1)}\) for \(i = 1, \ldots p\) and some positive
constant \(\gamma\). This way, the \(p\) eigenvalues depends on single
variables \(\gamma\) such that when \(\gamma\) is large, eigenvalues
decreases sharply referring high degree of multi-collinearity in
predictor variables.

Here we have assumed that the relevant components are know, as in
\citet{helland1994comparison}, which is rare in practice. But in
comparative studies of prediction methods, this can help to explain
interesting cases. For example, simultaneous envelope
\citep{cook2015simultaneous} has discussed about the informative
(material) and uninformative (immaterial) space on both \(\mathbf{X}\)
and \(\mathbf{Y}\). In this case, \texttt{simrel-m} can provide a
simulated data with various interesting cases for its assessment and
comparison with other methods.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Statistical Model}\label{statistical-model}

Let us consider a random regression model in
equation\textasciitilde{}\eqref{eq:model1} as our point of departure.

\begin{equation}
  \mathbf{Y} = \boldsymbol{\mu}_Y + \mathbf{B}^t (\mathbf{X} - \boldsymbol{\mu}_X) + \boldsymbol{\epsilon}
  \label{eq:model1}
\end{equation}

where \(\mathbf{Y}\) is a response matrix with \(m\) response variables
\(y_1, y_2, \ldots y_m\) with mean vector of \(\boldsymbol{\mu}_Y\);
\(\mathbf{X}\) is vector of \(p\) predictor variables and the random
error term \(\boldsymbol{\epsilon}\) is assumed to follow
\(N(\boldsymbol{0},\; \boldsymbol{\Sigma}_{Y|X})\). In addition, we
assume equation\textasciitilde{}\eqref{eq:model1} as a random regression
model where
\(\mathbf{X} \sim N\left(\boldsymbol{\mu}_X, \boldsymbol{\Sigma}_{XX}\right)\)
independent of \(\boldsymbol{\epsilon}\). Equivalently, this
relationship can be written as,

\begin{equation}
  \begin{bmatrix}\mathbf{Y}\\ \mathbf{X}\end{bmatrix} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})
  = N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_Y \\
      \boldsymbol{\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{YY} & \boldsymbol{\Sigma}_{XY}^t \\
      \boldsymbol{\Sigma}_{XY} & \boldsymbol{\Sigma}_{XX}
    \end{bmatrix}
  \right)
  \label{eq:model2}
\end{equation}

Here, \(\boldsymbol{\Sigma}_{YY}\) is Covariance Matrix of response
\(\mathbf{Y}\); \(\boldsymbol{\Sigma}_{XY}\) is Covariance Matrix
between \(\mathbf{X}\) and \(\mathbf{Y}\); \(\boldsymbol{\Sigma}_{XX}\)
is Covariance matrix of predictor variables \(\mathbf{X}\);
\(\boldsymbol{\mu}_Y\) and \(\boldsymbol{\mu}_X\) are Mean vectors of
response \(\mathbf{Y}\) and predictor \(\mathbf{X}\) respective.

Simulation of \((\mathbf{Y, X})\) for
model\textasciitilde{}\eqref{eq:model2} requires the fact that -- a set of
latent variable spanning \(\mathbf{X}\) and \(\mathbf{Y}\) will contain
same information in different structure. With two matrices
\(\mathbf{R}_{p\times p}\) and \(\mathbf{Q}_{q \times q}\) with rank
\(p\) and \(q\) respectively, lets define a transformation as
\(\mathbf{Z} = \mathbf{RX}\) and \(\mathbf{W} = \mathbf{QY}\) so that,

\begin{align}
  \begin{bmatrix}\mathbf{W} \\ 
  \boldsymbol{Z}\end{bmatrix}  & \sim N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_W \\ \boldsymbol{\mu}_Z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{WW} & \boldsymbol{\Sigma}_{WZ}^t \\
      \boldsymbol{\Sigma}_{ZW} & \boldsymbol{\Sigma}_{ZZ}
    \end{bmatrix} \right) \nonumber \\
  & = N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_Y \\
      \boldsymbol{R\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{YY}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{XY}^t\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{XY}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{XX}\mathbf{R}^t
    \end{bmatrix}
  \right)
  \label{eq:model3}
\end{align}

Further, if both \(\mathbf{Q}\) and \(\mathbf{R}\) are orthonormal
matrix such that \(\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_q\) and
\(\mathbf{R}^t\mathbf{R} = \mathbf{I}_p\), the inverse transformation
can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{XX} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{ZZ} = \mathbf{R} \boldsymbol{\Sigma}_{XX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{XY} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{ZW} = \mathbf{R} \boldsymbol{\Sigma}_{XY} \mathbf{Q}^t \\
    \boldsymbol{\Sigma}_{YX} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{WZ} = \mathbf{Q} \boldsymbol{\Sigma}_{YX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{YY} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{WW} = \mathbf{Q} \boldsymbol{\Sigma}_{YY} \mathbf{Q}^t
  \end{matrix}
  \label{eq:cov-yx-wz}
\end{equation}

In addition, a linear model relating \(\mathbf{W}\) and \(\mathbf{Z}\)
can be written as,

\begin{equation}
  \mathbf{W} =  \boldsymbol{\mu}_W + \mathbf{A}^t \left(\mathbf{Z} - \boldsymbol{\mu}_Z\right) + \boldsymbol{\tau}; \qquad
  \boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{W|Z}\right)
  \label{eq:latent-model}
\end{equation}

Here, we can find a direct connection between different population
properties between \eqref{eq:model1} and \eqref{eq:latent-model}. Some of
them are:

\begin{description}
\tightlist
\item[Regression Coefficients]
Regression coefficients for model\textasciitilde{}\eqref{eq:model1} is,
\[\mathbf{B} = \boldsymbol{\Sigma}_{YX} \boldsymbol{\Sigma}_{XX}^{-1}\]
Using the transformation matrix \(\mathbf{P}\) and \(\mathbf{Q}\), we
can obtain the regression coefficients corresponding to the latent
structure of predictors. \[
  \begin{aligned}
\mathbf{A} &= \boldsymbol{\Sigma}_{WZ} \boldsymbol{\Sigma}_{ZZ}^{-1}
  &= \boldsymbol{Q\Sigma}_{YZ}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{XX}\mathbf{R}^t\right]^{-1} \\
  &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\right]\mathbf{R}^t \\
  &= \mathbf{QBR}^t
  \end{aligned}
  \]
\item[Error Variance]
The noise variance and the minimum prediction error for
model\textasciitilde{}\eqref{eq:model1} is,
\[\boldsymbol{\Sigma}_{Y|X} = \boldsymbol{\Sigma}_{YY} - \boldsymbol{\Sigma}_{YX} \boldsymbol{\Sigma}_{XX}^{-1} \boldsymbol{\Sigma}_{XY}\]
Further, the noise variance of transformed
model\textasciitilde{}\eqref{eq:latent-model} is, \[
  \begin{aligned}
\boldsymbol{\Sigma}_{W|Z}
&= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t -
  \boldsymbol{Q \Sigma}_{YX}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{XX}\boldsymbol{R}^t\right]^{-1}
  \boldsymbol{R\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
&= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t - 
  \boldsymbol{Q \Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
&= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YY} -
  \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\right]\mathbf{Q}^{t} \nonumber \\
&= \mathbf{Q} \boldsymbol{\Sigma}_{Y|X}\mathbf{Q}^t
  \end{aligned}
  \]
\end{description}

\begin{description}
\tightlist
\item[Population Coefficient of Determination]
The population coefficient of determination for
model\textasciitilde{}\eqref{eq:model1} is,
\[\boldsymbol{\rho}^2_{XY} = \boldsymbol{\Sigma}_{YX} \boldsymbol{\Sigma}_{XX}^{-1} \boldsymbol{\Sigma}_{XY} \boldsymbol{\Sigma}_{YY}^{-1}\]
Further, the coefficient of determination corresponding to
model\textasciitilde{}\eqref{eq:latent-model} is, \[
  \begin{aligned}
\boldsymbol{\rho}^2_{ZW} &= \boldsymbol{\Sigma}_{WZ} 
\boldsymbol{\Sigma}_{ZZ}^{-1} \boldsymbol{\Sigma}_{ZW} 
\boldsymbol{\Sigma}_{WW}^{-1} \\
  &=\mathbf{Q}^t
\boldsymbol{\Sigma}_{WZ}\boldsymbol{\Sigma}^{-1}_{ZZ}
\boldsymbol{\Sigma}_{ZW}\boldsymbol{\Sigma}_{WW}^{-1}\boldsymbol{Q} \nonumber \\
  &=\mathbf{Q}^t\boldsymbol{\mathcal{R}}^2_{WZ}\mathbf{Q} \nonumber \\
\text{i.e. }\boldsymbol{\mathcal{R}}^2_{WZ} 
  &=\mathbf{Q}\boldsymbol{\mathcal{R}}^2_{XY}\mathbf{Q}^t
  \end{aligned}
  \]
\end{description}

\subsection{Model parameterization and relevant
components}\label{model-parameter-relevant-components}

Eigenvalue decomposition principal states that a variance-covariance
matrix \(\boldsymbol{\Sigma}\) can be decomposed as,

\begin{equation}
  \Sigma = \mathbf{E}\Lambda \mathbf{E}^t
\end{equation}

where,
\(\mathbf{E} = (\mathbf{e}_1, \mathbf{e}_2, \ldots \mathbf{e}_p)\) is an
orthogonal matrix of eigenvectors and \(\Lambda\) is a diagonal matrix
of eigenvalues \(\lambda_1 \le \lambda_2 \le \ldots \lambda_p\). From
expression in equation\textasciitilde{}\eqref{eq:cov-yx-wz},
\(\boldsymbol{\Sigma}_{XX}\) and \(\boldsymbol{\Sigma}_{WW}\) can have
similar decomposition with some suitable choice of orthonormal matrix
\(\mathbf{R}\) and \(\mathbf{Q}\) respectively.

In this study, all the components of \(\mathbf{Y}\), i.e. \(\mathbf{W}\)
are considered to be uncorrelated. Since, the component structure also
contains the irrelevant components, each of their correlation with
others are considered to be zero. Hence, the unconditional covariance
structure for the component matrix (\(\mathbf{W}\)) is \(\mathbf{I}_m\).
Furthermore, if
\(\boldsymbol{\Sigma}_{ZZ} = \Lambda = \text{diag}({\lambda_1, \lambda_2, \ldots, \lambda_p})\),
where \(\lambda_i, i = 1, \ldots p\) are eigenvalues of \(\mathbf{X}\),
the expression in \mbox{equation~\\eqref{eq:cov-yx-wz}} helps to simulate
\(\mathbf{X}\) from \(\mathbf{R}\), the orthonormal rotation matrix and
its eigen structure \(\boldsymbol{\Sigma}_{ZZ}\). Similarly from
\(\Sigma_{WW} = \mathbf{I}_m\) and rotaion matrix \(\mathbf{Q}\), we can
simulate \(\mathbf{Y}\).

Let \(\mathbf{W}_1, \ldots, \mathbf{W}_l\) are the components of \(Y\)
that are relevant to \(\mathbf{Z}\) and consequently \(\mathbf{X}\),
\(\mathbf{W}_{l+1}, \ldots, \mathbf{W}_q\) are not the outcome of
\(\mathbf{Z}\), the principal components of \(\mathbf{Z}\) that are
relevant for \(\mathbf{W}\) are applicable for
\(\mathbf{W}_1, \ldots, \mathbf{W}_l\) only. The covariance matrix of
\(\mathbf{W}\) and \(\mathbf{Z}\) (\(\boldsymbol{\Sigma}_{WZ}\)) is
constructed referring to the terminology in
\citet{helland1994comparison} that the principal components are termed
as relevant for which \(\boldsymbol{\Sigma}_{WZ}\) are non-zero.

Assume \(a_1, \ldots, a_l\) number of principal components of
\(\mathbf{X}\) are relevant to \(\mathbf{W}_1, \ldots, \mathbf{W}_l\)
respectively. Let \(\mathcal{P}_1, \ldots, \mathcal{P}_l\) are the sets
of positions of these components, then \((\Sigma_{WZ})_{ij} \ne 0\) if
\(j \in \mathcal{P}_i\), \(i = 1, \ldots, l\) and zero otherwise. This
follows us to the matrix of regression coefficients as,

\begin{equation}
  \mathbf{A} =
  \begin{cases}
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Sigma}_{ZZ}^{-1} =
    \sum_{j \in \mathcal{P}_i}{\left(\frac{\sigma_{ij}}{\lambda_j} \mathbf{t}_j\right)} & \text{ for } i = 1, \ldots, l \\
    0 & \text{ otherwise }
  \end{cases}
\end{equation}

where, \(\mathbf{t}_j\) is a \(p\)-vector with 1 at position \(j\) and
zero otherwise. As in the previous version of simrel by
\citet{saebo2015simrel}, eigenvalues of \(\boldsymbol{\Sigma}_{XX}\) is
assumed to be different and has adopted the parametric representation as
\(\lambda_j = e^{-\nu(j - 1)}\text{ for } \nu>0 \text{ and } j = 1, \ldots p\).
Here, the parameter \(\nu\) regulates the decline of
\(\lambda_j, j = 1, \ldots p\). Without loss of generality, for further
simplification, the first and largest eigenvalues are set to one.

For complete parametrization of the matrix \(\boldsymbol{\Xi}_{WZ}\) in
equation\textasciitilde{}\eqref{eq:model3}, covariances between \(W\) and
\(Z\) (\(\boldsymbol{\Sigma}_{WZ}\)) should be constructed such that it
is positive definite and satisfy the relation,

\begin{align}
  \boldsymbol{\mathcal{R}}^{2}_{WZ}                                      &=
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Sigma}^{-1}_{ZZ}\boldsymbol{\Sigma}_{ZW}\boldsymbol{\Sigma}_{WW}^{-1} \nonumber \\
  \text{i.e, } \boldsymbol{\mathcal{R}}_{WZ}^{2}\boldsymbol{\Sigma}_{WW} &=
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Lambda}^{-1}\boldsymbol{\Sigma}_{ZW}
\label{eq:sigmaRhoRelation}
\end{align}

For given \(\boldsymbol{\mathcal{R}}_{WZ}^{2}\) and
\(\Sigma_{WW} = \mathbf{I}_m\),
equation\textasciitilde{}\eqref{eq:sigmaRhoRelation} will be satisfied for
some \(\boldsymbol{\Sigma}_{WX}\) whose rows correspond to the relevant
components for \(\mathbf{W}\). As we have considered the situation that
no relevant components are common, elements in
\(\boldsymbol{\Sigma}_{WZ}\) are sampled from a uniform distribution
\(\mathcal{U}(-1, 1) = \{s_{\mathcal{P}_{1i}}, s_{\mathcal{P}_{2i}}, \ldots s_{\mathcal{P}_{pi}}\}\),
for each \(i = 1, \ldots q\) as in \citet{saebo2015simrel} such that,

\[
\left(\boldsymbol{\sigma}_{WZ}\right)_{ij} = \text{sign}\left(s_{ij}\right)
\sqrt{
  \frac
    {\boldsymbol{\mathcal{R}}_{WZ}^{2}.\left|s_{ij}\right|}
    {\sum_{k\in\mathcal{P}_i}{\left|s_{ik}\right|}}
  \lambda_{j}
}
\]

for \(j \in \mathcal{P}_i\) and for each \(i = 1, \ldots q\)

\subsection{Data Simulation}\label{data-simulation}

After the construction of \(\boldsymbol{\Xi}_{WZ}\), \(n\) samples are
generated from standard normal distribution
of\(\left(\mathbf{W}, \mathbf{Z} \right)\) considering their mean to be
zero, i.e.\(\boldsymbol{\mu}_W = 0\) and\(\boldsymbol{\mu}_Z=0\).
Since\(\boldsymbol{\Xi}_{WZ}\) is positive
definite,\(\boldsymbol{\Xi}_{WZ}^{1/2}\) obtained from its Cholesky
decomposition, can serve as one of its square root. The simulation
process constitute of following steps,

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  A matrix \(\mathbf{U}_{n\times (p + q)}\) is sampled from standard
  normal distribution
\item
  Compute \(\mathbf{G} = \boldsymbol{U\Xi}_{WZ}^{1/2}\)
\end{enumerate}

Here, first \(m\) columns of \(\mathbf{G}\) will serve as \(\mathbf{W}\)
and remaining \(p\) columns will serve as \(\mathbf{Z}\). Further, each
row of \(\mathbf{G}\) will be a vector sampled independently from joint
normal distribution of \(\left(\mathbf{W}, \mathbf{Z}\right)\). The
final step to generate \(\mathbf{X}\) and \(\mathbf{Y}\) from
\(\mathbf{Z}\) and \(\mathbf{W}\) requires corresponding rotation
matrices which is discusses on following section.

\subsection{Rotation of predictor space}\label{rotation-predictor-space}

Simulation of predictor variables from principal components requires a
construction of a rotation matrix \(\mathbf{R}\) that defines a new
basis for the same space as is spanned by the principle components. As
any rotation matrix can be considered as \(\mathbf{R}\), an eigenvalue
matrix from eigenvalue decomposition of \(\boldsymbol{\Sigma}_{XX}\) can
be a candidate. Since simulation is a reverse engineering, the
underlying covariance structure for the predictors are unknown. So, the
method is free to construct a real valued orthogonal matrix that can
serve for the purpose.

Among several methods
\citep{anderson1987generation, heiberger1978algorithm} to generate
random orthogonal matrix the same method as is used in
\citet{saebo2015simrel} is implemented here. The \(\mathcal{Q}\) matrix
obtained from QR-decomposition of a matrix filled with standard normal
variates can serve as the rotation matrix \(\mathbf{R}\).

The rotation can be a) unrestricted and b) restricted. The former one
rotates all \(p\) predictors making them some what relevant for the all
response conponents and consequently all responses. However, only
\(q_i \le p\) predictors are relevant for for \(i^\text{th}\) response
component, the resticted rotation is implemented in \texttt{simrel-M}.
This also ensure that \(p-q_i\) predictors does not contribute anything
on response component \(i\) and consequently the simulated data can also
be used for testing variable selection methods.

\subsection{Rotation of response space}\label{rotation-response-space}

\texttt{Simrel-M} has considered an exclusive relevant predictor space
for each response components, i.e.~a set of predictor variables only
influence one response component. However, it allows user to simulate
more response variable than response components. In this case, noise are
added during the orthogonal rotation of response components. For
example, if user wants to simulation 5 response variation from 3
response components. Two standard normal vectors are combined with
response components and rotated simultaneously. The rotation can be both
restricted and unrestricted as discussed in previous section. The
restricted rotation is carried out combining response vectors along with
noise vector in a block-wise manner according to the users choice.
Illustration in fig-\ldots{}

Suppose, in our previous example, if response components are combined as
-- \(\mathbf{W}_1, \mathbf{W}_4\), \(\mathbf{W}_2\) and
\(\mathbf{W}_3, \mathbf{W}_5\). Here, any predictor variable is only
relevant for \(\mathbf{W}_1, \mathbf{W}_2\) and \(\mathbf{W}_3\) while
\(\mathbf{W}_4\) and \(\mathbf{W}_5\) are noise. The resulting response
variables are \(\mathbf{Y}_1 \ldots \mathbf{Y}_5\) where, the first and
fourth response variable spans the same space as by the first response
components \(\mathbf{W}_1\) and noise component \(\mathbf{W}_4\) and so
on. Thus, the predictors and predictor space relevant for response
component \(\mathbf{W}_1\) is also relevant for response
\(\mathbf{Y}_1\) and \(\mathbf{Y}_4\).

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}


\renewcommand\refname{References}
\bibliography{packages.bib,ref-db.bib}



\end{document}
