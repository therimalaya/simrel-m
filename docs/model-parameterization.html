<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>simrel-m: A versatile tool for simulating multi-response linear model data</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<code>simrel-m</code>: A versatile tool for simulating multi-response linear model data">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="simrel-m: A versatile tool for simulating multi-response linear model data" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://therimalaya.github.io/simrel-m" />
  
  
  <meta name="github-repo" content="therimalaya/simrel-m" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="simrel-m: A versatile tool for simulating multi-response linear model data" />
  
  
  



<meta name="date" content="2017-04-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="relevant-components.html">
<link rel="next" href="references.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Simrel-M: A versatile Simulation Tool</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="statistical-model.html"><a href="statistical-model.html"><i class="fa fa-check"></i>Statistical Model</a></li>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html"><i class="fa fa-check"></i>Relevant Components</a></li>
<li class="chapter" data-level="" data-path="model-parameterization.html"><a href="model-parameterization.html"><i class="fa fa-check"></i>Model Parameterization</a><ul>
<li class="chapter" data-level="" data-path="model-parameterization.html"><a href="model-parameterization.html#position-of-relevant-components"><i class="fa fa-check"></i>Position of relevant components</a></li>
<li class="chapter" data-level="" data-path="model-parameterization.html"><a href="model-parameterization.html#data-simulation"><i class="fa fa-check"></i>Data Simulation</a></li>
<li class="chapter" data-level="" data-path="model-parameterization.html"><a href="model-parameterization.html#rotation-of-predictor-space"><i class="fa fa-check"></i>Rotation of predictor space</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li class="toc-footer"><a href="https://bookdown.org" target="blank">Published with BookDown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><code>simrel-m</code>: A versatile tool for simulating multi-response linear model data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-parameterization" class="section level1">
<h1>Model Parameterization</h1>
<p>In order to construct a covariance matrix of <span class="math inline">\(\mathbf{z}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> for model in equation~<a href="statistical-model.html#eq:model3">(3)</a>, we need to identify <span class="math inline">\(1/2 (p+m)(p+m+1)\)</span> unknown parameters. For the purpose of this simulation, we implement some assumption to re-parameterize and simplify the model parameters. This enables us to construct diverse nature of model from few key parameters.</p>
<dl>
<dt><strong>Parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span></strong></dt>
<dd>If we consider the rotation matrix <span class="math inline">\(\mathbf{R}\)</span> equals to the eigenvectors of <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span>, then <span class="math inline">\(\mathbf{z}\)</span> becomes the set of principle components of <span class="math inline">\(\mathbf{x}\)</span>. In that case <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span> is a diagonal matrix with eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_p\)</span>. Further, we adopt the following parametric representation of these eigenvalues, <span class="math display">\[\lambda_j = e^{-\gamma(i - 1)}, \gamma &gt;0 \text{ and } j = 1, 2, \ldots, p\]</span> Here as <span class="math inline">\(\gamma\)</span> increases, the decline of eigenvalues becomes steeper and hence a single parameter <span class="math inline">\(\gamma\)</span> can be used for <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span>.
</dd>
<dt><strong>Parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{ww}\)</span></strong></dt>
<dd>Here, we assume that <span class="math inline">\(\mathbf{w}\)</span>’s are independent and thus their covariance matrix is considered to be Identity <span class="math inline">\(\mathbf{I}_m\)</span>.
</dd>
<dt><strong>Parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{zw}\)</span></strong></dt>
<dd>After parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{ww}\)</span>, we are left with <span class="math inline">\(m \times p\)</span> number of unknowns corresponding to <span class="math inline">\(\boldsymbol{\Sigma}_{zw}\)</span>. The elements in this covariance matrix depends on position of x-component that are relevant for <span class="math inline">\(\mathbf{y}\)</span>. In order to re-parameterize this covariance matrix, it is necessary to discuss about the position of relevant components in details.
</dd>
</dl>
<div id="position-of-relevant-components" class="section level2">
<h2>Position of relevant components</h2>
<p>Let <span class="math inline">\(k_1\)</span> components be relevant for <span class="math inline">\(\mathbf{w}_1\)</span>, <span class="math inline">\(k_2\)</span> components be relevant for <span class="math inline">\(\mathbf{w}_2\)</span> and so on. Let the position of these components be given by the set <span class="math inline">\(\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m\)</span> respectively. Further, the covariance between <span class="math inline">\(\mathbf{w}_j\)</span> and <span class="math inline">\(\mathbf{z}_i\)</span> is non-zero only if <span class="math inline">\(\mathbf{z}_i\)</span> is relevant for <span class="math inline">\(\mathbf{w}_j\)</span>. If <span class="math inline">\(\sigma_{ij}\)</span> is the covariance between <span class="math inline">\(\mathbf{w}_j\)</span> and <span class="math inline">\(\mathbf{z_i}\)</span> then <span class="math inline">\(\sigma_{ij} \ne 0\)</span> if <span class="math inline">\(i \in \mathcal{P}_j\)</span> where <span class="math inline">\(i = 1, \ldots, p\)</span> and <span class="math inline">\(j = 1, \ldots, m\)</span> and <span class="math inline">\(\sigma_{ij} = 0\)</span> otherwise.</p>
<p>In addition, the corresponding regression coefficient for <span class="math inline">\(\mathbf{w}_j\)</span> is,</p>
<p><span class="math display">\[
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i}\mathbf{t}_{ij},\qquad j = 1, 2, \ldots m
\]</span></p>
<p>where, for <span class="math inline">\(j = 1, \ldots m\)</span>, <span class="math inline">\(\mathbf{t}_{ij}\)</span> is a vector with 1’s and 0’s such that <span class="math inline">\(\mathbf{t}_{ij} = 1\)</span> if the position of relevant components for <span class="math inline">\(\mathbf{w}_j\)</span> is in set <span class="math inline">\(\mathcal{P}_j\)</span> and 0 otherwise.</p>
<p>The position of relevant components have heavy impact on prediction. <span class="citation">Inge S Helland and Almøy (<a href="#ref-helland1994comparison">1994</a>)</span> have shown that if relevant components have large eigenvalues (variance), prediction of <span class="math inline">\(\mathbf{y}\)</span> from <span class="math inline">\(\mathbf{x}\)</span> is relatively easy and if the eigenvalues (variance) of relevant components is small, the prediction becomes difficult given that coefficient of determination and other model parameters held constant. For example, if first and second components of <span class="math inline">\(\mathbf{x}\)</span> are relevant for <span class="math inline">\(\mathbf{y}_1\)</span> and fifth and sixth componets are relevant for <span class="math inline">\(\mathbf{y}_2\)</span>, it is relatively easy to predict <span class="math inline">\(\mathbf{y}_1\)</span> than <span class="math inline">\(\mathbf{y}_2\)</span>. Since, the first and second principle components have larger variance than fifth and sixth components.</p>
<p>Although the covariance matrix depends only on few relevant components, we can not choose these covariances freely since we also need to satisfy following two conditions:</p>
<ul>
<li>The covariance matrix <span class="math inline">\(\Sigma_{zz}\)</span>, <span class="math inline">\(\Sigma_{ww}\)</span> and <span class="math inline">\(\Sigma\)</span> must be positive definite</li>
<li>The covariance <span class="math inline">\(\sigma_{ij}\)</span> must satisfy user defined coefficient of determination</li>
</ul>
<p>We have the relation, <span class="math display">\[\boldsymbol{\rho}_w^2 = \boldsymbol{\Sigma}_{zw}^t\boldsymbol{\Sigma}_{zz}^{-1}\boldsymbol{\Sigma}_{zw}\boldsymbol{\Sigma}_{ww}^I\]</span> Applying our assumption for simulation, <span class="math inline">\(\boldsymbol{\Sigma_{ww}} = \mathbf{I}_m\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{zz} = \Lambda\)</span>, we obtain,</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\rho}_w^2 &amp;= \boldsymbol{\Sigma}_{zw}^t \Lambda^{-1} \boldsymbol{\Sigma}_{zw} \mathbf{I}_m \\
&amp;= \begin{bmatrix}
\sum_{i = 1}^p \sigma_{i1}^2/\lambda_i          &amp; \ldots &amp; \sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i \\
\vdots                                          &amp; \ddots &amp; \vdots \\
\sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i &amp; \ldots &amp; \sum_{i = 1}^p \sigma_{im}^2/\lambda_i
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>Furthermore, we assume that there are no overlapping relevant components for any two <span class="math inline">\(\mathbf{w}\)</span>, i.e, <span class="math inline">\(n\left(\mathcal{P}_j \cap \mathcal{P}_{j*}\right) = 0\)</span> or <span class="math inline">\(\sigma_{ij}\sigma_{ij*} = 0\)</span> for <span class="math inline">\(j\ne j*\)</span>. The additional unknown parameters in diagonal of <span class="math inline">\(\boldsymbol{\rho}_w^2\)</span> should agree with user specified coefficient of determination for <span class="math inline">\(\mathbf{w}_j\)</span>. i.e, <span class="math inline">\(\rho_{wj}^2\)</span> is,</p>
<p><span class="math display">\[
\rho_{wj}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i}
\]</span></p>
<p>Here, only the relevant components have non-zero covariances with <span class="math inline">\(\mathbf{w}_j\)</span>, so,</p>
<p><span class="math display">\[
\rho_{wj}^2 = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}^2}{\lambda_i}
\]</span></p>
<p>For some user defined <span class="math inline">\(\rho_{jw}^2\)</span>, <span class="math inline">\(\sigma_{ij}^2\)</span> determined as follows,</p>
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(k_j\)</span> values from uniform distribution <span class="math inline">\(\mathcal{U}(-1, 1)\)</span> distribution. Let them be, <span class="math inline">\(\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}\)</span>.</li>
<li>Define, <span class="math display">\[\sigma_{ij} = \text{Sign}\left(\mathcal{S}_i\right)\sqrt{\frac{\rho_{wj}^2\left|\mathcal{S}_i\right|}{\sum_{k\in \mathcal{P}_j}\left|\mathcal{S}_k\right|} \lambda_i}\]</span> for <span class="math inline">\(i \in \mathcal{P}_j\)</span> and <span class="math inline">\(j = 1, \ldots, m\)</span></li>
</ol>
</div>
<div id="data-simulation" class="section level2">
<h2>Data Simulation</h2>
<p>After the construction of covariance matrix, <span class="math display">\[
  \boldsymbol{\Sigma} = 
  \begin{pmatrix}
    \boldsymbol{\Sigma}_{ww} &amp; \boldsymbol{\Sigma}_{wz} \\
    \boldsymbol{\Sigma}_{zw} &amp; \boldsymbol{\Sigma}_{zz}
  \end{pmatrix}
\]</span> <span class="math inline">\(n\)</span> observations are sampled from standard normal distribution of <span class="math inline">\(\left(\mathbf{w}, \mathbf{z} \right)\)</span> considering their mean to be zero, i.e. <span class="math inline">\(\boldsymbol{\mu}_w = 0\)</span> and <span class="math inline">\(\boldsymbol{\mu}_z=0\)</span>. Let us define <span class="math inline">\(\mathbf{G} = \mathbf{U}\boldsymbol{\Sigma}^{1/2}\)</span>, such that <span class="math inline">\(\mathbf{G}^t \mathbf{G} = \boldsymbol{\Sigma}\)</span>. Since <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is positive definite, <span class="math inline">\(\boldsymbol{\Sigma}^{1/2}\)</span> obtained from its Cholesky decomposition can serve as one of its square root and the matrix <span class="math inline">\(\mathbf{U}_{n\times (p + q)}\)</span> is sampled from standard normal distribution so that its covariance matrix <span class="math inline">\(\mathbf{U}^t\mathbf{U} = \mathbf{I}\)</span>. In addition the covariance matrix of <span class="math inline">\(\mathbf{G}\)</span> is <span class="math inline">\(\boldsymbol{\Sigma}\)</span> which satisfies all user defined population properties.</p>
<p>Here the first <span class="math inline">\(m\)</span> columns of <span class="math inline">\(\mathbf{G}\)</span> will serve as <span class="math inline">\(\mathbf{w}\)</span> and remaining <span class="math inline">\(p\)</span> columns will serve as <span class="math inline">\(\mathbf{z}\)</span>. Further, each row of <span class="math inline">\(\mathbf{G}\)</span> will be a vector sampled independently from joint normal distribution of <span class="math inline">\(\left(\mathbf{w}, \mathbf{z}\right)\)</span>. Finally, these simulated matrices <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{z}\)</span> are orthogonally rotated in order to obtain <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> respectively. Following section discuss about these rotation matrices in details.</p>
</div>
<div id="rotation-of-predictor-space" class="section level2">
<h2>Rotation of predictor space</h2>
<p>In order to make comments on predictor space, let us consider an example where a regression model with <span class="math inline">\(p = 10\)</span> predictors <span class="math inline">\((\mathbf{x})\)</span> and <span class="math inline">\(m = 4\)</span> responses <span class="math inline">\((\mathbf{y})\)</span>. Let only 3 principle components <span class="math inline">\((w_1, w_2\)</span> and <span class="math inline">\(w_3)\)</span> are needed to describe all 4 response variables. Further, let <span class="math inline">\(\mathcal{P}_1 = \{1, 2\}, \mathcal{P}_2 = \{3, 4\}\)</span> and <span class="math inline">\(\mathcal{P}_3 = \{5, 6\}\)</span> principle components of <span class="math inline">\(\mathbf{x}\)</span> are relevant for <span class="math inline">\(w_1, w_2\)</span> and <span class="math inline">\(w_3\)</span> respectively. Let <span class="math inline">\(\mathcal{S}_1\)</span>, <span class="math inline">\(\mathcal{S}_2\)</span> and <span class="math inline">\(\mathcal{S}_3\)</span> be the space spanned by them. These space together <span class="math inline">\(\mathcal{S}_k = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3\)</span> is the minimum relevant space similar to the x-envelope as discussed by <span class="citation">R. Cook, Helland, and Su (<a href="#ref-cook2013envelopes">2013</a>)</span>.</p>
<p>Moreover, let <span class="math inline">\(q_1 = 3, q_2 = 3\)</span> and <span class="math inline">\(q_3 = 2\)</span> number of predictor variables we want to be relevant for <span class="math inline">\(w_1, w_2\)</span> and <span class="math inline">\(w_3\)</span> respectively. So that <span class="math inline">\(q_1 = 3\)</span> predictor can be obtained by rotating the principle components in <span class="math inline">\(\mathcal{P}_1\)</span> along with one more irrelevant principle components. Similarly, <span class="math inline">\(q_2 = 3\)</span> predictors, relevant for <span class="math inline">\(w_2\)</span>, can be obtained by rotating principle components in <span class="math inline">\(\mathcal{P}_2\)</span> along with one more irrelevant components and <span class="math inline">\(q_3 = 2\)</span> predictors, relevant for <span class="math inline">\(w_3\)</span>, can be obtained by rotating principle components in <span class="math inline">\(\mathcal{P}_3\)</span> without any additional irrelevant components. Let the space spanned by <span class="math inline">\(q_1, q_2\)</span> and <span class="math inline">\(q_3\)</span> number of predictors be <span class="math inline">\(\mathcal{S}_{q_1}\)</span>, <span class="math inline">\(\mathcal{S}_{q_2}\)</span> and <span class="math inline">\(\mathcal{S}_{q_3}\)</span>. Together they form a space <span class="math inline">\(\mathcal{S}_q = \mathcal{S}_{q_1} \oplus \mathcal{S}_{q_2} \oplus \mathcal{S}_{q_3}\)</span>. This space is bigger than <span class="math inline">\(\mathcal{S}_k\)</span>. Here, <span class="math inline">\(\mathcal{S}_k\)</span> is orthogonal to <span class="math inline">\(\mathcal{S}_{p - k}\)</span> and <span class="math inline">\(\mathcal{S}_q\)</span> is orthogonal to <span class="math inline">\(\mathcal{S}_{p - q}\)</span>. Generally speaking, here we are splitting complete variable space <span class="math inline">\(\mathcal{S}_p\)</span> into two orthogonal space – <span class="math inline">\(\mathcal{S}_k\)</span> relevant for <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathcal{S}_{p - k}\)</span> irrelevant for <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>In the previous section, we discussed about constructing covariance matrix of latent structure. Figure~<a href="model-parameterization.html#fig:cov-plot-print">1</a> (left) shows a similar structure resembling the example here. The three colors represents their relevance with three latent structure of response <span class="math inline">\((w_1, w_2\)</span> and <span class="math inline">\(w_3)\)</span>. Here we can see that <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> (first and second principle components of <span class="math inline">\(\mathbf{x}\)</span>) have non-zero covariance with <span class="math inline">\(w_1\)</span> (first latent component of response <span class="math inline">\(\mathbf{y}\)</span>). In the similar manner other non-zero covariances are self-explanatory.</p>
<div class="figure"><span id="fig:cov-plot-print"></span>
<img src="Simrel-M_files/figure-html/cov-plot-print-1.svg" alt="Simulation of predictor and response variables after orthogonal transformation of principle components by a rotation matrix" width="100%" />
<p class="caption">
Figure 1: Simulation of predictor and response variables after orthogonal transformation of principle components by a rotation matrix
</p>
</div>
<p>In order to simulate predictor variables <span class="math inline">\((\mathbf{x})\)</span>, we construct matrix <span class="math inline">\(\mathbf{R}\)</span> which then is used for orthogonal rotation of principle components <span class="math inline">\(\mathbf{z}\)</span>. This defines a new basis for the same space as is spanned by the principle components. In principle, there are many possible options for a rotation matrix. Among them, the eigenvector matrix of <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span> can be a candidate. However, in this reverse engineering both rotation matrices <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\(\mathbf{Q}\)</span> along with the covariance matrices <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span> are unknown. So, we are free to choose any <span class="math inline">\(\mathbf{R}\)</span> that satisfied the properties of a real valued rotation matrix, i.e <span class="math inline">\(\mathbf{R}^{-1} = \mathbf{R}^t\)</span> so that <span class="math inline">\(\mathbf{R}\)</span> is orthonormal and its determinant becomes <span class="math inline">\(\pm 1\)</span>. Here the rotation matrix <span class="math inline">\(\mathbf{R}\)</span> should be block diagonal as in figure~<a href="model-parameterization.html#fig:cov-plot-print">1</a> (middle) in order to rotate spaces <span class="math inline">\(\mathcal{S}_1, \mathcal{S}_2 \ldots\)</span> separately. Figure~<a href="model-parameterization.html#fig:simulated-data">2</a> (left) shows the simulated principle components <span class="math inline">\(\mathbf{z}\)</span> that we are following in our example where we can see that the principle component <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> relevant for <span class="math inline">\(w_1\)</span> is getting rotated together with an irrelevant <span class="math inline">\(z_10\)</span>. The resultant predictors (Figure~<a href="model-parameterization.html#fig:simulated-data">2</a>, right) <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_10\)</span> will also be relevant for <span class="math inline">\(w_1\)</span>. In the figure, we can see that principle components <span class="math inline">\(z_8\)</span> and <span class="math inline">\(z_9\)</span> and corresponding predictors are not relevant for any of the response.</p>
<div class="figure"><span id="fig:simulated-data"></span>
<img src="Simrel-M_files/figure-html/simulated-data-1.svg" alt="Simulated Data before (left) and after (right) rotation" width="100%" />
<p class="caption">
Figure 2: Simulated Data before (left) and after (right) rotation
</p>
</div>
<p>Among several methods <span class="citation">(Anderson, Olkin, and Underhill <a href="#ref-anderson1987generation">1987</a>; Heiberger <a href="#ref-heiberger1978algorithm">1978</a>)</span> for generating random orthogonal matrix, in this paper we are using orthogonal matrix <span class="math inline">\(\mathcal{Q}\)</span> obtained from QR-decomposition of a matrix filled with standard normal variate. The rotation here can be a) restricted and b) unrestricted. The later one rotates all principle components <span class="math inline">\(\mathbf{z}\)</span> together and makes all predictor variables somewhat relevant for all response variables. However, the former one performs a block-wise rotation so that it rotates certain selected principle components together. This gives control for specifying certain predictors as relevant for selected response, which was discussed in our example above. This also lets us to simulate irrelevant predictors such as <span class="math inline">\(x_7\)</span> and <span class="math inline">\(x_8\)</span> which can be detected during variables selection procedure.</p>
<!-- ## Rotation of response space ## -->
<!-- `Simrel-M` has considered an exclusive relevant predictor space for each response components, i.e. a set of predictor variables only influence one response component. However, it allows user to simulate more response variable than response components. In this case, noise are added during the orthogonal rotation of response components. For example, if user wants to simulation 5 response variation from 3 response components. Two standard normal vectors are combined with response components and rotated simultaneously. The rotation can be both restricted and unrestricted as discussed in previous section. The restricted rotation is carried out combining response vectors along with noise vector in a block-wise manner according to the users choice. Illustration in fig-... -->
<!-- Suppose, in our previous example, if response components are combined as -- $\mathbf{w}_1, \mathbf{w}_4$, $\mathbf{w}_2$ and $\mathbf{w}_3, \mathbf{w}_5$. Here, any predictor variable is only relevant for $\mathbf{w}_1, \mathbf{w}_2$ and $\mathbf{w}_3$ while $\mathbf{w}_4$ and $\mathbf{w}_5$ are noise. The resulting response variables are $\mathbf{y}_1 \ldots \mathbf{y}_5$ where, the first and fourth response variable spans the same space as by the first response components $\mathbf{w}_1$ and noise component $\mathbf{w}_4$ and so on. Thus, the predictors and predictor space relevant for response component $\mathbf{w}_1$ is also relevant for response $\mathbf{y}_1$ and $\mathbf{y}_4$.  -->

<!-- # References {-} -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-helland1994comparison">
<p>Helland, Inge S, and Trygve Almøy. 1994. “Comparison of Prediction Methods When Only a Few Components Are Relevant.” <em>Journal of the American Statistical Association</em> 89 (426). Taylor &amp;amp; Francis Group: 583–91.</p>
</div>
<div id="ref-cook2013envelopes">
<p>Cook, RD, IS Helland, and Z Su. 2013. “Envelopes and Partial Least Squares Regression.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 75 (5). Wiley Online Library: 851–77.</p>
</div>
<div id="ref-anderson1987generation">
<p>Anderson, Theodore W, Ingram Olkin, and Les G Underhill. 1987. “Generation of Random Orthogonal Matrices.” <em>SIAM Journal on Scientific and Statistical Computing</em> 8 (4). SIAM: 625–29.</p>
</div>
<div id="ref-heiberger1978algorithm">
<p>Heiberger, Richard M. 1978. “Algorithm as 127: Generation of Random Orthogonal Matrices.” <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 27 (2). JSTOR: 199–206.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="relevant-components.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/therimalaya/simrel-m/edit/master/Includes/02-StatisticalModel.Rmd",
"text": "Edit"
},
"download": ["Simrel-M.epub", "Simrel-M.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
