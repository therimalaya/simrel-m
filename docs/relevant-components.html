<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A tool for simulating multi-response linear model data</title>
  <meta name="description" content="A tool for simulating multi-response linear model data">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="A tool for simulating multi-response linear model data" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://therimalaya.github.io/simrel-m" />
  
  
  <meta name="github-repo" content="therimalaya/simrel-m" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A tool for simulating multi-response linear model data" />
  
  
  



<meta name="date" content="2017-10-31">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="statistical-model.html">
<link rel="next" href="implementation.html">
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="lead"><a href="./index.html">Simrel-M: A versatile Simulation Tool</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="statistical-model.html"><a href="statistical-model.html"><i class="fa fa-check"></i>Statistical Model</a></li>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html"><i class="fa fa-check"></i>Relevant Components</a><ul>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html#model-parameterization"><i class="fa fa-check"></i>Model Parameterization</a><ul>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html#position-of-relevant-components"><i class="fa fa-check"></i>Position of relevant components</a></li>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html#data-simulation"><i class="fa fa-check"></i>Data Simulation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html#rotation-of-predictor-space"><i class="fa fa-check"></i>Rotation of predictor space</a></li>
<li class="chapter" data-level="" data-path="relevant-components.html"><a href="relevant-components.html#rotation-of-response-space"><i class="fa fa-check"></i>Rotation of response space</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i>Implementation</a></li>
<li class="chapter" data-level="" data-path="web-interface.html"><a href="web-interface.html"><i class="fa fa-check"></i>Web Interface</a></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li class="toc-footer"><a href="https://bookdown.org" target="blank">Published with BookDown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A tool for simulating multi-response linear model data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="relevant-components" class="section level1">
<h1>Relevant Components</h1>
<p>Consider a single response linear model with <span class="math inline">\(p\)</span> predictors.</p>
<p><span class="math display">\[y = \mu_y + \boldsymbol{\beta}^t\left(\mathbf{x} - \mu_x\right) + \epsilon\]</span></p>
<p>where, <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span> and <span class="math inline">\(\mathbf{x}\)</span> is a vector of random predictors. Following the concept of relevant space and irrelevant space which is discussed extensively in <span class="citation">Helland and Almøy (1994)</span>, <span class="citation">Helland (2000)</span>, <span class="citation">Helland et al. (2012)</span>, <span class="citation">Cook, Helland, and Su (2013)</span>, and <span class="citation">Sæbø, Almøy, and Helland (2015)</span>, we can assume that there exists a subspace of the full predictor space which is relevant for <span class="math inline">\(y\)</span>. An orthogonal space to this space does not contain any information about <span class="math inline">\(y\)</span> and is considered as irrelevant. Here, the <span class="math inline">\(y-\)</span>relevant subspace of <span class="math inline">\(\mathbf{x}\)</span> is spanned by a subset of the principal components defined by the eigenvectors of the covariance matrix of <span class="math inline">\(\mathbf{x}\)</span>, i.e. <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span>.</p>
<p>This concept can be extended to <span class="math inline">\(m\)</span> responses so that the subspace of <span class="math inline">\(\mathbf{x}\)</span> is relevant for a subspace of <span class="math inline">\(\mathbf{y}\)</span>. This corresponds to the concept of simultaneous envelopes <span class="citation">(R. Dennis Cook and Zhang 2015)</span> where relevant (material) and irrelevant (immaterial) space were discussed for both response and predictor variables.</p>
<div id="model-parameterization" class="section level2">
<h2>Model Parameterization</h2>
<p>In order to construct a fully specified and unrestricted covariance matrix of <span class="math inline">\(\mathbf{z}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> for the model in equation <a href="statistical-model.html#eq:model3">(3)</a>, we need to identify <span class="math inline">\(1/2 (p+m)(p+m+1)\)</span> unknown parameters. For the purpose of simulation, we implement some assumptions to re-parameterize and simplify the model. This enables us to construct a wide range of model properties from only few key parameters.</p>
<dl>
<dt><strong>Parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span></strong></dt>
<dd>If we let the rotation matrix <span class="math inline">\(\mathbf{R}\)</span> correspond to the eigenvectors of <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span>, then <span class="math inline">\(\mathbf{z}\)</span> becomes the set of principal components of <span class="math inline">\(\mathbf{x}\)</span>. In that case <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span> is a diagonal matrix with eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_p\)</span>. Further, we adopt the same parametric representation as <span class="citation">Sæbø, Almøy, and Helland (2015)</span> for these eigenvalues:
<span class="math display" id="eq:gamma-parameter">\[\begin{equation}
  \lambda_i = e^{-\gamma(i - 1)}, \gamma &gt;0 \text{ and } i = 1, 2, \ldots, p
  \tag{6}
  \end{equation}\]</span>
Here, as <span class="math inline">\(\gamma\)</span> increases, the decline of eigenvalues becomes steeper, hence the parameter <span class="math inline">\(\gamma\)</span> controls the level of multicollinearity in <span class="math inline">\(\mathbf{x}\)</span>. Hence, we can write <span class="math inline">\(\Sigma_{zz} = \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)\)</span>.
</dd>
<dt><strong>Parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{ww}\)</span></strong></dt>
<dd>In similar manner, a parametric representation of eigenvalues corresponding to <span class="math inline">\(\Sigma_{ww}\)</span> is adopted as,
<span class="math display" id="eq:eta-parameter">\[\begin{equation}
  \kappa_j = e^{-\eta(j - 1)}, \eta &gt;0 \text{ and } j = 1, 2, \ldots, m
  \tag{7}
  \end{equation}\]</span>
Here, the decline of eigenvalues becomes steeper as <span class="math inline">\(\eta\)</span> gets far from zero. At <span class="math inline">\(\eta = 0\)</span>, all <span class="math inline">\(w\)</span> will have equal variance. Hence we can write <span class="math inline">\(\Sigma_{ww} = \text{diag}(\kappa_1, \ldots, \kappa_m)\)</span>.
</dd>
<dt><strong>Parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{zw}\)</span></strong></dt>
<dd>After parameterization of <span class="math inline">\(\boldsymbol{\Sigma}_{zz}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{ww}\)</span>, we are left with <span class="math inline">\(m \times p\)</span> number of unknowns corresponding to <span class="math inline">\(\boldsymbol{\Sigma}_{zw}\)</span>. Some of the elements of <span class="math inline">\(\boldsymbol{\Sigma}_{zw}\)</span> may be equal to zero, which implies that the given <span class="math inline">\(z\)</span> is irrelevant for the given variable <span class="math inline">\(w\)</span>. The non-zero elements define which of the <span class="math inline">\(z\)</span> are relevant for <span class="math inline">\(\mathbf{w}\)</span>. We typically refer to the indices of these <span class="math inline">\(z\)</span> variables as the positions of relevant components. In order to re-parameterize this covariance matrix, it is necessary to discuss the position of relevant components in detail.
</dd>
</dl>
<div id="position-of-relevant-components" class="section level3">
<h3>Position of relevant components</h3>
<p>Let <span class="math inline">\(k_1\)</span> components be relevant for <span class="math inline">\(w_1\)</span>, <span class="math inline">\(k_2\)</span> components be relevant for <span class="math inline">\(w_2\)</span> and so on. Let the positions of these components be given by the index sets <span class="math inline">\(\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m\)</span> respectively. Further, the covariance between <span class="math inline">\(w_j\)</span> and <span class="math inline">\(z_i\)</span> is non-zero only if <span class="math inline">\(z_i\)</span> is relevant for <span class="math inline">\(w_j\)</span>. If <span class="math inline">\(\sigma_{ij}\)</span> is the covariance between <span class="math inline">\(w_j\)</span> and <span class="math inline">\(z_i\)</span> then <span class="math inline">\(\sigma_{ij} \ne 0\)</span> if <span class="math inline">\(i \in \mathcal{P}_j\)</span> where <span class="math inline">\(i = 1, \ldots, p\)</span> and <span class="math inline">\(j = 1, \ldots, m\)</span> and <span class="math inline">\(\sigma_{ij} = 0\)</span> otherwise.</p>
<p>In addition, the true regression coefficients for <span class="math inline">\(w_j\)</span> [ref:<a href="statistical-model.html#eq:latent-model">(4)</a>] is given by:</p>
<p><span class="math display">\[
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i},\qquad j = 1, 2, \ldots m
\]</span></p>
<p>The positions of the relevant components have heavy impact on prediction. <span class="citation">Helland and Almøy (1994)</span> have shown that if the relevant components have large eigenvalues (variances), which here implies small index values in <span class="math inline">\(\mathcal{P}_j\)</span>, prediction of <span class="math inline">\(\mathbf{y}\)</span> from <span class="math inline">\(\mathbf{x}\)</span> is relatively easy and if the eigenvalues (variances) of relevant components are small, the prediction becomes difficult, given that the coefficient of determination and other model parameters are held constant. For example, if the first and second components, <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span>, are relevant for <span class="math inline">\(w_1\)</span> and fifth and sixth components, <span class="math inline">\(z_5\)</span> and <span class="math inline">\(z_6\)</span>, are relevant for <span class="math inline">\(w_2\)</span>, it is relatively easier to predict <span class="math inline">\(w_1\)</span> than <span class="math inline">\(w_2\)</span>, other properties being similar. This might be so, because the first and second principal components have larger variances than the fifth and sixth components.</p>
<p>Although the covariance matrix may depend on few relevant components, we can not choose these covariances freely since we also need to satisfy following two conditions:</p>
<ul>
<li>The covariance matrices <span class="math inline">\(\Sigma_{zz}\)</span>, <span class="math inline">\(\Sigma_{ww}\)</span> and <span class="math inline">\(\Sigma\)</span> must be positive definite</li>
<li>The covariance <span class="math inline">\(\sigma_{ij}\)</span> must satisfy user defined coefficient of determination</li>
</ul>
<p>We have the relation, <span class="math display">\[\boldsymbol{\rho}_w^2 = \boldsymbol{\Sigma}_{zw}^t\boldsymbol{\Sigma}_{zz}^{-1}\boldsymbol{\Sigma}_{zw}\boldsymbol{\Sigma}_{ww}^{-1}\]</span> Applying our assumptions that, <span class="math inline">\(\boldsymbol{\Sigma_{ww}} = \text{diag}(\kappa_1, \ldots, \kappa_m)\)</span> (<a href="relevant-components.html#eq:eta-parameter">(7)</a>) and <span class="math inline">\(\boldsymbol{\Sigma}_{zz} = \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)\)</span> (<a href="relevant-components.html#eq:gamma-parameter">(6)</a>), we obtain,</p>
<p><span class="math display">\[
  \rho_{w}^2 = \Sigma_{zw}^t\Lambda^{-1}\Sigma_{zw}\Sigma_{ww}^{-1} =
  \begin{bmatrix}
    \sum_{i = 1}^p\frac{\sigma_{i1}^2}{\lambda_i\kappa_1} &amp; \ldots &amp; \sum_{i=1}^p\frac{\sigma_{i1}\sigma_{im}}{\lambda_i\kappa_1} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \sum_{i=1}^p\frac{\sigma_{i1}\sigma_{im}}{\lambda_i\kappa_m} &amp; \ldots &amp; \sum_{i = 1}^p\frac{\sigma_{im}^2}{\lambda_i\kappa_m}
  \end{bmatrix}
\]</span></p>
<p>Furthermore, we assume that there are no overlapping relevant components for any two <span class="math inline">\(w\)</span>, i.e, <span class="math inline">\(\mathcal{P}_j \cap \mathcal{P}_{j*} = \varnothing\)</span> or <span class="math inline">\(\sigma_{ij}\sigma_{ij*} = 0\)</span> for <span class="math inline">\(j\ne j*\)</span>. The additional unknown parameters in the diagonal of <span class="math inline">\(\boldsymbol{\rho}_w^2\)</span> should agree with user specified coefficients of determination for <span class="math inline">\(\mathbf{w}\)</span>. i.e, <span class="math inline">\(\rho_{wj}^2\)</span> is,</p>
<p><span class="math display">\[
\rho_{w_j}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i\kappa_j}
\]</span></p>
<p>Here, only the relevant components have non-zero covariances with <span class="math inline">\(w_j\)</span>, so,</p>
<p><span class="math display">\[
\rho_{w_j}^2 = \sum_{i \in \mathcal{P}_j} \frac{\sigma_{ij}^2}{\lambda_i\kappa_j}
\]</span></p>
<p>For some user defined <span class="math inline">\(\rho_{jw}^2\)</span> the <span class="math inline">\(\sigma_{ij}^2\)</span> is determined as follows,</p>
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(k_j\)</span> values from a uniform distribution <span class="math inline">\(\mathcal{U}(-1, 1)\)</span> distribution. Let them be denoted <span class="math inline">\(\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}\)</span>.</li>
<li>Define, <span class="math display">\[\sigma_{ij} = \text{Sign}(\mathcal{S}_i)\sqrt{\frac{\rho_{wj}^2\left|\mathcal{S}_i\right|}{\sum_{k\in\mathcal{P}_j}\left|\mathcal{S}_k\right|}\lambda_i\kappa_j}\]</span> for <span class="math inline">\(i \in \mathcal{P}_j\)</span> and <span class="math inline">\(j = 1, \ldots m\)</span></li>
</ol>
</div>
<div id="data-simulation" class="section level3">
<h3>Data Simulation</h3>
<p>From the above given parameterizations and the user defined choices of model parameters, a fully defined and known covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> of <span class="math inline">\((\mathbf{w, z})\)</span> is given. For the simulation of a single observation of <span class="math inline">\((\mathbf{w, z})\)</span> let us define <span class="math inline">\(\mathbf{g} = \boldsymbol{\Sigma}^{-1/2}\mathbf{u}\)</span> such that <span class="math inline">\(\text{cov}(\mathbf{g}) = \boldsymbol{\Sigma}\)</span>. Here <span class="math inline">\(\boldsymbol{\Sigma}^{-1/2}\)</span> is obtained from Choleskey decomposition of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, and <span class="math inline">\(\mathbf{u}\)</span> is simulated from independent standard normal distribution.</p>
<p>Similarly, in order to simulate <span class="math inline">\(n\)</span> observations, we define <span class="math inline">\(\underset{n \times (m + p)}{\mathbf{G}} = \mathbf{U}\boldsymbol{\Sigma}^{-1/2}\)</span>. Here the first <span class="math inline">\(m\)</span> columns of <span class="math inline">\(\mathbf{G}\)</span> will serve as <span class="math inline">\(\mathbf{W}\)</span> and remaining <span class="math inline">\(p\)</span> columns will serve as <span class="math inline">\(\mathbf{Z}\)</span>. Further, each row of <span class="math inline">\(\mathbf{G}\)</span> will be a vector sampled independently from the joint normal distribution of <span class="math inline">\(\left(\mathbf{w}, \mathbf{z}\right)\)</span>. Finally, these simulated matrices <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span> are orthogonally rotated in order to obtain <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> respectively. The following section discuss about these rotation matrices in detail.</p>
</div>
</div>
<div id="rotation-of-predictor-space" class="section level2">
<h2>Rotation of predictor space</h2>
<p>Initially, let us consider an example where a regression model with <span class="math inline">\(p = 10\)</span> predictors <span class="math inline">\((\mathbf{x})\)</span> and <span class="math inline">\(m = 4\)</span> responses <span class="math inline">\((\mathbf{y})\)</span>. Let’s assume that only three response components <span class="math inline">\((w_1, w_2\)</span> and <span class="math inline">\(w_3)\)</span> are needed to describe all four response variables. Further, let the index sets <span class="math inline">\(\mathcal{P}_1 = \{1, 2\}, \mathcal{P}_2 = \{3, 4\}\)</span> and <span class="math inline">\(\mathcal{P}_3 = \{5, 6\}\)</span> define the positions of the predictor components of <span class="math inline">\(\mathbf{x}\)</span> that are relevant for <span class="math inline">\(w_1, w_2\)</span> and <span class="math inline">\(w_3\)</span> respectively. Let <span class="math inline">\(\mathcal{S}_1\)</span>, <span class="math inline">\(\mathcal{S}_2\)</span> and <span class="math inline">\(\mathcal{S}_3\)</span> be the orthogonal spaces spanned by each set of predictor components. These spaces together span <span class="math inline">\(\mathcal{S}_k = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3\)</span> which is the minimum relevant space and equivalent to the x-envelope as discussed by <span class="citation">Cook, Helland, and Su (2013)</span>.</p>
<p>Moreover, let <span class="math inline">\(q_1 = 3, q_2 = 3\)</span> and <span class="math inline">\(q_3 = 2\)</span> be the number of predictor variables we want to have relevant for <span class="math inline">\(w_1, w_2\)</span> and <span class="math inline">\(w_3\)</span> respectively. Then <span class="math inline">\(q_1 = 3\)</span> predictors may be obtained by rotating the predictor components in <span class="math inline">\(\mathcal{P}_1\)</span> along with one more irrelevant component. Similarly, <span class="math inline">\(q_2 = 3\)</span> predictors, relevant for <span class="math inline">\(w_2\)</span>, can be obtained by rotating predictor components in <span class="math inline">\(\mathcal{P}_2\)</span> along with one more irrelevant component and finally, <span class="math inline">\(q_3 = 2\)</span> predictors, relevant for <span class="math inline">\(w_3\)</span>, can be obtained by rotating the components in <span class="math inline">\(\mathcal{P}_3\)</span> without any additional irrelevant component. Let the space spanned by the <span class="math inline">\(q_1, q_2\)</span> and <span class="math inline">\(q_3\)</span> number of predictors be <span class="math inline">\(\mathcal{S}_{q_1}\)</span>, <span class="math inline">\(\mathcal{S}_{q_2}\)</span> and <span class="math inline">\(\mathcal{S}_{q_3}\)</span>. Together they span a space <span class="math inline">\(\mathcal{S}_q = \mathcal{S}_{q_1} \oplus \mathcal{S}_{q_2} \oplus \mathcal{S}_{q_3}\)</span>. This space is bigger than <span class="math inline">\(\mathcal{S}_k\)</span> since in the process two irrelevant components were included in the rotations. Here, <span class="math inline">\(\mathcal{S}_k\)</span> is orthogonal to <span class="math inline">\(\mathcal{S}_{p - k}\)</span> and <span class="math inline">\(\mathcal{S}_q\)</span> is orthogonal to <span class="math inline">\(\mathcal{S}_{p - q}\)</span>. Generally speaking, here we are splitting the complete variable space <span class="math inline">\(\mathcal{S}_p\)</span> into two orthogonal spaces – <span class="math inline">\(\mathcal{S}_k\)</span> relevant for <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathcal{S}_{p - k}\)</span> irrelevant for <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>In the previous section, we discussed about the construction of a covariance matrix for the latent structure. Figure <a href="relevant-components.html#fig:cov-plot-print">1</a>(a) shows a similar structure resembling the example here. The three colors represent the relevance with the three latent response components <span class="math inline">\((w_1, w_2\)</span> and <span class="math inline">\(w_3)\)</span>. Here we can see that <span class="math inline">\(z_{1}\)</span> and <span class="math inline">\(z_{2}\)</span> (first and second predictor components of <span class="math inline">\(\mathbf{x}\)</span>) have non-zero covariance with <span class="math inline">\(w_1\)</span> (first latent component of response <span class="math inline">\(\mathbf{y}\)</span>). In the similar manner other non-zero covariances are self-explanatory.</p>
<div class="figure"><span id="fig:cov-plot-print"></span>
<img src="main_files/figure-html/cov-plot-print-1.svg" alt="Simulation of predictor and response variables after orthogonal transformation of predictor and response components by rotation matrices $Q$ and $R$ shown as the upper left and the lower right block matrices in (b)." width="33%" /><img src="main_files/figure-html/cov-plot-print-2.svg" alt="Simulation of predictor and response variables after orthogonal transformation of predictor and response components by rotation matrices $Q$ and $R$ shown as the upper left and the lower right block matrices in (b)." width="33%" /><img src="main_files/figure-html/cov-plot-print-3.svg" alt="Simulation of predictor and response variables after orthogonal transformation of predictor and response components by rotation matrices $Q$ and $R$ shown as the upper left and the lower right block matrices in (b)." width="33%" />
<p class="caption">
Figure 1: Simulation of predictor and response variables after orthogonal transformation of predictor and response components by rotation matrices <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> shown as the upper left and the lower right block matrices in (b).
</p>
</div>
<p>In order to simulate predictor variables <span class="math inline">\((\mathbf{x})\)</span>, we construct matrix <span class="math inline">\(\mathbf{R}\)</span> which then is used for orthogonal rotation of the predictor components <span class="math inline">\(\mathbf{z}\)</span>. This defines a new basis for the same space as is spanned by the predictor components. In principle, there are many possible options for defining a rotation matrix. Among them, the eigenvector matrix of <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span> can be a candidate. However, in this reverse engineering approach both rotation matrices <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\(\mathbf{Q}\)</span> along with the covariance matrices <span class="math inline">\(\boldsymbol{\Sigma}_{xx}\)</span> are unknown. So, we are free to choose any <span class="math inline">\(\mathbf{R}\)</span> that satisfied the properties of a real valued rotation matrix, i.e <span class="math inline">\(\mathbf{R}^{-1} = \mathbf{R}^t\)</span> so that <span class="math inline">\(\mathbf{R}\)</span> is orthonormal. Here the rotation matrix <span class="math inline">\(\mathbf{R}\)</span> should be block diagonal as in Figure <a href="relevant-components.html#fig:cov-plot-print">1</a>(b) in order to rotate spaces <span class="math inline">\(\mathcal{S}_1, \mathcal{S}_2 \ldots\)</span> separately. Figure <a href="relevant-components.html#fig:simulated-data">2</a>(a) shows the simulated predictor components <span class="math inline">\(\mathbf{z}\)</span> that we are following in our example where we can see that the components <span class="math inline">\(z_{1}\)</span> and <span class="math inline">\(z_{2}\)</span> (relevant for <span class="math inline">\(w_1\)</span>) is getting rotated together with an irrelevant component <span class="math inline">\(z_{8}\)</span>. The resultant predictors (Figure <a href="relevant-components.html#fig:simulated-data">2</a>(b)) <span class="math inline">\(x_{1}, x_{2}\)</span> and <span class="math inline">\(x_{8}\)</span> will hence also be relevant for <span class="math inline">\(w_1\)</span>. In the figure, we can see that components <span class="math inline">\(z_{7}, z_{8}, z_{9}\)</span> and <span class="math inline">\(z_{10}\)</span> are not relevant for any responses before rotation, however, the <span class="math inline">\(x_{8}, x_{10}\)</span> predictors become relevant after rotation keeping <span class="math inline">\(x_{7}\)</span> and <span class="math inline">\(x_{9}\)</span> still irrelevant.</p>
<div class="figure" style="text-align: center"><span id="fig:simulated-data"></span>
<img src="main_files/figure-html/simulated-data-1.svg" alt="Simulated Data before" width="48%" /><img src="main_files/figure-html/simulated-data-2.svg" alt="Simulated Data before" width="48%" />
<p class="caption">
Figure 2: Simulated Data before
</p>
</div>
<p>Among several methods <span class="citation">(Anderson, Olkin, and Underhill 1987; Heiberger 1978)</span> for generating random orthogonal matrix, in this paper we are using orthogonal matrix <span class="math inline">\(\mathcal{Q}\)</span> obtained from QR-decomposition of a matrix filled with standard normal variate. The rotation here can be a) restricted and b) unrestricted. The latter rotates all components <span class="math inline">\(\mathbf{z}\)</span> together and makes all predictor variables somewhat relevant for all response components. However, the former performs a block-wise rotation so that it rotates certain selected predictor components together. This gives control for specifying certain predictors as relevant for selected responses, which was discussed in our example above. This also allows us to simulate irrelevant predictors such as <span class="math inline">\(x_{7}\)</span> and <span class="math inline">\(x_{9}\)</span> which can be detected during variable selection procedures.</p>
</div>
<div id="rotation-of-response-space" class="section level2">
<h2>Rotation of response space</h2>
<p>The previous example has four response variables with only three informative components <span class="math inline">\(w_1, w_2\)</span> and <span class="math inline">\(w_3\)</span>. During the rotation procedure, the response space is also rotated along with the predictor space. Figure <a href="relevant-components.html#fig:cov-plot-print">1</a> shows that the informative response component <span class="math inline">\(w_3\)</span> is rotated together with the uninformative response component <span class="math inline">\(w_4\)</span> so that the predictors which were relevant for <span class="math inline">\(w_3\)</span> will be relevant for response variables <span class="math inline">\(y_3\)</span> and <span class="math inline">\(y_4\)</span>. Similarly, response components <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are rotated separately so that predictors relevant for <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> will only be relevant for <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> respectively, which we can see in Figure~<a href="relevant-components.html#fig:simulated-data">2</a>. In the r-package <em>simrel-m</em>, the combining of the response components is specified by a parameter <code>ypos</code>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="implementation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/therimalaya/simrel-m/edit/master/Includes/02-StatisticalModel.Rmd",
"text": "Edit"
},
"download": ["main.epub", "main.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
