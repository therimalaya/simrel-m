[
["index.html", "A tool for simulating multi-response linear model data Introduction", " A tool for simulating multi-response linear model data Raju Rimal KBMraju.rimal@nmbu.no Trygve Almøy KBMtrygve.almoy@nmbu.no Solve Sæbø NMBUsolve.sabo@nmbu.no 2017-10-31 Abstract Data science is generating enormous amounts of data, and new and advanced analytical methods are constantly being developed to cope with the challenge of extracting information from such “big-data”. Researchers often use simulated data to assess and document the properties of these new methods, and in this paper we present an extension to the R-package simrel, which is a versatile and transparent tool for simulating linear model data with an extensive range of adjustable properties. The method is based on the concept of relevant components (Helland and Almøy 1994), and is equivalent to the envelope model by Dennis Cook. It is a multi-response extension of R-package simrel which is available in R-package repository CRAN, and as simrel the new approach is essentially based on random rotations of latent relevant components to obtain a predictor matrix \\(\\mathbf{X}\\), but in addition we introduce random rotations of latent components spanning a response space in order to obtain a multivariate response matrix \\(\\mathbf{Y}\\). The properties of the linear relation between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are defined by a small set of input parameters which allow versatile and adjustable simulations. Sub-space rotations also allow for generating data suitable for testing variable selection methods in multi-response settings. The method is implemented as an update to the R-package simrel. Introduction Technological advancement has opened a door for complex and sophisticated scientific experiments that was not possible before. Due to this change, enormous amounts of raw data are generated which contain massive information but is difficult to excavate. Finding information and performing scientific research on these raw data has now become another problem. In order to tackle this situation new methods are being developed. However, before implementing any method, it is essential to test its performance and explore its properties. Often, researchers use simulated data for the purpose which itself is a time-consuming process. The main focus of this paper is to present a simulation method, along with an extension to the r-package called simrel, that is versatile in nature and yet simple to use. The simulation method we are presenting here is based on the principle of relevant space for prediction (Helland and Almøy 1994) which assumes that there exists a y-relevant subspace in the complete space of predictor variables that is spanned by a subset of eigenvectors of these predictor variables. Our extension to this principle is to introduce a subspace in \\(\\mathbf{y}\\) (material space) which contains the information that predictor space is relevant for. The concept of response reduction to the material space in response variable was introduced by Cook, Li, and Chiaromonte (2010). Our r-package based on this principle lets the user to specify various population properties such as, which latent components are relevant for a latent subspace of the responses \\(\\mathbf{y}\\) and the collinearity structure of \\(\\mathbf{x}\\). This enables the possibility to construct data for evaluating estimation methods and methods developed for variable selection. Among several publications on simulation, Johnson (2013); Ripley (2009) and Gamerman and Lopes (2006) have exhaustively discussed the topic. In particular, methods based on covariance structure has been discussed by Arteaga and Ferrer (2010); Arteaga and Ferrer (2013) and Camacho (2017), which follows an algorithmic approach to find simulated data satisfying the desired correlation structure. In addition, many publications have implemented simulated data in order to investigate new estimation methods and prediction strategies (see: R. Dennis Cook and Zhang 2015; Cook, Helland, and Su 2013; Helland et al. 2012). However, most of the simulations in these studies were developed to address their specific problem. A systematic tool for simulating linear model data with single response, which could serve as a general tool for all such comparisons, was presented in Sæbø, Almøy, and Helland (2015) and as the r-package simrel. This paper extends simrel in order to simulate linear model data with multivariate response. The github repository of the package at http://github.com/simulatr/simrel has rich documentation with many examples and cases along with detail description on simulation parameters. In the following two section, discussion encircle the mathematical framework behind. In addition, section-?? and ?? have also discussed about the input parameters needed for simrel function in brief. In section ??, an implementation is presented as an example of use-cases and the final section introduce shiny web application for this tool. "],
["statistical-model.html", "Statistical Model", " Statistical Model In this section we describe the model and the model parameterization which is assumed throughout this paper. We assume: \\[\\begin{equation} \\begin{bmatrix}\\mathbf{y}\\\\ \\mathbf{x}\\end{bmatrix} \\sim N \\left( \\begin{bmatrix} \\boldsymbol{\\mu}_y \\\\ \\boldsymbol{\\mu}_x \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma}_{yy} &amp; \\boldsymbol{\\Sigma}_{yx} \\\\ \\boldsymbol{\\Sigma}_{yx}^t &amp; \\boldsymbol{\\Sigma}_{xx} \\end{bmatrix} \\right) \\tag{1} \\end{equation}\\] where, \\(\\mathbf{y}\\) is a response vector with \\(m\\) response variables \\(y_1, y_2, \\ldots y_m\\) with mean vector \\(\\boldsymbol{\\mu}_y\\), and \\(\\mathbf{x}\\) is vector of \\(p\\) predictor variables with mean vector \\(\\boldsymbol{\\mu}_x\\). Further, \\(\\boldsymbol{\\Sigma}_{yy} (m \\times m)\\) is the variance-covariance matrix of \\(\\mathbf{y}\\) \\(\\boldsymbol{\\Sigma}_{xx} (p \\times p)\\) is the variance-covariance matrix of variables \\(\\mathbf{x}\\) \\(\\boldsymbol{\\Sigma}_{yx} (m \\times p)\\) is the matrix of covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) Standard theory in multivariate statistics may be used to show that \\(\\mathbf{y}\\) conditioned on \\(\\mathbf{x}\\) corresponds to the linear model, \\[\\begin{equation} \\mathbf{y} = \\boldsymbol{\\mu}_y + \\boldsymbol{\\beta}^t (\\mathbf{x} - \\boldsymbol{\\mu}_x) + \\boldsymbol{\\varepsilon} \\tag{2} \\end{equation}\\] where, \\(\\boldsymbol{\\beta}^t\\) is a \\((m \\times p)\\) matrix of regression coefficient, and \\(\\boldsymbol{\\varepsilon}\\) is an error term such that \\(\\boldsymbol{\\varepsilon} \\sim N\\left(0, \\boldsymbol{\\Sigma}_{y|x}\\right)\\). The properties of the linear model (2) can be expressed in terms of covariance matrices in (1). Regression Coefficients The matrix of regression coefficients is given by \\[ \\boldsymbol{\\beta} = \\boldsymbol{\\Sigma}_{xx}^{-1}\\boldsymbol{\\Sigma}_{xy}\\] Coefficient of Determination The diagonal elements of the coefficient-of-determination matrix \\(\\boldsymbol{\\rho}_y^2 (m \\times m)\\) gives the amount of variation in each response variable that is explained by \\(\\mathbf{x}\\). \\[\\boldsymbol{\\rho}_y^2 = \\boldsymbol{\\Sigma}_{yx}\\boldsymbol{\\Sigma}_{xx}^{-1}\\boldsymbol{\\Sigma}_{xy}\\boldsymbol{\\Sigma}_{yy}^{-1}\\] Conditional variance The conditional variance-covariance matrix of \\(\\mathbf{y}\\) given \\(\\mathbf{x}\\) is, \\[\\boldsymbol{\\Sigma}_{y|x} = \\boldsymbol{\\Sigma}_{yy} - \\boldsymbol{\\Sigma}_{yx}\\boldsymbol{\\Sigma}_{xx}^{-1}\\boldsymbol{\\Sigma}_{xy}.\\] The diagonal elements of this matrix equals the minimum least square error of prediction \\(\\left[\\mathrm{E}(y - \\hat{y})^2\\right]\\) for each of the response variables. Let us define a transformation of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) as, \\(\\mathbf{z} = \\mathbf{Rx}\\) and \\(\\mathbf{w} = \\mathbf{Qy}\\). Here, \\(\\mathbf{R}_{p\\times p}\\) and \\(\\mathbf{Q}_{m\\times m}\\) are rotation matrices that rotate \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) to yield \\(\\mathbf{z}\\) and \\(\\mathbf{w}\\), respectively. The model (1) can be re-expressed in terms of these transformed variables as: \\[\\begin{align} \\begin{bmatrix}\\mathbf{w} \\\\ \\mathbf{z}\\end{bmatrix} &amp; \\sim N \\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right) = N \\left( \\begin{bmatrix} \\boldsymbol{\\mu}_w \\\\ \\boldsymbol{\\mu}_z \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma}_{ww} &amp; \\boldsymbol{\\Sigma}_{wz} \\\\ \\boldsymbol{\\Sigma}_{zw} &amp; \\boldsymbol{\\Sigma}_{zz} \\end{bmatrix} \\right) \\nonumber \\\\ &amp;= N \\left( \\begin{bmatrix} \\boldsymbol{Q\\mu}_y \\\\ \\boldsymbol{R\\mu}_x \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{Q\\Sigma}_{yy}\\boldsymbol{Q}^t &amp; \\boldsymbol{Q\\Sigma}_{yx}\\mathbf{R}^t \\\\ \\boldsymbol{R\\Sigma}_{xy}\\boldsymbol{Q}^t &amp; \\boldsymbol{R\\Sigma}_{xx}\\mathbf{R}^t \\end{bmatrix} \\right) \\tag{3} \\end{align}\\] In addition, a linear model relating \\(\\mathbf{w}\\) conditioned on \\(\\mathbf{z}\\) can be written as, \\[\\begin{equation} \\mathbf{w} = \\boldsymbol{\\mu}_w + \\boldsymbol{\\alpha}^t \\left(\\mathbf{z} - \\boldsymbol{\\mu}_z\\right) + \\boldsymbol{\\tau} \\tag{4} \\end{equation}\\] where \\(\\boldsymbol{\\alpha}\\) is the regression coefficient vector for the transformed model and \\(\\boldsymbol{\\tau} \\sim N\\left(\\mathbf{0}, \\boldsymbol{\\Sigma}_{w|z}\\right)\\). Further, if both \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) are orthonormal matrices, i.e., \\(\\mathbf{Q}^t\\mathbf{Q} = \\mathbf{I}_q\\) and \\(\\mathbf{R}^t\\mathbf{R} = \\mathbf{I}_p\\), the inverse transformation can be defined as, \\[\\begin{equation} \\begin{matrix} \\boldsymbol{\\Sigma}_{yy} = \\mathbf{Q}^t \\boldsymbol{\\Sigma}_{ww} \\mathbf{Q} &amp; \\boldsymbol{\\Sigma}_{yx} = \\mathbf{Q}^t \\boldsymbol{\\Sigma}_{wz} \\mathbf{R} \\\\ \\boldsymbol{\\Sigma}_{xy} = \\mathbf{R}^t \\boldsymbol{\\Sigma}_{zw} \\mathbf{Q} &amp; \\boldsymbol{\\Sigma}_{xx} = \\mathbf{R}^t \\boldsymbol{\\Sigma}_{zz} \\mathbf{R} \\end{matrix} \\tag{5} \\end{equation}\\] From this, we can find a direct connection between different population properties of (2) and (4). Regression Coefficients : \\[ \\begin{aligned} \\boldsymbol{\\alpha} &amp;= \\boldsymbol{\\Sigma}_{wz} \\boldsymbol{\\Sigma}_{zz}^{-1} = \\boldsymbol{Q\\Sigma}_{yz}\\mathbf{R}^t\\left[\\boldsymbol{R\\Sigma}_{xx}\\mathbf{R}^t\\right]^{-1} = \\mathbf{Q}\\left[\\boldsymbol{\\Sigma}_{yx}\\boldsymbol{\\Sigma}_{xx}^{-1}\\right]\\mathbf{R}^t = \\mathbf{Q}\\boldsymbol{\\beta}\\mathbf{R}^t \\end{aligned} \\] Conditional Variance Further, the conditional variance-covariance matrix of \\(\\mathbf{w}\\) given \\(\\mathbf{z}\\) is, \\[ \\begin{aligned} \\boldsymbol{\\Sigma}_{w|z} &amp;= \\boldsymbol{\\Sigma}_{ww} - \\boldsymbol{\\Sigma}_{wz}\\boldsymbol{\\Sigma}_{zz}^{-1}\\boldsymbol{\\Sigma}_{zw} \\\\ &amp;= \\boldsymbol{Q\\Sigma}_{yy}\\mathbf{Q}^t - \\boldsymbol{Q \\Sigma}_{yx}\\mathbf{R}^t \\left[\\boldsymbol{R\\Sigma}_{xx}\\boldsymbol{R}^t\\right]^{-1} \\boldsymbol{R\\Sigma}_{xy}\\mathbf{Q}^t \\nonumber \\\\ &amp;= \\boldsymbol{Q\\Sigma}_{yy}\\mathbf{Q}^t - \\boldsymbol{Q \\Sigma}_{yx}\\boldsymbol{\\Sigma}_{xx}^{-1}\\boldsymbol{\\Sigma}_{xy}\\mathbf{Q}^t \\\\ &amp;= \\mathbf{Q}\\left[\\boldsymbol{\\Sigma}_{yy} - \\boldsymbol{\\Sigma}_{yx}\\boldsymbol{\\Sigma}_{xx}^{-1}\\boldsymbol{\\Sigma}_{xy}\\right]\\mathbf{Q}^{t} = \\mathbf{Q} \\boldsymbol{\\Sigma}_{y|x}\\mathbf{Q}^t \\end{aligned} \\] Coefficient of Determination The coefficient-of-determination matrix for (4) is, \\[ \\begin{aligned} \\boldsymbol{\\rho}^2_w &amp;= \\boldsymbol{\\Sigma}_{wz} \\boldsymbol{\\Sigma}_{zz}^{-1} \\boldsymbol{\\Sigma}_{zw} \\boldsymbol{\\Sigma}_{ww}^{-1} \\\\ &amp;=\\mathbf{Q}^t \\boldsymbol{\\Sigma}_{yx}\\mathbf{R}^t \\left(\\mathbf{R}\\boldsymbol{\\Sigma}_{xx}\\mathbf{R}^t\\right)^{-1} \\mathbf{R}\\boldsymbol{\\Sigma}_{xy}\\mathbf{Q}^t \\left(\\mathbf{Q} \\boldsymbol{\\Sigma}_{yy}^{-1} \\mathbf{Q}^t\\right) \\nonumber \\\\ &amp;=\\mathbf{Q}^t\\left[\\boldsymbol{\\Sigma}_{yx}\\boldsymbol{\\Sigma}_{xx}\\boldsymbol{\\Sigma}_{xy}\\boldsymbol{\\Sigma}_{yy}^{-1}\\right]\\mathbf{Q} = \\mathbf{Q}\\boldsymbol{\\rho}_{Y}^2 \\mathbf{Q}^t \\end{aligned} \\] From the eigenvalue decomposition principle, if \\(\\boldsymbol{\\Sigma}_{xx} = \\mathbf{R}\\boldsymbol{\\Lambda}\\mathbf{R}^t\\) and \\(\\boldsymbol{\\Sigma}_{yy} = \\mathbf{Q}\\boldsymbol{\\Omega}\\mathbf{Q}^t\\) then \\(\\mathbf{z}\\) and \\(\\mathbf{w}\\) can be interpreted as principal components of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) respectively. In this paper, these principal components will be termed as predictor components and response components respectively. Here, \\(\\boldsymbol{\\Lambda}\\) and \\(\\boldsymbol{\\Omega}\\) are diagonal matrices of eigenvalues of \\(\\boldsymbol{\\Sigma}_{xx}\\) and \\(\\boldsymbol{\\Sigma}_{yy}\\), respectively. "],
["relevant-components.html", "Relevant Components Model Parameterization Rotation of predictor space Rotation of response space", " Relevant Components Consider a single response linear model with \\(p\\) predictors. \\[y = \\mu_y + \\boldsymbol{\\beta}^t\\left(\\mathbf{x} - \\mu_x\\right) + \\epsilon\\] where, \\(\\epsilon \\sim N(0, \\sigma^2)\\) and \\(\\mathbf{x}\\) is a vector of random predictors. Following the concept of relevant space and irrelevant space which is discussed extensively in Helland and Almøy (1994), Helland (2000), Helland et al. (2012), Cook, Helland, and Su (2013), and Sæbø, Almøy, and Helland (2015), we can assume that there exists a subspace of the full predictor space which is relevant for \\(y\\). An orthogonal space to this space does not contain any information about \\(y\\) and is considered as irrelevant. Here, the \\(y-\\)relevant subspace of \\(\\mathbf{x}\\) is spanned by a subset of the principal components defined by the eigenvectors of the covariance matrix of \\(\\mathbf{x}\\), i.e. \\(\\boldsymbol{\\Sigma}_{xx}\\). This concept can be extended to \\(m\\) responses so that the subspace of \\(\\mathbf{x}\\) is relevant for a subspace of \\(\\mathbf{y}\\). This corresponds to the concept of simultaneous envelopes (R. Dennis Cook and Zhang 2015) where relevant (material) and irrelevant (immaterial) space were discussed for both response and predictor variables. Model Parameterization In order to construct a fully specified and unrestricted covariance matrix of \\(\\mathbf{z}\\) and \\(\\mathbf{w}\\) for the model in equation (3), we need to identify \\(1/2 (p+m)(p+m+1)\\) unknown parameters. For the purpose of simulation, we implement some assumptions to re-parameterize and simplify the model. This enables us to construct a wide range of model properties from only few key parameters. Parameterization of \\(\\boldsymbol{\\Sigma}_{zz}\\) If we let the rotation matrix \\(\\mathbf{R}\\) correspond to the eigenvectors of \\(\\boldsymbol{\\Sigma}_{xx}\\), then \\(\\mathbf{z}\\) becomes the set of principal components of \\(\\mathbf{x}\\). In that case \\(\\boldsymbol{\\Sigma}_{zz}\\) is a diagonal matrix with eigenvalues \\(\\lambda_1, \\ldots, \\lambda_p\\). Further, we adopt the same parametric representation as Sæbø, Almøy, and Helland (2015) for these eigenvalues: \\[\\begin{equation} \\lambda_i = e^{-\\gamma(i - 1)}, \\gamma &gt;0 \\text{ and } i = 1, 2, \\ldots, p \\tag{6} \\end{equation}\\] Here, as \\(\\gamma\\) increases, the decline of eigenvalues becomes steeper, hence the parameter \\(\\gamma\\) controls the level of multicollinearity in \\(\\mathbf{x}\\). Hence, we can write \\(\\Sigma_{zz} = \\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_p)\\). Parameterization of \\(\\boldsymbol{\\Sigma}_{ww}\\) In similar manner, a parametric representation of eigenvalues corresponding to \\(\\Sigma_{ww}\\) is adopted as, \\[\\begin{equation} \\kappa_j = e^{-\\eta(j - 1)}, \\eta &gt;0 \\text{ and } j = 1, 2, \\ldots, m \\tag{7} \\end{equation}\\] Here, the decline of eigenvalues becomes steeper as \\(\\eta\\) gets far from zero. At \\(\\eta = 0\\), all \\(w\\) will have equal variance. Hence we can write \\(\\Sigma_{ww} = \\text{diag}(\\kappa_1, \\ldots, \\kappa_m)\\). Parameterization of \\(\\boldsymbol{\\Sigma}_{zw}\\) After parameterization of \\(\\boldsymbol{\\Sigma}_{zz}\\) and \\(\\boldsymbol{\\Sigma}_{ww}\\), we are left with \\(m \\times p\\) number of unknowns corresponding to \\(\\boldsymbol{\\Sigma}_{zw}\\). Some of the elements of \\(\\boldsymbol{\\Sigma}_{zw}\\) may be equal to zero, which implies that the given \\(z\\) is irrelevant for the given variable \\(w\\). The non-zero elements define which of the \\(z\\) are relevant for \\(\\mathbf{w}\\). We typically refer to the indices of these \\(z\\) variables as the positions of relevant components. In order to re-parameterize this covariance matrix, it is necessary to discuss the position of relevant components in detail. Position of relevant components Let \\(k_1\\) components be relevant for \\(w_1\\), \\(k_2\\) components be relevant for \\(w_2\\) and so on. Let the positions of these components be given by the index sets \\(\\mathcal{P}_1, \\mathcal{P}_2, \\ldots, \\mathcal{P}_m\\) respectively. Further, the covariance between \\(w_j\\) and \\(z_i\\) is non-zero only if \\(z_i\\) is relevant for \\(w_j\\). If \\(\\sigma_{ij}\\) is the covariance between \\(w_j\\) and \\(z_i\\) then \\(\\sigma_{ij} \\ne 0\\) if \\(i \\in \\mathcal{P}_j\\) where \\(i = 1, \\ldots, p\\) and \\(j = 1, \\ldots, m\\) and \\(\\sigma_{ij} = 0\\) otherwise. In addition, the true regression coefficients for \\(w_j\\) [ref:(4)] is given by: \\[ \\boldsymbol{\\alpha}_j = \\Lambda^{-1} \\sigma_{ij} = \\sum_{i \\in \\mathcal{P}_j}\\frac{\\sigma_{ij}}{\\lambda_i},\\qquad j = 1, 2, \\ldots m \\] The positions of the relevant components have heavy impact on prediction. Helland and Almøy (1994) have shown that if the relevant components have large eigenvalues (variances), which here implies small index values in \\(\\mathcal{P}_j\\), prediction of \\(\\mathbf{y}\\) from \\(\\mathbf{x}\\) is relatively easy and if the eigenvalues (variances) of relevant components are small, the prediction becomes difficult, given that the coefficient of determination and other model parameters are held constant. For example, if the first and second components, \\(z_1\\) and \\(z_2\\), are relevant for \\(w_1\\) and fifth and sixth components, \\(z_5\\) and \\(z_6\\), are relevant for \\(w_2\\), it is relatively easier to predict \\(w_1\\) than \\(w_2\\), other properties being similar. This might be so, because the first and second principal components have larger variances than the fifth and sixth components. Although the covariance matrix may depend on few relevant components, we can not choose these covariances freely since we also need to satisfy following two conditions: The covariance matrices \\(\\Sigma_{zz}\\), \\(\\Sigma_{ww}\\) and \\(\\Sigma\\) must be positive definite The covariance \\(\\sigma_{ij}\\) must satisfy user defined coefficient of determination We have the relation, \\[\\boldsymbol{\\rho}_w^2 = \\boldsymbol{\\Sigma}_{zw}^t\\boldsymbol{\\Sigma}_{zz}^{-1}\\boldsymbol{\\Sigma}_{zw}\\boldsymbol{\\Sigma}_{ww}^{-1}\\] Applying our assumptions that, \\(\\boldsymbol{\\Sigma_{ww}} = \\text{diag}(\\kappa_1, \\ldots, \\kappa_m)\\) (7) and \\(\\boldsymbol{\\Sigma}_{zz} = \\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_p)\\) (6), we obtain, \\[ \\rho_{w}^2 = \\Sigma_{zw}^t\\Lambda^{-1}\\Sigma_{zw}\\Sigma_{ww}^{-1} = \\begin{bmatrix} \\sum_{i = 1}^p\\frac{\\sigma_{i1}^2}{\\lambda_i\\kappa_1} &amp; \\ldots &amp; \\sum_{i=1}^p\\frac{\\sigma_{i1}\\sigma_{im}}{\\lambda_i\\kappa_1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{i=1}^p\\frac{\\sigma_{i1}\\sigma_{im}}{\\lambda_i\\kappa_m} &amp; \\ldots &amp; \\sum_{i = 1}^p\\frac{\\sigma_{im}^2}{\\lambda_i\\kappa_m} \\end{bmatrix} \\] Furthermore, we assume that there are no overlapping relevant components for any two \\(w\\), i.e, \\(\\mathcal{P}_j \\cap \\mathcal{P}_{j*} = \\varnothing\\) or \\(\\sigma_{ij}\\sigma_{ij*} = 0\\) for \\(j\\ne j*\\). The additional unknown parameters in the diagonal of \\(\\boldsymbol{\\rho}_w^2\\) should agree with user specified coefficients of determination for \\(\\mathbf{w}\\). i.e, \\(\\rho_{wj}^2\\) is, \\[ \\rho_{w_j}^2 = \\sum_{i = 1}^p\\frac{\\sigma_{ij}^2}{\\lambda_i\\kappa_j} \\] Here, only the relevant components have non-zero covariances with \\(w_j\\), so, \\[ \\rho_{w_j}^2 = \\sum_{i \\in \\mathcal{P}_j} \\frac{\\sigma_{ij}^2}{\\lambda_i\\kappa_j} \\] For some user defined \\(\\rho_{jw}^2\\) the \\(\\sigma_{ij}^2\\) is determined as follows, Sample \\(k_j\\) values from a uniform distribution \\(\\mathcal{U}(-1, 1)\\) distribution. Let them be denoted \\(\\mathcal{S}_{\\mathcal{P}_1}, \\ldots, \\mathcal{S}_{\\mathcal{P}_{k_j}}\\). Define, \\[\\sigma_{ij} = \\text{Sign}(\\mathcal{S}_i)\\sqrt{\\frac{\\rho_{wj}^2\\left|\\mathcal{S}_i\\right|}{\\sum_{k\\in\\mathcal{P}_j}\\left|\\mathcal{S}_k\\right|}\\lambda_i\\kappa_j}\\] for \\(i \\in \\mathcal{P}_j\\) and \\(j = 1, \\ldots m\\) Data Simulation From the above given parameterizations and the user defined choices of model parameters, a fully defined and known covariance matrix \\(\\boldsymbol{\\Sigma}\\) of \\((\\mathbf{w, z})\\) is given. For the simulation of a single observation of \\((\\mathbf{w, z})\\) let us define \\(\\mathbf{g} = \\boldsymbol{\\Sigma}^{-1/2}\\mathbf{u}\\) such that \\(\\text{cov}(\\mathbf{g}) = \\boldsymbol{\\Sigma}\\). Here \\(\\boldsymbol{\\Sigma}^{-1/2}\\) is obtained from Choleskey decomposition of \\(\\boldsymbol{\\Sigma}\\), and \\(\\mathbf{u}\\) is simulated from independent standard normal distribution. Similarly, in order to simulate \\(n\\) observations, we define \\(\\underset{n \\times (m + p)}{\\mathbf{G}} = \\mathbf{U}\\boldsymbol{\\Sigma}^{-1/2}\\). Here the first \\(m\\) columns of \\(\\mathbf{G}\\) will serve as \\(\\mathbf{W}\\) and remaining \\(p\\) columns will serve as \\(\\mathbf{Z}\\). Further, each row of \\(\\mathbf{G}\\) will be a vector sampled independently from the joint normal distribution of \\(\\left(\\mathbf{w}, \\mathbf{z}\\right)\\). Finally, these simulated matrices \\(\\mathbf{W}\\) and \\(\\mathbf{Z}\\) are orthogonally rotated in order to obtain \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) respectively. The following section discuss about these rotation matrices in detail. Rotation of predictor space Initially, let us consider an example where a regression model with \\(p = 10\\) predictors \\((\\mathbf{x})\\) and \\(m = 4\\) responses \\((\\mathbf{y})\\). Let’s assume that only three response components \\((w_1, w_2\\) and \\(w_3)\\) are needed to describe all four response variables. Further, let the index sets \\(\\mathcal{P}_1 = \\{1, 2\\}, \\mathcal{P}_2 = \\{3, 4\\}\\) and \\(\\mathcal{P}_3 = \\{5, 6\\}\\) define the positions of the predictor components of \\(\\mathbf{x}\\) that are relevant for \\(w_1, w_2\\) and \\(w_3\\) respectively. Let \\(\\mathcal{S}_1\\), \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) be the orthogonal spaces spanned by each set of predictor components. These spaces together span \\(\\mathcal{S}_k = \\mathcal{S}_1 \\oplus \\mathcal{S}_2 \\oplus \\mathcal{S}_3\\) which is the minimum relevant space and equivalent to the x-envelope as discussed by Cook, Helland, and Su (2013). Moreover, let \\(q_1 = 3, q_2 = 3\\) and \\(q_3 = 2\\) be the number of predictor variables we want to have relevant for \\(w_1, w_2\\) and \\(w_3\\) respectively. Then \\(q_1 = 3\\) predictors may be obtained by rotating the predictor components in \\(\\mathcal{P}_1\\) along with one more irrelevant component. Similarly, \\(q_2 = 3\\) predictors, relevant for \\(w_2\\), can be obtained by rotating predictor components in \\(\\mathcal{P}_2\\) along with one more irrelevant component and finally, \\(q_3 = 2\\) predictors, relevant for \\(w_3\\), can be obtained by rotating the components in \\(\\mathcal{P}_3\\) without any additional irrelevant component. Let the space spanned by the \\(q_1, q_2\\) and \\(q_3\\) number of predictors be \\(\\mathcal{S}_{q_1}\\), \\(\\mathcal{S}_{q_2}\\) and \\(\\mathcal{S}_{q_3}\\). Together they span a space \\(\\mathcal{S}_q = \\mathcal{S}_{q_1} \\oplus \\mathcal{S}_{q_2} \\oplus \\mathcal{S}_{q_3}\\). This space is bigger than \\(\\mathcal{S}_k\\) since in the process two irrelevant components were included in the rotations. Here, \\(\\mathcal{S}_k\\) is orthogonal to \\(\\mathcal{S}_{p - k}\\) and \\(\\mathcal{S}_q\\) is orthogonal to \\(\\mathcal{S}_{p - q}\\). Generally speaking, here we are splitting the complete variable space \\(\\mathcal{S}_p\\) into two orthogonal spaces – \\(\\mathcal{S}_k\\) relevant for \\(\\mathbf{w}\\) and \\(\\mathcal{S}_{p - k}\\) irrelevant for \\(\\mathbf{w}\\). In the previous section, we discussed about the construction of a covariance matrix for the latent structure. Figure 1(a) shows a similar structure resembling the example here. The three colors represent the relevance with the three latent response components \\((w_1, w_2\\) and \\(w_3)\\). Here we can see that \\(z_{1}\\) and \\(z_{2}\\) (first and second predictor components of \\(\\mathbf{x}\\)) have non-zero covariance with \\(w_1\\) (first latent component of response \\(\\mathbf{y}\\)). In the similar manner other non-zero covariances are self-explanatory. Figure 1: Simulation of predictor and response variables after orthogonal transformation of predictor and response components by rotation matrices \\(Q\\) and \\(R\\) shown as the upper left and the lower right block matrices in (b). In order to simulate predictor variables \\((\\mathbf{x})\\), we construct matrix \\(\\mathbf{R}\\) which then is used for orthogonal rotation of the predictor components \\(\\mathbf{z}\\). This defines a new basis for the same space as is spanned by the predictor components. In principle, there are many possible options for defining a rotation matrix. Among them, the eigenvector matrix of \\(\\boldsymbol{\\Sigma}_{xx}\\) can be a candidate. However, in this reverse engineering approach both rotation matrices \\(\\mathbf{R}\\) and \\(\\mathbf{Q}\\) along with the covariance matrices \\(\\boldsymbol{\\Sigma}_{xx}\\) are unknown. So, we are free to choose any \\(\\mathbf{R}\\) that satisfied the properties of a real valued rotation matrix, i.e \\(\\mathbf{R}^{-1} = \\mathbf{R}^t\\) so that \\(\\mathbf{R}\\) is orthonormal. Here the rotation matrix \\(\\mathbf{R}\\) should be block diagonal as in Figure 1(b) in order to rotate spaces \\(\\mathcal{S}_1, \\mathcal{S}_2 \\ldots\\) separately. Figure 2(a) shows the simulated predictor components \\(\\mathbf{z}\\) that we are following in our example where we can see that the components \\(z_{1}\\) and \\(z_{2}\\) (relevant for \\(w_1\\)) is getting rotated together with an irrelevant component \\(z_{8}\\). The resultant predictors (Figure 2(b)) \\(x_{1}, x_{2}\\) and \\(x_{8}\\) will hence also be relevant for \\(w_1\\). In the figure, we can see that components \\(z_{7}, z_{8}, z_{9}\\) and \\(z_{10}\\) are not relevant for any responses before rotation, however, the \\(x_{8}, x_{10}\\) predictors become relevant after rotation keeping \\(x_{7}\\) and \\(x_{9}\\) still irrelevant. Figure 2: Simulated Data before Among several methods (Anderson, Olkin, and Underhill 1987; Heiberger 1978) for generating random orthogonal matrix, in this paper we are using orthogonal matrix \\(\\mathcal{Q}\\) obtained from QR-decomposition of a matrix filled with standard normal variate. The rotation here can be a) restricted and b) unrestricted. The latter rotates all components \\(\\mathbf{z}\\) together and makes all predictor variables somewhat relevant for all response components. However, the former performs a block-wise rotation so that it rotates certain selected predictor components together. This gives control for specifying certain predictors as relevant for selected responses, which was discussed in our example above. This also allows us to simulate irrelevant predictors such as \\(x_{7}\\) and \\(x_{9}\\) which can be detected during variable selection procedures. Rotation of response space The previous example has four response variables with only three informative components \\(w_1, w_2\\) and \\(w_3\\). During the rotation procedure, the response space is also rotated along with the predictor space. Figure 1 shows that the informative response component \\(w_3\\) is rotated together with the uninformative response component \\(w_4\\) so that the predictors which were relevant for \\(w_3\\) will be relevant for response variables \\(y_3\\) and \\(y_4\\). Similarly, response components \\(w_1\\) and \\(w_2\\) are rotated separately so that predictors relevant for \\(w_1\\) and \\(w_2\\) will only be relevant for \\(y_1\\) and \\(y_2\\) respectively, which we can see in Figure~2. In the r-package simrel-m, the combining of the response components is specified by a parameter ypos. "],
["implementation.html", "Implementation", " Implementation This section demonstrates an application of multi-response extension of simrel in order to compare different estimation methods on the basis of prediction error. For the comparison, we have considered four well established estimation methods. Ordinary Least Squares (OLS), Principal Component Regression (PCR), Partial Least Squares predicting individual response variable separately (PLS1) and Partial Least Squares predicting all response variables together (PLS2). We have also considered four relatively new estimation methods in multi-response regression: Canonically Powered Partial Least Squares regression (CPPLS) (Indahl, Liland, and Næs 2009), Canonical Partial Least Squares regression (CPLS) (Indahl, Liland, and Næs 2009), Envelope estimation in predictor space (Xenv) (Cook, Li, and Chiaromonte 2010), Envelope estimation in response space (Yenv) (R Dennis Cook and Zhang 2015) and Simultaneous estimation of x- and y-envelope (Senv) (R. Dennis Cook and Zhang 2015) From the possible combinations of two levels of coefficient of determination \\((R^2)\\) and two levels of \\(\\gamma\\) (6) (the factor that controls the multicollinearity in predictor variables), four simulation designs (design 1-4) were prepared. Replicating each design 20 times, 80 datasets with five response variables \\((m=5)\\) and 16 predictor variables \\((p = 16)\\) were simulated using the method discussed in this paper. It was also assumed that three principle components of response variables (\\(w_1, w_2\\) and \\(w_3\\)) completely describe the variation present in five response variables (\\(y_1 \\ldots y_5\\)). Here, in this example we have assumed that all \\(w\\)’s have equal variance, i.e. \\(\\Sigma_{ww} = \\mathbf{I}_m\\). The four designs are presented in Table~1. All datasets contained 100 sampled observations and out of 16 predictor variables, three disjoint set of five predictor variables are relevant for response components \\(w_1, w_2\\) and \\(w_3\\). Although the simulation method is well equipped to simulate data with \\(p \\gg n\\), for incorporating envelope estimation methods which are based on maximization of likelihood, we have chosen \\(n &gt; p\\) situation in the example. Further, predictor components \\(z_1\\) and \\(z_6\\) were relevant for response component \\(w_1\\), predictor components \\(z_2\\) and \\(z_5\\) were relevant for response component \\(w_2\\) and predictor component \\(z_3\\) and \\(z_4\\) was relevant for response component \\(w_3\\). In addition, following the discussion about rotation of response space, \\(w_1\\) was rotated together with \\(w_4\\) and \\(w_2\\) was rotated together with \\(w_5\\). Figure 3 visualize the covariance structure and relationship between the response and predictor variables for first design. Figure 3: Simulation of predictor and response variables for design one after orthogonal transformation of predictor and response components by rotation matrices \\(Q\\) and \\(R\\) shown as the upper left and the lower right block matrices in (b). Here (a) is the covariance structure of latent space which is rotated by the block diagonal rotation matrix in (b) resulting the covariance structure of simulated data in (c). Table 1: Parameter setting of simulated data for comparison of estimation methods Design1 Design2 Design3 Design4 Decay of eigenvalues \\((\\gamma)\\) 0.2 0.8 0.2 0.8 Coef. of Determination \\((\\rho^2_{w_j})\\) 0.8, 0.8, 0.4 0.8, 0.8, 0.4 0.4, 0.4, 0.4 0.4, 0.4, 0.4 For each method, an estimate of expected squared prediction error was computed as, \\[\\underset{m \\times m}{\\boldsymbol{\\vartheta}} = \\mathrm{E}\\left[\\left( \\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta} \\right) ^t \\boldsymbol{\\Sigma}_{xx} \\left( \\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta} \\right)\\right] + \\boldsymbol{\\Sigma}_{y|x}\\] where, \\(\\hat{\\boldsymbol{\\beta}}\\) is an estimate of true regression coefficient \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\Sigma}_{xx}\\) is the true covariance structure of the predictor variables obtained from simrel. Also, \\(\\boldsymbol{\\Sigma}_{y|x}\\) is the true minimum error of the model. Here \\(\\hat{\\boldsymbol{\\beta}}\\) varies across different estimation methods while the remaining terms are same for each dataset design. Further, an overall prediction error of all responses is measured by the trace of \\(\\boldsymbol{\\vartheta}\\). The minimum prediction error (measured as discussed above) for nine estimation methods averaged over 20 replications of four designs are shown in Table 2. The table also gives the number of predictor components (response components in case of Yenv), a method has used in order to obtain the minimum of average prediction error. Table 2: Minimum average prediction error (number of components corresponding to minimum prediction error, minimum prediction error) (For \\(Y\\text{env}\\), the number of response components is given) Model Design: 1 Design: 2 Design: 3 Design: 4 CPLS (3, 3.24) (4, 3.22) (3, 4.09) (3, 4.05) CPPLS (3, 3.21) (3, 3.17) (3, 4.11) (3, 4.04) OLS (1, 3.60) (1, 3.58) (1, 4.57) (1, 4.50) PCR (7, 3.28) (6, 3.19) (6, 4.08) (6, 4.04) PLS1 (2, 3.32) (5, 3.20) (1, 4.16) (5, 4.07) PLS2 (5, 3.29) (6, 3.19) (3, 4.11) (6, 4.06) Senv (4, 3.17) (5, 3.14) (3, 4.35) (5, 4.28) Xenv (5, 3.23) (6, 3.20) (5, 4.10) (6, 4.11) Yenv (3, 3.24) (3, 3.23) (3, 4.29) (3, 4.24) Table 2 shows that the simultaneous envelope has prediction error of 3.17 and 3.14 in design 1 (with 4 components) and design 2 (with 5 components), respectively, which is smaller than other methods. However the method was not able to show the same performance in design 3 and design 4. The PCR model has the smallest prediction error (4.08) from 6 components in design 3 and Canonically Powered PLS has minimum prediction error (4.04) from 3 components in design 4. In design 3, we can also see that the Canonical PLS method has second best performance with only three components. The number of components vary across different replicated dataset but the component corresponding to minimum prediction error is discussed here. A detailed picture of prediction error for each estimation method obtained for each additional component is shown in Figure 4. Although designs 2 and 4 have higher levels of multicollinearity, the performance of the estimation methods is indifferent to its effect. Since all the methods, except OLS, are based on shrinking of estimates, they are less influenced by the multicollinearity problem. Figure 4: Minimum of Average Prediction Error The analysis presented in Figure 4 has addressed some questions such as how methods work when there exist a true reduced dimension in response space, but also arose other questions like why they perform differently. For example, what is the reason for the decreasing relative performance of the simultaneous envelope method as the \\(\\rho^2\\) values are reduced? Does this depend on the dimensions and shape of the \\(\\mathbf{y}\\) envelopes? Since, the example is merely intended as a demonstration of how simrel can be used in scientific study, a more elaborative studies would be necessary to answer such questions, but for this purpose simrel would be a powerful tool. "],
["web-interface.html", "Web Interface", " Web Interface In order to give an alternative interface for simrel, we have created a shiny app which allows users to input the simulation parameters through different input fields. Figure 5 shows a screenshot of the application. The application contains three main sections through which the user can interact with this simulation approach. A random seed can be selected using section Figure 5 (a) so that a particular set of data can be re-simulated if needed. Figure 5 (b) has all the input panels where the user dependent parameters for simulation can be entered. Here the user also has the option to simulate univariate, bivariate and multivariate simulation. In addition, a simulated R-object comprised by the simulated data can be download as Rdata format (section (e) in Figure 5). The object holds the simulated data along with other properties such as coefficient of determination for each response, true regression coefficients and rotation matrices. Users can also download simulated data in JSON and CSV format. All simrel parameters can be entered using a simple user interface where a vectors are separated with comma(,) and lists are separated with semicolon(;). For instance, the relevant position discussed in the implementation section of this paper can be entered as 1, 6; 2, 5; 3, 4 which is equivalent to R syntax list(c(1, 6), c(2, 5), c(3, 4)). An R expression equivalent to the input parameters as shown in Figure - 5(b) can be written as, simrel( n = 200, # Number of training observations ntest = 50, # Number of test observations p = 15, # Number of predictor variables q = c(5, 4), # Number of relevant predictors relpos = list(c(1, 2), c(3, 4, 6)), # Relevant predictor components R2 = c(0.8, 0.7), # Rsq for each response components m = 4, # Number of response variables gamma = 0.6, # Decay factor of eigenvalues of predictors eta = 0, # Decay factor of eigenvalues of responses ypos = list(c(1, 3), c(2, 4)), # Combination of response components on rotation type = &quot;multivariate&quot; ) Figure 5: Web interface of shiny application of simrel: (a) Buttons to trigger simulation, (b) Parameters for simulation, (c) Visualization of the true properties of simulated data (regression coefficients, true and estimated covariance between response and predictors components) (d) Additional analysis (e) Download option of simulated data. With the parameters for simulation in the screenshot (Figure-5) 200 training sets (n) and 50 test sets (ntest) will be simulated with 15 predictor variables (p) and 4 response variables (m). The 4 response variables will have 2 true latent dimension which is referred as response components. The first response component is rotated together with third (uninformative) response component and second response component is rotated together with fourth (uninformative) response components (ypos). Out of 15 predictors, 5 will be relevant for first response component and 4 will be relevant for second response component (q). The relevant principal components of the predictor variables are predictor components. The 5 predictor variables which are relevant for first response components span the same space as the predictor components at position 1 and 2. Similarly, the 4 predictor variables which are relevant for second response components span the same space as the predictor components at position 3, 4 and 6 (relpos). The coefficient of determination for first response component is 0.8 and second response component is 0.7 (R2). The eigenvalues of predictor components decay exponentially by the factor of 0.6 (gamma) but the eigenvalues of response components are constant (but can be set to exponential decay) (eta). The application not only allows users to simulate data, but also gives some insight into simulated data properties. Section (c) in Figure 5 contains three plots – a) true regression coefficients b) relevant components and c) estimated relevant components. In the first plot (Figure-5(c) top) we can see that predictor variables (1, 2, 8, 9 and 13) are relevant for the first and third response variable (red and blue line) by their non-zero coefficients, whereas predictor variables (3, 4, 6 and 15) are relevant for the second and fourth response variables (purple and green line). The second plot (Figure 5(c) middle) shows the covariances between the response components and the predictor components along with the corresponding eigenvalues in the background (bar plot). In the plot the absolute value of the covariances after scaling with the largest covariance are shown. As in our parameter setting, the plot shows that first (red line) and second (green line) predictor components have non-zero covariance with first response component and third, fourth and sixth predictor components have non-zero covariance with second response component. The third plot (Figure-5(c) bottom) is the estimated covariance between predictor components and the response variables, for the simulated data. Since the first and third response components are rotated together, in the plot, the covariance between predictor components and first and third response variables (red and blue line) are following similar pattern. This also suggests that the predictor components which were relevant for first response component gets relevant for first and third response variables after rotation. Along with these main sections, section (d) in the same figure contains additional analysis performed with the simulated data such as its estimation with different methods. This section is intended for educational purposes to show how changing the data properties influences the performances of different estimation and prediction methods. Beside this application, for Rstudio users, a gadget will be available after installing the r-package. This gadget provides an interface enabling users to input simulation parameters and access some of the properties. Many scientific studies (Helland et al. 2012; Sæbø et al. 2008; R. Dennis Cook and Zhang 2015) are using simulated data in order to compare their findings with others or assess its properties. In many of these situations, a user-friendly and versatile simulation tool like simrel can play an important role. Gangsei, Almøy, and Sæbø (2016) and Sæbø, Almøy, and Helland (2015) are some examples where the univariate and bivariate form of simrel have been used for such purposes. "],
["conclusion.html", "Conclusion", " Conclusion Whether comparing methods or assessing and understanding properties of any methods, tools or procedure; simulated data allows controlled tests for researchers. However, researchers spend enormous amount of time for creating such simulation tools so that they can obtain particular nature of data. We believe that this tool along with R-package and the easy to use shiny web interface will become an assistive tool for researchers in this respect. "],
["references.html", "References", " References "]
]
