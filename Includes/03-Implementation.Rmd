# Implementation {#implementation}

```{r KnitrSetup, include=FALSE}
knitr::opts_chunk$set(
  comment = NA,
  message = FALSE,
  warning = FALSE,
  echo    = FALSE
)
```

```{r LoadingPackages}
pkgs <- c("pls", "envlp", "simulatr", "data.table", "pander", "magrittr", "purrr", "ggplot2")
for (pkg in pkgs) 
  suppressPackageStartupMessages(require(pkg, quietly = T, 
                                         warn.conflicts = F, 
                                         character.only = T))
```

```{r SourceScript}
source("functions.R")
```

## Example of model comparison with simulated data from `simrel-m` package {#example}

In this section, `simrel-m` is implemented to simulate multi-response linear model dataset and use it to compare Principal Components Regression (PCR), Partial Least Squared Regression (PLS), Cannonically Powered Partial Least Squared Regression (CPPLS), Maximum Likelihood under Envelope Estimation and Ordinary Least Squared Regression (OLS) on the basis of their prediction ability of test observations. Here two design of parameters as in Table-\@ref(tab:parameter-settings) are implemented for the simulation.

```{r MakingDesign, results='hide'}
set.seed(225)
opts <- list(
  n = c(100, 50, 200, 20),
  p = c(20, 40, 50, 10),
  q = c("5, 5, 7", 
        "15, 15, 9",
        "12, 15, 8",
        "3, 4"),
  m = c(6, 5, 5, 3),
  relpos = c("1, 2; 3, 4, 6; 5",
             "1, 2; 3, 4; 5, 7",
             "1, 2, 3; 4, 6; 5, 7, 9",
             "1; 2, 3"),
  gamma = c(0.9, 0.4, 0.7, 0.6),
  R2 = c("0.8, 0.9, 0.7",
         "0.8, 0.9, 0.5",
         "0.7, 0.9, 0.8",
         "0.6, 0.8"),
  ypos = c("1, 4; 2, 6; 3, 5",
           "1, 4; 2, 5; 3",
           "1; 3, 4; 2, 5",
           "1, 3; 2"),
  ntest = rep(1000, 4)
)
design <- as.data.table(.prepare_design(opts))
design[, type := "multivariate"]
mdls <- c("OLS", "PCR", "PLS", "Envelope", "CPPLS")
```


```{r parameter-settings}
design_table <- t(design) %>% 
  as.data.table(keep.rownames = TRUE) %>% 
  setnames(c("parameters", paste0("Design", 1:4)))
design_desc <- c(
  "Number of observations",
  "Number of predictors",
  "Relevant number of predictors",
  "Number of responses",
  "Position of relevant components",
  "Gamma (decay of eigenvalues)",
  "Coefficient of Determination",
  "Position of response components",
  "Number of test observations"
)
design_table <- cbind(Parameters = design_desc, design_table[-.N, -"parameters"])
knitr::kable(design_table, align = "rrr",
             caption = "Parameter setting of simulated data for model comparison",
             booktabs = TRUE)
```

Here, the first design has large number of observations as compared to the number of variables while the number of variables in second design is nearly equals to its number of observations. Both the models have three informative response components from which first design generates `r opts[['m']][1]` responses and the second design generates  `r opts[['m']][2]` responses. In addition, the eigenvalues of predictors decreases sharply in the first design than the second one. The prediction error are compared on the basis of 1000 test samples.

Prediction error is measured as mean squared error of prediction (MSEP) using the expression in equation~\@ref(eq:msep).


\begin{equation}
\text{MSEP}_\text{train} = \frac{\mathbf{Y}_\text{train} - 
  \hat{\mathbf{Y}}_\text{train}}{n_\text{train}} \nonumber \text{ and }
\text{MSEP}_\text{test}  = \frac{\mathbf{Y}_\text{test} - 
  \hat{\mathbf{Y}}_\text{test}}{n_\text{test}}
(\#eq:msep)
\end{equation}

where, $\hat{\mathbf{Y}}_\text{train} = \mathbf{X}_\text{train} \boldsymbol{\beta}$ and $\hat{\mathbf{Y}}_\text{test} = \mathbf{X}_\text{test} \boldsymbol{\beta}$

```{r Simulation, results='hide'}
load_if_not("sim_obj", expression = {
  set.seed(7777)
  design %>% 
    apply(1, function(x) do.call(simulatr, x)) %>% 
    data.table %>% 
    setnames("sim_obj") %>% 
    cbind(design)
})
sim_obj[, c("Train", "Test") := list(
  lapply(sim_obj, function(obj) {
    data.frame(x = I(obj$X), y = I(obj$Y))
  }),
  lapply(sim_obj, function(obj) {
    data.frame(x = I(obj$testX), y = I(obj$testY))
  })
)]

## ---- Writing Matlab file ----------------------------------------
R.matlab::writeMat(
  "sim-data.mat", 
  Train1 = I(sim_obj[1, Train][[1]]),
  Test1 = I(sim_obj[1, Test][[1]]),
  Train2 = I(sim_obj[2, Train][[1]]),
  Test2 = I(sim_obj[2, Test][[1]]),
  Train3 = I(sim_obj[3, Train][[1]]),
  Test3 = I(sim_obj[3, Test][[1]]),
  Train4 = I(sim_obj[4, Train][[1]]),
  Test4 = I(sim_obj[4, Test][[1]])
)
```

```{r, fitSimulEnv}
# matlabr::run_matlab_script('FitSimulEnv.m') #! Important Don't delete it
senv.fit <- R.matlab::readMat("sim-fit-obj.mat")
senv <- data.table(senvelope = lapply(senv.fit, function(x) {
  out <- list(
    beta = x["beta", , ],
    Gyhat = x["Gyhat", , ],
    Gxhat = x["Gxhat", , ]
  )
  out <- lapply(out, `dimnames<-`, 
                list(paste0("compX", 1:10), 
                     paste0("compY", 1:4)))
}))
```


```{r ModelFitting, results='hide'}
fit <- copy(sim_obj[1:2])
fit <- fit[, .(sim_obj = sim_obj)][, design := 1:.N]
fit[, ols := lapply(sim_obj, fit_ols)]
fit[, pcr := lapply(sim_obj, fit_mvr, mvr_fun = "pcr", ncomp = 10)]
fit[, pls := lapply(sim_obj, fit_mvr, mvr_fun = "plsr", ncomp = 10)]
fit[, cppls := lapply(sim_obj, fit_mvr, mvr_fun = "cppls", ncomp = 10)]
fit[, xenv := lapply(sim_obj, fit_env)]
fit[, yenv := lapply(sim_obj, fit_env, type = "y", ncomp = 3)]
fit[, senvelope := senv]
```


```{r PredictionError, results='hide'}
mdls <- names(fit)[-c(1:2)]
pred_error  <- map_df(mdls, function(mdl){
  fit[, map2_df(.SD[["sim_obj"]], .SD[[mdl]], ~get_pred_error(.x, .y, mdl)), 
      by = design, .SDcols = c("sim_obj", mdl)]
}, .id = "Model")
pred_error[, Model := mdls[as.numeric(Model)]]
pred_error[, comp := as.numeric(gsub('[a-zA-Z]', '', comp))]
```

```{r PredictionErrorPropOLS}
pe.wide <- dcast(pred_error, design + comp ~ Model, value.var = 'pred_err') %>% 
  .[, ols := ols[1], by = design]
predProp <- pe.wide[, .SD/pls, by = .(design, comp)] %>% 
  melt(1:2, variable.name = "Methods", value.name = "PropPredError") %>% 
  na.omit()
```

## Comparison of Estimation Methods based on Prediction error
Figure-\@ref(fig:prediction-error-plot) shows the performance of different models with components 1 to 10 ...

```{r prediction-error-plot, fig.cap='Model comparison based on test prediction error', fig.asp=0.4, out.width="100%", fig.pos='H'}
ggplot(pred_error[pred_err <= 2.5], aes(comp, pred_err, color = Model)) +
  geom_line(aes(group = Model)) +
  geom_point(size = 1) +
  facet_grid(.~design, labeller = label_both) +
  scale_x_continuous(breaks = 1:10) +
  labs(x = 'Number of Components', y = expression(paste('||', alpha, '||'[F])))
```

Some more text and more and more