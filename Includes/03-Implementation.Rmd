# Implementation {#implementation}

```{r KnitrSetup, include=FALSE}
knitr::opts_chunk$set(
  comment = NA,
  message = FALSE,
  warning = FALSE,
  echo    = FALSE
)
```

```{r LoadingPackages}
pkgs <- c("pls", "envlp", "data.table", "ggplot2", "purrr", "abind", "simulatr")
for (pkg in pkgs) require(pkg, quietly = T, warn.conflicts = F, character.only = T)
```

```{r SourceScript}
source("functions.R")
```

## Example of model comparison with simulated data from {\tt simrel-m package} {#example}

In this section, `simrel-m` is implemented to simulate multi-response linear model dataset and use it to compare Principal Components Regression (PCR), Partial Least Squared Regression (PLS), Cannonically Powered Partial Least Squared Regression (CPPLS), Maximum Likelihood under Envelope Estimation and Ordinary Least Squared Regression (OLS) on the basis of their prediction ability of test observations. Here two design of parameters as in Table-\@ref(tab:parameter-settings) are implemented for the simulation.

```{r MakingDesign}
set.seed(225)
opts <- list(
  n = c(100, 50),
  p = c(20, 40),
  q = c("5, 5, 7", "15, 15, 9"),
  m = c(6, 5),
  relpos = c("1, 2; 3, 4, 6; 5",
             "1, 2; 3, 4; 5, 7"),
  gamma = c(0.9, 0.4),
  R2 = c("0.8, 0.9, 0.7",
         "0.8, 0.9, 0.5"),
  ypos = c("1, 4; 2, 6; 3, 5",
           "1, 4; 2, 5; 3"),
  ntest = rep(1000, 2)
)
design <- as.data.table(.prepare_design(opts))
mdls <- c("OLS", "PCR", "PLS", "Envelope", "CPPLS")
```


```{r parameter-settings}
design_table <- setnames(as.data.table(transpose(opts)),
                         paste0("V", 1:2), paste0("Design", 1:2))[]
design_desc <- c(
  "Number of observations",
  "Number of predictors",
  "Relevant number of predictors",
  "Number of responses",
  "Position of relevant components",
  "Gamma (decay of eigenvalues)",
  "Coefficient of Determination",
  "Position of response components",
  "Number of test observations"
)
knitr::kable(data.frame(Parameters = design_desc, design_table), align = "rcc",
             caption = "Parameter setting of simulated data for model comparison",
             booktabs = TRUE)
```

Here, the first design has large number of observations as compared to the number of variables while the number of variables in second design is nearly equals to its number of observations. Both the models have three informative response components from which first design generates `r opts[['m']][1]` responses and the second design generates  `r opts[['m']][2]` responses. In addition, the eigenvalues of predictors decreases sharply in the first design than the second one. The prediction error are compared on the basis of 1000 test samples.

Prediction error is measured as mean squared error of prediction (MSEP) using the expression in equation~\@ref(eq:msep).

\begin{align}
  \text{MSEP}_\text{train} &= \frac{\mathbf{Y}_\text{train} - 
    \hat{\mathbf{Y}}_\text{train}}{n_\text{train}} \nonumber \\
  \text{MSEP}_\text{test}  &= \frac{\mathbf{Y}_\text{test} - 
    \hat{\mathbf{Y}}_\text{test}}{n_\text{test}}
  (\#eq:msep)
\end{align}

where, $\hat{\mathbf{Y}}_\text{train} = \mathbf{X}_\text{train} \boldsymbol{\beta}$ and $\hat{\mathbf{Y}}_\text{test} = \mathbf{X}_\text{test} \boldsymbol{\beta}$

```{r Simulation, results='hide'}
fit_obj <- design[, .(sim_obj = apply(.SD, 1, function(x){
  x$type <- "multivariate"
  do.call(simulatr, x)
})), by = .I]
fit_obj[, c("Train", "Test") := list(
  lapply(sim_obj, function(obj) {
    data.frame(x = I(obj$X), y = I(obj$Y))
  }),
  lapply(sim_obj, function(obj) {
    data.frame(x = I(obj$testX), y = I(obj$testY))
  })
)]
```

```{r ModelFitting, results='hide'}
fit_obj[, (mdls) := sapply(mdls, function(mdl){
  list(lapply(Train, fit_me, tolower(mdl), ncomp = 15))
})]
```

```{r PredictionError, results='hide'}
pred_err <- map_df(1:nrow(fit_obj), function(row){
  map_df(mdls, function(mdl){
    out <- fit_obj[row, c("Train", "Test", mdl), with = F]
    out[, do.call(get_pred_error, t(.SD))]
  }, .id = "Model")
}, .id = "Design")
pred_err[, Model := mdls[as.numeric(Model)]]
pred_err[, Design := as.numeric(Design)]
pred_err <- melt(pred_err, c(1:3, 6),
                 variable.name = "Estimate",
                 value.name = "MSEP")
setnames(pred_err, "response", "Response")
pred_err[, Estimate := gsub("^t", "T", Estimate)]
```

```{r pred-plot-fun}
getPredPlot <- function(dgn, ymin = 0, ymax = 1.5){
  dta <- pred_err[Design == dgn]
  ggplot(dta[Model != "OLS"], aes(ncomp, MSEP, color = Model)) +
    facet_grid(Estimate ~ Response, labeller = label_both) +
    geom_line() + geom_point(size = 0.5, shape = 4) +
    theme(legend.position = "top",
          panel.grid.minor = element_blank()) +
    coord_cartesian(xlim = c(0, 15), ylim = c(ymin, ymax)) +
    scale_x_continuous(breaks = seq(0, 15, 3)) +
    labs(x = "Number of Components",
         y = "Mean square error of prediction") +
    geom_hline(data = dta[Model == "OLS" & ncomp != 0],
               aes(yintercept = MSEP, color = Model)) +
    scale_color_brewer("Model", palette = "Set1")
}
```

```{r pred-plot-1, fig.height = 4, fig.cap = "Model comparison based on prediction error for each response.", fig.width = 7, fig.pos = 'H', fig.fullwidth=TRUE}
getPredPlot(1, ymin = 0.3, ymax = 1.2)
```


```{r pred-plot-2, fig.height = 4, fig.cap = "Train and test prediction error for different response comparing models under comparison", fig.width = 7, fig.pos = 'H', fig.fullwidth=TRUE}
getPredPlot(2)
```
