# Implementation #

This section demonstrates an application of `simrel-m` in order to compare different estimation methods on the basis of prediction error. For the comparison, we have considered four well established estimation methods.

a) Ordinary Least Squares (OLS),
b) Principal Component Regression (PCR),
c) Partial Least Squares predicting individual response variable separately (PLS1) and
d) Partial Least Squares predicting all response variables together (PLS).

We have also considered four relatively new estimation methods

a) Canonically Powered Partial Least Squares regression (CPPLS) [@indahl2009canonical],
b) Canonical Partial Least Squares regression (CPLS) [@indahl2009canonical],
c) Envelope estimation in predictor space (xenv) [@cook2010envelope],
d) Envelope estimation in response space (yenv) [@cook2015foundations] and
e) Simultaneous estimation of x- and y-envelope (senv) [@cook2015simultaneous]

From the possible combinations of two levels of coefficient of determination $(R^2)$ and two levels of `gamma` (factor that controls the multicollinearity in predictor variable), four simulation designs (`r paste0('design ', 1:4)`) are prepared. Replicating each design 20 times, 80 datasets with five response variables $(m=5)$ and 16 predictor variables $(p = 16)$ are simulated using the method discussed in this paper. It is also assumed that three principle components of response variables ($w_1, w_2$ and $w_3$) completely describes the variation present in five response variables ($y_1 \ldots y_5$). The four designs are presented in the Table~\@ref(tab:parameter-settings). All datasets contains 100 sample observations and out of 16 predictor variables, three disjoint set of five predictor variables are relevant for response components $w_1, w_2$ and $w_3$. Further, predictor components $z_1$ and $z_6$ are relevant for response component $w_1$, predictor components $z_2$ and $z_5$ are relevant for response component $w_2$ and predictor component $z_3$ is relevant for response component $w_3$. In addition, following the discussion about [rotation of response space](#rotation-of-response-space), $w_1$ is rotated together with $w_4$ and $w_2$ is rotated together with $w_5$.

```{r KnitrSetup, include=FALSE}
knitr::opts_chunk$set(
  comment = NA,
  message = FALSE,
  warning = FALSE,
  echo    = FALSE,
  fig.pos = 'H'
)
evl <- !file.exists('scripts/output/pred-err.rdata')
source("scripts/01-setup.r")
if (evl) {
  source("08-final-data.r") 
} else {
  load('output/pred-err.rdata')
  setwd(owd)
}
```

```{r design-table}
design_table <- do.call(rbind, opts[c("gamma", "R2")])
dimnames(design_table) <- list(
  c("Decay of eigenvalue $(\\gamma)$",
    "Coef. of Determination $(\\rho^2)$"),
  paste0("Design", 1:4))
```

```{r design-table-print}
pander::pander(
  design_table, type = "rmarkdown", 
  split.cells = c(40, rep(30, 4)), split.tables = Inf,
  justify = paste(rep("r", ncol(design_table) + 1), collapse = ""),
  caption = "(\\#tab:parameter-settings) Parameter setting of simulated data for model comparison"
)
```

For each method, an estimate of test prediction error is computed as,

$$\underset{m \times m}{\boldsymbol{\alpha}} = 
\left(
\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}
\right) ^t \boldsymbol{\Sigma}_{xx}
\left(
\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}
\right) + \boldsymbol{\sigma}^2$$

where, $\hat{\boldsymbol{\beta}}$ is an estimate of true regression coefficient $\boldsymbol{\beta}$ and $\boldsymbol{\Sigma}_{xx}$ is the true covariance structure of predictor variable obtained from `simrel-m`. Also, $\boldsymbol{\sigma}^2$ is the true minimum error of the model. Here $\hat{\beta}$ vary accross different estimation methods while the remaining terms are same for each dataset design. Further, an overall prediction error of all responses is measured by the Forbenius norm $\left\lVert\alpha\right\rVert_F \in \mathbb{R}^{m \times m}$ defined as [@golub2012matrix], 

$$\left\lVert\alpha\right\rVert_F = \sqrt{\sum_{i = 1}^m \sum_{j = 1}^m {\left|a_{ij}\right|^2}}.$$

The minimimum prediction error (measured as discussed above) for nine estimation methods averaged over 20 replications of four designs are in Table~\@ref(tab:min-error). The table also shows that the number of components a method has used in order to obtain the minimum of average prediciton error.

```{r Average-Prediction}
avg_pred_err <- myData %>% 
  rename(pred_err = without_norm) %>% 
  group_by(Model, design, comp, R2, gamma) %>% 
  do(mean_se(.$pred_err)) %>% 
  rename(pred_err = y, upper = ymax, lower = ymin)
```

<center>
```{r minimum-average-prediction}
avgPredErr <- avg_pred_err %>%
  group_by(Model, design) %>%
  summarise(
    comp = comp[which.min(pred_err)],
    pred_err = min(pred_err),
    label = paste0("(", comp, ", ", round(pred_err, 2), ")")
  )
avgPredErr_lbl <- avgPredErr %>% 
  select(-comp, -pred_err) %>%
  mutate(design = paste0("Design: ", design)) %>% 
  spread(design, label)

avgPredErr_idx <- avgPredErr %>% 
  select(-comp, -label) %>% 
  mutate(design = paste0("Design: ", design)) %>% 
  spread(design, pred_err) %>% 
  ungroup() %>% 
  select(-Model) %>% 
  do({sapply(., function(x) x == min(x)) %>% as_tibble()}) %>%
  as.matrix() %>% 
  which(arr.ind = TRUE)
avgPredErr_idx[, "col"] <- avgPredErr_idx[, "col"] + 1

avgPredErr_lbl %>% 
  as.data.frame() %>% 
  pander(emphasize.strong.cells = avgPredErr_idx,
         emphasize.verbatim.cols = 2:ncol(.),
         emphasize.italics.cells = avgPredErr_idx,
         emphasize.italics.cols = 1,
         split.cells = Inf, justify = "lcccc",
         caption = "(\\#tab:min-error) Minimum average prediction error (number of components, prediction error)")
```
</center>

```{r}
topBest <- avgPredErr %>% 
  ungroup() %>% 
  group_by(design) %>% 
  summarize(
    Model = Model[which.min(pred_err)],
    pred_err = min(pred_err)
  )
```

Table~\@ref(tab:min-error) shows that simulteneous envelope has prediction error of `r paste(round(topBest[["pred_err"]][1:2], 2), collapse = " and ")` in design 1 and design 2 respectively which is smaller than other methods. However the model was not able to show the same performance in design 3 and design 4. Cannonical PLS and Cannonically Powered PLS has out performed other methods in these designs. Here, the difference between CPLS and CPPLS is minimal. These methods has also shown a fair performance in the first two designs with only three components. A detail picture of prediction error for each estimation method obtained for each additional component is shown in Figure~\@ref(fig:Average-Prediction-Plot). Although design 2 and design 4 has higher level of multicollinearity, the performance of the estimation methods is indifferent to its effect. Since all the methods, except OLS, are based on shrinking of estimates, they are less influenced by multicollinearity problem.

```{r Average-Prediction-Plot, fig.cap="Minimum of Average Prediction Error", fig.asp=0.7, out.width='100%', fig.width=8}
plot_label <- avg_pred_err %>% 
  ungroup() %>% 
  select(R2, gamma, design) %>% 
  unique() %>% 
  transmute(
    design = unique(design),
    Model = NA,
    x = Inf, y = c(Inf, Inf, -Inf, -Inf), 
    label = paste0("R2: ", R2, "\ngamma: ", gamma),
    type = "Properties",
    v = c(1.2, 1.2, -1.2, -1)
  )
plt <- avg_pred_err %>% 
  filter(Model != "OLS") %>% 
  mutate(type = "Plot") %>% 
  ggplot(aes(comp, pred_err, fill = Model)) +
  geom_ribbon(aes(ymax = upper, ymin = lower), alpha = 0.1) +
  geom_line(aes(color = Model), size = 0.5) +
  geom_point(shape = 21, size = 1, aes(color = Model)) +
  geom_hline(data = avg_pred_err %>% 
               filter(Model == "OLS", comp == 1),
             aes(yintercept = pred_err, color = Model), 
             linetype = 2, size = 0.5) +
  geom_point(data = avg_pred_err %>% 
               filter(Model == "OLS", comp == 1),
             aes(y = pred_err, color = Model), size = 1) +
  geom_text(aes(label = label, x = Inf, y = y, vjust = v), 
            data = plot_label,
            hjust = 1, family = "mono", size = rel(4)) +
  scale_size_continuous(range = c(0.1, 2), breaks = NULL) +
  scale_fill_discrete(l = 40) +
  labs(x = "Number of Components", 
       y = expression(paste('Prediction Error (||', alpha, '||'[F], ")"))) +
  facet_wrap( ~ design, labeller = label_both) +
  theme_gray(base_size = 14) +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  ggtitle("Prediction Error", 
          sub = "Averaged over 20 replicated Datasets of same properties") +
  guides(color = guide_legend(nrow = 1), 
         fill = guide_legend(nrow = 1))
plot(plt)
```

Above analysis has answered some questions such as how methods works when there exist a true reduced dimension in response space but also arised question like why they perform differently. For example, the reduced performance of simulteneous envelope going from design with $R^2 = (0.8, 0.8, 0.4)$ to design with $R^2 = (0.4, 0.4, 0.4)$ has arised question such as -- Does the performance of the method depends on $R^2$ or it is a random situation? Since, this paper is intended for a demonstration of how `simrel-m` can be used in scientific study, a more elaborative study is needed in order to answer such question which `simrel-m` can help as an useful instrument.


