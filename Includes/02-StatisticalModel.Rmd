# Statistical Model {#statistical-model}

Let us consider a random regression model in equation~\@ref(eq:rand-reg-model) as our point of departure.

\begin{equation}
  \begin{bmatrix}\mathbf{Y}\\ \mathbf{X}\end{bmatrix} \sim N
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_Y \\
      \boldsymbol{\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{YY} & \boldsymbol{\Sigma}_{YX} \\
      \boldsymbol{\Sigma}_{XY} & \boldsymbol{\Sigma}_{XX}
    \end{bmatrix}
  \right)
  (\#eq:rand-reg-model)
\end{equation}
where, $\mathbf{Y}$ is a response matrix with $m$ response variables $y_1, y_2, \ldots y_m$ with mean vector of $\boldsymbol{\mu}_Y$ and $\mathbf{X}$ is vector of $p$ predictor variables. Further, 

----------------------------------------------- ----------------------------------------------------------------------------------
                     $\boldsymbol{\Sigma}_{YY}$ is variance-covariance matrix of response $\mathbf{Y}$
                     $\boldsymbol{\Sigma}_{XX}$ is variance-covariance matrix of predictor variables $\mathbf{X}$
                     $\boldsymbol{\Sigma}_{XY}$ is matrix of covariance between $\mathbf{X}$ and $\mathbf{Y}$
  $\boldsymbol{\mu}_Y$ and $\boldsymbol{\mu}_X$ are mean vectors of response $\mathbf{Y}$ and predictor $\mathbf{X}$ respective.
----------------------------------------------- ----------------------------------------------------------------------------------

A linear relationship between $\mathbf{X}$ and $\mathbf{Y}$ for model~\@ref(eq:rand-reg-model) can be imagined as,

\begin{equation}
\mathbf{Y} = \boldsymbol{\mu}_Y + \boldsymbol{\beta}^t (\mathbf{X} - \boldsymbol{\mu}_X) + \boldsymbol{\varepsilon}
  (\#eq:linear-model)
\end{equation}
where, $\boldsymbol{\beta}^t$ is regression coefficient and $\boldsymbol{\varepsilon}$ is error term such that $\boldsymbol{\varepsilon} \sim N\left(0, \boldsymbol{\Sigma}_{Y|X}\right)$. The properties of the linear model in equation~\@ref(eq:linear-model) can be expressed in terms of covariance matrices from equation~\@ref(eq:rand-reg-model).

Regression Coefficients
: $$ \boldsymbol{\beta} = \boldsymbol{\Sigma}_{XY} \boldsymbol{\Sigma}_{XX}^{-1}$$

Coefficient of Determination $\boldsymbol{\rho}_Y^2$
: The diagonal elements of coefficient of determination matrix $\boldsymbol{\rho}_Y^2$ gives the amount of variation that $\boldsymbol{X}$ has explained about $\boldsymbol{Y}$ in equation~\@ref(eq:linear-model). 
$$\boldsymbol{\rho}_Y^2 = \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\boldsymbol{\Sigma}_{YY}^{-1}$$

Error variance
: The minimum error $\boldsymbol{\Sigma}_{Y|X}$ of the model is,
$$\boldsymbol{\Sigma}_{Y|X} = \boldsymbol{\Sigma}_{YY} - \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}$$

Let us define a transformation of $\boldsymbol{X}$ and $\boldsymbol{Y}$ as, $\mathbf{Z} = \mathbf{RX}$ and $\mathbf{W} = \mathbf{QY}$. Here, $\mathbf{R}_{p\times p}$ and $\mathbf{Q}_{m\times m}$ are rotation matrices which rotates $\mathbf{X}$ and $\mathbf{Y}$ giving $\mathbf{Z}$ and $\mathbf{W}$ respectively. The random regression model in equation~\@ref(eq:rand-reg-model) can be expressed with these transformed variables as,

\begin{align}
  \begin{bmatrix}\mathbf{W} \\ 
  \boldsymbol{Z}\end{bmatrix}  & \sim N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_W \\ \boldsymbol{\mu}_Z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{WW} & \boldsymbol{\Sigma}_{WZ} \\
      \boldsymbol{\Sigma}_{ZW} & \boldsymbol{\Sigma}_{ZZ}
    \end{bmatrix} \right) \nonumber \\
  & = N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_Y \\
      \boldsymbol{R\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{YY}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{YX}\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{XY}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{XX}\mathbf{R}^t
    \end{bmatrix}
  \right)
  (\#eq:model3)
\end{align}

In addition, a linear model relating $\mathbf{W}$ and $\mathbf{Z}$ can be written as,

\begin{equation}
\mathbf{W} = 	\boldsymbol{\mu}_W + \boldsymbol{\alpha}^t \left(\mathbf{Z} - \boldsymbol{\mu}_Z\right) + \boldsymbol{\tau}
(\#eq:latent-model)
\end{equation}
where, $\boldsymbol{\alpha}$ is regression coefficient for the transformed model and $\boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{W|Z}\right)$. Further, if both $\mathbf{Q}$ and $\mathbf{R}$ are orthonormal matrix such that $\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_q$ and $\mathbf{R}^t\mathbf{R} = \mathbf{I}_p$, the inverse transformation can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{XX} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{ZZ} = \mathbf{R} \boldsymbol{\Sigma}_{XX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{XY} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{ZW} = \mathbf{R} \boldsymbol{\Sigma}_{XY} \mathbf{Q}^t \\
    \boldsymbol{\Sigma}_{YX} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{WZ} = \mathbf{Q} \boldsymbol{\Sigma}_{YX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{YY} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{WW} = \mathbf{Q} \boldsymbol{\Sigma}_{YY} \mathbf{Q}^t
  \end{matrix}
  (\#eq:cov-yx-wz)
\end{equation}

Here, we can find a direct connection between different population properties between \@ref(eq:linear-model) and \@ref(eq:latent-model).

Regression Coefficients
  : 
  $$
  \begin{aligned}
  \boldsymbol{\alpha} &= \boldsymbol{\Sigma}_{WZ} \boldsymbol{\Sigma}_{ZZ}^{-1}
      &&= \boldsymbol{Q\Sigma}_{YZ}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{XX}\mathbf{R}^t\right]^{-1} \\
      &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\right]\mathbf{R}^t
      &&= \mathbf{Q}\boldsymbol{\beta}\mathbf{R}^t
  \end{aligned}
  $$

Error Variance
  : Further, the noise variance of transformed model~\@ref(eq:latent-model) is,
  $$
  \begin{aligned}
    \boldsymbol{\Sigma}_{W|Z}
    &= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t -
      \boldsymbol{Q \Sigma}_{YX}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{XX}\boldsymbol{R}^t\right]^{-1}
      \boldsymbol{R\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
    &= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t - 
      \boldsymbol{Q \Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
    &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YY} -
      \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\right]\mathbf{Q}^{t} \nonumber \\
    &= \mathbf{Q} \boldsymbol{\Sigma}_{Y|X}\mathbf{Q}^t
  \end{aligned}
  $$

Population Coefficient of Determination
  : The population coefficient of determination for model~\@ref(eq:latent-model) is,
  $$
  \begin{aligned}
    \boldsymbol{\rho}^2_W &= \boldsymbol{\Sigma}_{WZ} 
    \boldsymbol{\Sigma}_{ZZ}^{-1} \boldsymbol{\Sigma}_{ZW} 
    \boldsymbol{\Sigma}_{WW}^{-1} \\
  &=\mathbf{Q}^t
  \boldsymbol{\Sigma}_{YX}\mathbf{R}^t \left(\mathbf{R}\boldsymbol{\Sigma}_{XX}\mathbf{R}^t\right)^{-1}
  \mathbf{R}\boldsymbol{\Sigma}_{XY}\mathbf{Q}^t \left(\mathbf{Q} \boldsymbol{\Sigma}_{YY}^{-1} \mathbf{Q}^t\right) \nonumber \\
  &=\mathbf{Q}^t\left[\boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}\boldsymbol{\Sigma}_{XY}\boldsymbol{\Sigma}_{YY}^{-1}\right]\mathbf{Q} \\
  &= \mathbf{Q}\boldsymbol{\rho}_{Y}^2 \mathbf{Q}^t
  \end{aligned}
  $$

From eigenvalue decomposition principle, if $\boldsymbol{\Sigma}_{XX} = \mathbf{R}\boldsymbol{\Lambda}\mathbf{R}^t$ and $\boldsymbol{\Sigma}_{YY} = \mathbf{Q}\boldsymbol{\Omega}\mathbf{Q}^t$ then $\mathbf{Z}$ and $\mathbf{W}$ can be principal components of $\mathbf{X}$ and $\mathbf{Y}$ respectively. Here, $\boldsymbol{\Sigma}$ and $\boldsymbol{\Omega}$ are diagonal matrix of eigenvalues corresponding to $\mathbf{X}$ and $\mathbf{Y}$ respectively. 

# Relevant Components #

Let us consider a single response linear model with $p$ predictors.

$$\mathbf{y} = \mu_y + \beta^t\left(\mathbf{X} - \mu_x\right) + \epsilon$$

where, $\epsilon \sim N(0, \sigma^2)$ and $\mathbf{X}$ and $\epsilon$ are random and independent. Following the principal of relevant space and irrelevant space which are discussed extensively in @helland1994comparison, @Helland_2000, @helland2012near, @cook2013envelopes, @saebo2015simrel and @helland2017, we can assume that there exists a subspace of complete variable space which is relevant for $\mathbf{y}$. An orthogonal space to this space does not contain any information about $\mathbf{y}$ and are irrelevant. Here, the $y-$relevant subspace of $\mathbf{X}$ is spanned by a subset of eigenvectors $\mathbf{(e)}$ of covariance matrix of $\mathbf{X}$, i.e. $\boldsymbol{\Sigma}_{XX})$.

This concept can be extended for $m$ response so that the subspace of $\mathbf{X}$ is relevant for a subspace of $\mathbf{Y}$. This corresponds to the concept of simultaneous envelope [@Cook_2014] where relevant (material) and irrelevant (immaterial) space were discussed for both response and predictors.

# Model Parameterization #
In order to construct a covariance matrix of $\mathbf{Z}$ and $\mathbf{W}$ for model in equation~\@ref(eq:model3), we need to identify $1/2 (p+m)(p+m+1)$ unknowns. For the purpose of this simulation, we implement some assumption to re-parameterize and simplify the model parameters. This enables us to construct diverse nature of model from few key parameters.

**Parameterization of $\boldsymbol{\Sigma}_{ZZ}$:**
: Since $\mathbf{X}$'s are principal components of $\mathbf{X}$, the $\boldsymbol{\Sigma}_{ZZ}$ can be a diagonal matrix with eigenvalues $\lambda_1, \ldots, \lambda_p$ of predictors $\mathbf{X}$. Further, we adopt following approximate parametric representation of these eigenvalues,
$$\lambda_j = e^{-\gamma(i - 1)}, \gamma >0 \text{ and } j = 1, 2, \ldots, p$$
Here as $\gamma$ increases, the decline of eigenvalues becomes steeper and hence a single parameter $\gamma$ can be used for $\boldsymbol{\Sigma}_{ZZ}$.

**Parameterization of $\boldsymbol{\Sigma}_{WW}$:**
: Here, we assume that $\mathbf{W}$'s are independent and thus their covariance matrix is considered to be Identity $\mathbf{I}_m$.

**Parameterization of $\boldsymbol{\Sigma}_{ZW}$:**
: After parameterization of $\boldsymbol{\Sigma}_{ZZ}$ and $\boldsymbol{\Sigma}_{WW}$, we are left with $m \times p$ number of unknowns corresponding to $\boldsymbol{\Sigma}_{ZW}$. The elements in this covariance matrix depends on position of x-component that are relevant for $\mathbf{Y}$. In order to re-parameterize this covariance matrix, it is necessary to discuss about the position of relevant components in details.

## Position of relevant components ##
Let only $k_1$ components are relevant for $\mathbf{w}_1$, $k_2$ components are relevant for $\mathbf{w}_2$ and so on. Let the position of these components are given by the set $\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m$ respectively. Further, the covariance between $\mathbf{w}_j$ and $\mathbf{z}_i$ is non-zero only if $\mathbf{z}_i$ is relevant for $\mathbf{w}_j$. If $\sigma_{ij}$ be the covariance between $\mathbf{w}_j$ and $\mathbf{z_i}$ then $\sigma_{ij} \ne 0$ if $i \in \mathcal{P}_j$ where $i = 1, \ldots, p$ and $j = 1, \ldots, m$ and $\sigma_{ij} = 0$ otherwise.

In addition, the corresponding regression coefficient for $\mathbf{w}_j$ is,

$$
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i}\mathbf{t}_{ij},\qquad j = 1, 2, \ldots m
$$

where, $\mathbf{t}_{ij}$ is a matrix with column vectors of 1's and 0's such that $\mathbf{t}_{ij} = 1$ if the position relevant components for $\mathbf{w}_j$ in set $\mathcal{P}_j$ and 0 otherwise.

The position of relevant components have heavy impact on prediction. @helland1994comparison have shown that if relevant components have large variance, prediction of $\mathbf{Y}$ from $\mathbf{X}$ is relatively easy and if the variance of relevant components is small, the prediction becomes difficult given that coefficient of determination and other model parameters held constant. For example, if first and second components of $\mathbf{X}$ are relevant for $\mathbf{Y}_1$ and fifth and sixth componets are relevant for $\mathbf{Y}_2$, it is relatively easy to predict $\mathbf{Y}_1$ than $\mathbf{Y}_2$. Since, the first and second principal components have larger variance than fifth and sixth components.

Although the covariance matrix depends only on few relevant components, we can not choose these covariances freely since we also need to satisfy following two conditions:

* The covariance matrix must be positive definite
* The covariance $\sigma_{ij}$ must satisfy user defined coefficient of determination

We have the relation, $$\boldsymbol{\rho}_W^2 = \boldsymbol{\Sigma}_{ZW}^t\boldsymbol{\Sigma}_{ZZ}^{-1}\boldsymbol{\Sigma}_{ZW}\boldsymbol{\Sigma}_{WW}^I$$ Applying our assumption for simulation, $\boldsymbol{\Sigma_{WW}} = \mathbf{I}_m$ and $\boldsymbol{\Sigma}_{ZZ} = \Lambda$, we obtain,

$$
\begin{aligned}
\boldsymbol{\rho}_W^2 &= \boldsymbol{\Sigma}_{ZW}^t \Lambda^{-1} \boldsymbol{\Sigma}_{ZW} \mathbf{I}_m \\
&= \begin{bmatrix}
\sum_{i = 1}^p \sigma_{i1}^2/\lambda_i          & \ldots & \sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i \\
\vdots                                          & \ddots & \vdots \\
\sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i & \ldots & \sum_{i = 1}^p \sigma_{im}^2/\lambda_i
\end{bmatrix}
\end{aligned}
$$

Furthermore, we assume that there are no overlapping relevant components for any two $\mathbf{W}$, i.e, $n\left(\mathcal{P}_j \cap \mathcal{P}_{j*}\right) = 0$ or $\sigma_{ij}\sigma_{ij*} = 0$ for $j\ne j*$. The additional unknown parameters in diagonal should agree with user specified coefficient of determination for $\mathbf{W}_j$. i.e, $\rho_{wj}^2$ is,

$$
\rho_{wj}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i}
$$

Here, only the relevant components have non-zero covariances with $\mathbf{w}_j$, so,

$$
\rho_{wj}^2 = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}^2}{\lambda_i}
$$

For some user defined $\rho_{jw}^2$, $\sigma_{ij}^2$ determined as follows,

1) Sample $k_j$ values from uniform distribution $\mathcal{U}(-1, 1)$ distribution. Let them be, $\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}$.
2) Define, $$\sigma_{ij} = \text{Sign}\left(\mathcal{S}_i\right)\sqrt{\frac{\rho_{wj}^2\left|\mathcal{S}_i\right|}{\sum_{k\in \mathcal{P}_j}\left|\mathcal{S}_k\right|} \lambda_i}$$
for $i \in \mathcal{P}_j$ and $j = 1, \ldots, m$

## Data Simulation {#data-simulation}
After the construction of $\boldsymbol{\Xi}_{WZ}$, $n$ samples are generated from standard normal distribution of$\left(\mathbf{W}, \mathbf{Z} \right)$ considering their mean to be zero, i.e.$\boldsymbol{\mu}_W = 0$ and$\boldsymbol{\mu}_Z=0$. Since$\boldsymbol{\Xi}_{WZ}$ is positive definite,$\boldsymbol{\Xi}_{WZ}^{1/2}$ obtained from its Cholesky decomposition, can serve as one of its square root.  The simulation process constitute of following steps,

1) A matrix $\mathbf{U}_{n\times (p + q)}$ is sampled from standard normal distribution
2) Compute $\mathbf{G} = \boldsymbol{U\Xi}_{WZ}^{1/2}$

Here, first $m$  columns of $\mathbf{G}$  will serve as $\mathbf{W}$  and remaining $p$  columns will serve as $\mathbf{Z}$. Further, each row of $\mathbf{G}$  will be a vector sampled independently from joint normal distribution of $\left(\mathbf{W}, \mathbf{Z}\right)$. The final step to generate $\mathbf{X}$  and $\mathbf{Y}$  from $\mathbf{Z}$  and $\mathbf{W}$  requires corresponding rotation matrices which is discusses on following section.

## Rotation of predictor space {#rotation-predictor-space}
Simulation of predictor variables from principal components requires a construction of a rotation matrix $\mathbf{R}$ that defines a new basis for the same space as is spanned by the principle components. As any rotation matrix can be considered as $\mathbf{R}$, an eigenvalue matrix from eigenvalue decomposition of $\boldsymbol{\Sigma}_{XX}$ can be a candidate. Since simulation is a reverse engineering, the underlying covariance structure for the predictors are unknown. So, the method is free to construct a real valued orthogonal matrix that can serve for the purpose.

Among several methods [@anderson1987generation; @heiberger1978algorithm] to generate random orthogonal matrix the same method as is used in @saebo2015simrel is implemented here. The $\mathcal{Q}$ matrix obtained from QR-decomposition of a matrix filled with standard normal variates can serve as the rotation matrix $\mathbf{R}$.

The rotation can be a) unrestricted and b) restricted. The former one rotates all $p$ predictors making them some what relevant for the all response conponents and consequently all responses. However, only $q_i \le p$ predictors are relevant for for $i^\text{th}$ response component, the resticted rotation is implemented in `simrel-M`. This also ensure that $p-q_i$ predictors does not contribute anything on response component $i$ and consequently the simulated data can also be used for testing variable selection methods.

## Rotation of response space {#rotation-response-space}
`Simrel-M` has considered an exclusive relevant predictor space for each response components, i.e. a set of predictor variables only influence one response component. However, it allows user to simulate more response variable than response components. In this case, noise are added during the orthogonal rotation of response components. For example, if user wants to simulation 5 response variation from 3 response components. Two standard normal vectors are combined with response components and rotated simultaneously. The rotation can be both restricted and unrestricted as discussed in previous section. The restricted rotation is carried out combining response vectors along with noise vector in a block-wise manner according to the users choice. Illustration in fig-...

Suppose, in our previous example, if response components are combined as -- $\mathbf{W}_1, \mathbf{W}_4$, $\mathbf{W}_2$ and $\mathbf{W}_3, \mathbf{W}_5$. Here, any predictor variable is only relevant for $\mathbf{W}_1, \mathbf{W}_2$ and $\mathbf{W}_3$ while $\mathbf{W}_4$ and $\mathbf{W}_5$ are noise. The resulting response variables are $\mathbf{Y}_1 \ldots \mathbf{Y}_5$ where, the first and fourth response variable spans the same space as by the first response components $\mathbf{W}_1$ and noise component $\mathbf{W}_4$ and so on. Thus, the predictors and predictor space relevant for response component $\mathbf{W}_1$ is also relevant for response $\mathbf{Y}_1$ and $\mathbf{Y}_4$.


