# Statistical Model {#statistical-model}

Let us consider a random regression model in equation~\@ref(eq:model1) as our point of departure.

\begin{equation}
  \mathbf{Y} = \boldsymbol{\mu}_Y + \mathbf{B}^t (\mathbf{X} - \boldsymbol{\mu}_X) + \boldsymbol{\epsilon}
  (\#eq:model1)
\end{equation}

where $\mathbf{Y}$ is a response matrix with $m$ response variables $y_1, y_2, \ldots y_m$ with mean vector of $\boldsymbol{\mu}_Y$; $\mathbf{X}$ is vector of $p$ predictor variables and the random error term $\boldsymbol{\epsilon}$ is assumed to follow $N(\boldsymbol{0},\; \boldsymbol{\Sigma}_{Y|X})$. In addition, we assume equation~\@ref(eq:model1) as a random regression model where $\mathbf{X} \sim N\left(\boldsymbol{\mu}_X, \boldsymbol{\Sigma}_{XX}\right)$ independent of $\boldsymbol{\epsilon}$. Equivalently, this relationship can be written as,

\begin{equation}
  \begin{bmatrix}\mathbf{Y}\\ \mathbf{X}\end{bmatrix} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})
  = N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_Y \\
      \boldsymbol{\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{YY} & \boldsymbol{\Sigma}_{XY}^t \\
      \boldsymbol{\Sigma}_{XY} & \boldsymbol{\Sigma}_{XX}
    \end{bmatrix}
  \right)
  (\#eq:model2)
\end{equation}

Here, 
$\boldsymbol{\Sigma}_{YY}$ is Covariance Matrix of response $\mathbf{Y}$;
$\boldsymbol{\Sigma}_{XY}$ is Covariance Matrix between $\mathbf{X}$ and $\mathbf{Y}$;
$\boldsymbol{\Sigma}_{XX}$ is Covariance matrix of predictor variables $\mathbf{X}$;
$\boldsymbol{\mu}_Y$ and $\boldsymbol{\mu}_X$ are Mean vectors of response $\mathbf{Y}$ and predictor $\mathbf{X}$ respective.

Simulation of $(\mathbf{Y, X})$ for model~\@ref(eq:model2) requires the fact that -- a set of latent variable spanning $\mathbf{X}$ and $\mathbf{Y}$ will contain same information in different structure. With two matrices $\mathbf{R}_{p\times p}$ and $\mathbf{Q}_{q \times q}$ with rank $p$ and $q$ respectively, lets define a transformation as $\mathbf{Z} = \mathbf{RX}$ and $\mathbf{W} = \mathbf{QY}$ so that,

\begin{align}
  \begin{bmatrix}\mathbf{W} \\ 
  \boldsymbol{Z}\end{bmatrix}  & \sim N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_W \\ \boldsymbol{\mu}_Z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{WW} & \boldsymbol{\Sigma}_{WZ}^t \\
      \boldsymbol{\Sigma}_{ZW} & \boldsymbol{\Sigma}_{ZZ}
    \end{bmatrix} \right) \nonumber \\
  & = N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_Y \\
      \boldsymbol{R\mu}_X
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{YY}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{XY}^t\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{XY}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{XX}\mathbf{R}^t
    \end{bmatrix}
  \right)
  (\#eq:model3)
\end{align}

Further, if both $\mathbf{Q}$ and $\mathbf{R}$ are orthonormal matrix such that $\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_q$ and $\mathbf{R}^t\mathbf{R} = \mathbf{I}_p$, the inverse transformation can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{XX} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{ZZ} = \mathbf{R} \boldsymbol{\Sigma}_{XX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{XY} = \mathbf{R}^t \boldsymbol{\Sigma}_{ZW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{ZW} = \mathbf{R} \boldsymbol{\Sigma}_{XY} \mathbf{Q}^t \\
    \boldsymbol{\Sigma}_{YX} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WZ} \mathbf{R} & \Rightarrow & \boldsymbol{\Sigma}_{WZ} = \mathbf{Q} \boldsymbol{\Sigma}_{YX} \mathbf{R}^t \\
    \boldsymbol{\Sigma}_{YY} = \mathbf{Q}^t \boldsymbol{\Sigma}_{WW} \mathbf{Q} & \Rightarrow & \boldsymbol{\Sigma}_{WW} = \mathbf{Q} \boldsymbol{\Sigma}_{YY} \mathbf{Q}^t
  \end{matrix}
  (\#eq:cov-yx-wz)
\end{equation}

In addition, a linear model relating $\mathbf{W}$ and $\mathbf{Z}$ can be written as,

<!-- The relationship above allows us to define the linear model relation in equation~\@ref(eq:model1) in terms of $\mathbf{W}$ and $\mathbf{Z}$ as in equation~\@ref(eq:latent-model), -->

\begin{equation}
  \mathbf{W} = 	\boldsymbol{\mu}_W + \mathbf{A}^t \left(\mathbf{Z} - \boldsymbol{\mu}_Z\right) + \boldsymbol{\tau}; \qquad
  \boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{W|Z}\right)
  (\#eq:latent-model)
\end{equation}

Here, we can find a direct connection between different population properties between \@ref(eq:model1) and \@ref(eq:latent-model). Some of them are:

<!-- Here, in this setting the model parameters can be defined as follows where each of them can be related to the model parameter for model in equation~\@ref(eq:model1) using the ortogonal matrices $\mathbf{P}$ and $\mathbf{Q}$. -->

Regression Coefficients
  : Regression coefficients for model~\@ref(eq:model1) is,
  $$\mathbf{B} = \boldsymbol{\Sigma}_{YX} \boldsymbol{\Sigma}_{XX}^{-1}$$
  Using the transformation matrix $\mathbf{P}$ and $\mathbf{Q}$, we can obtain the regression coefficients corresponding to the latent structure of predictors.
  $$
  \begin{aligned}
    \mathbf{A} &= \boldsymbol{\Sigma}_{WZ} \boldsymbol{\Sigma}_{ZZ}^{-1}
      &= \boldsymbol{Q\Sigma}_{YZ}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{XX}\mathbf{R}^t\right]^{-1} \\
      &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\right]\mathbf{R}^t \\
      &= \mathbf{QBR}^t
  \end{aligned}
  $$

Error Variance
  : The noise variance and the minimum prediction error for model~\@ref(eq:model1) is,
  $$\boldsymbol{\Sigma}_{Y|X} = \boldsymbol{\Sigma}_{YY} - \boldsymbol{\Sigma}_{YX} \boldsymbol{\Sigma}_{XX}^{-1} \boldsymbol{\Sigma}_{XY}$$
  Further, the noise variance of transformed model~\@ref(eq:latent-model) is,
  $$
  \begin{aligned}
    \boldsymbol{\Sigma}_{W|Z}
    &= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t -
      \boldsymbol{Q \Sigma}_{YX}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{XX}\boldsymbol{R}^t\right]^{-1}
      \boldsymbol{R\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
    &= \boldsymbol{Q\Sigma}_{YY}\mathbf{Q}^t - 
      \boldsymbol{Q \Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\mathbf{Q}^t \nonumber \\
    &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{YY} -
      \boldsymbol{\Sigma}_{YX}\boldsymbol{\Sigma}_{XX}^{-1}\boldsymbol{\Sigma}_{XY}\right]\mathbf{Q}^{t} \nonumber \\
    &= \mathbf{Q} \boldsymbol{\Sigma}_{Y|X}\mathbf{Q}^t
  \end{aligned}
  $$

  <!-- Since, $\mathbf{Y}$ is generated from orthogonal random rotation matrix $\mathbf{Q}$, the error variance depends on $\mathbf{Q}$. Further, the coefficient of determination is, -->

Population Coefficient of Determination
  : The population coefficient of determination for model~\@ref(eq:model1) is,
  $$\boldsymbol{\rho}^2_{XY} = \boldsymbol{\Sigma}_{YX} \boldsymbol{\Sigma}_{XX}^{-1} \boldsymbol{\Sigma}_{XY} \boldsymbol{\Sigma}_{YY}^{-1}$$
  Further, the coefficient of determination corresponding to model~\@ref(eq:latent-model) is,
  $$
  \begin{aligned}
    \boldsymbol{\rho}^2_{ZW} &= \boldsymbol{\Sigma}_{WZ} 
    \boldsymbol{\Sigma}_{ZZ}^{-1} \boldsymbol{\Sigma}_{ZW} 
    \boldsymbol{\Sigma}_{WW}^{-1} \\
  &=\mathbf{Q}^t
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Sigma}^{-1}_{ZZ}
    \boldsymbol{\Sigma}_{ZW}\boldsymbol{\Sigma}_{WW}^{-1}\boldsymbol{Q} \nonumber \\
  &=\mathbf{Q}^t\boldsymbol{\mathcal{R}}^2_{WZ}\mathbf{Q} \nonumber \\
    \text{i.e. }\boldsymbol{\mathcal{R}}^2_{WZ} 
  &=\mathbf{Q}\boldsymbol{\mathcal{R}}^2_{XY}\mathbf{Q}^t
  \end{aligned}
  $$

<!-- Thus, on the basis of these mathematical backgrounds, the simulation strategy follows, -->

<!-- a) Construct covariance structure of $\mathbf{W}$ and $\mathbf{Z}$ satisfying given parameters -->
<!-- b) Simulate $\mathbf{W}$ and $\mathbf{Z}$ from random standard normal distribution -->
<!-- c) Rotation $\mathbf{Z}$ by orthonormal matrix $\mathbf{R}$ to yield $\mathbf{X} = \mathbf{R}^t \mathbf{Z}$ -->
<!-- d) Rotation of $\mathbf{W}$ by orthogonal matrix $\mathbf{Q}$ to yield $\mathbf{Y} = \mathbf{Q}^t \mathbf{W}$ -->
<!-- e) For simplification, we assume that no common components of $\mathbf{X}$, i.e. $\mathbf{Z}$, relevant for $\mathbf{W}$. For example, if component 1 and component 2 are relevant for $\mathbf{W}_1$, they are not relevant for other $\mathbf{W}$'s. -->

## Model parameterization and relevant components {#model-parameter-relevant-components}
Eigenvalue decomposition principal states that a variance-covariance matrix $\boldsymbol{\Sigma}$ can be decomposed as,

\begin{equation}
  \Sigma = \mathbf{E}\Lambda \mathbf{E}^t
\end{equation}

where, $\mathbf{E} = (\mathbf{e}_1, \mathbf{e}_2, \ldots \mathbf{e}_p)$ is an orthogonal matrix of eigenvectors and $\Lambda$ is a diagonal matrix of eigenvalues $\lambda_1 \le \lambda_2 \le \ldots \lambda_p$. From expression in equation~\@ref(eq:cov-yx-wz), $\boldsymbol{\Sigma}_{XX}$ and $\boldsymbol{\Sigma}_{WW}$ can have similar decomposition with some suitable choice of orthonormal matrix $\mathbf{R}$ and $\mathbf{Q}$ respectively.

In this study, all the components of $\mathbf{Y}$, i.e. $\mathbf{W}$ are considered to be uncorrelated. Since, the component structure also contains the irrelevant components, each of their correlation with others are considered to be zero. Hence, the unconditional covariance structure for the component matrix ($\mathbf{W}$) is $\mathbf{I}_m$. Furthermore, if $\boldsymbol{\Sigma}_{ZZ} = \Lambda = \text{diag}({\lambda_1, \lambda_2, \ldots, \lambda_p})$, where $\lambda_i, i = 1, \ldots p$ are eigenvalues of $\mathbf{X}$, the expression in \mbox{equation~\@ref(eq:cov-yx-wz)} helps to simulate $\mathbf{X}$ from $\mathbf{R}$, the orthonormal rotation matrix and its eigen structure $\boldsymbol{\Sigma}_{ZZ}$. Similarly from $\Sigma_{WW} = \mathbf{I}_m$ and rotaion matrix $\mathbf{Q}$, we can simulate $\mathbf{Y}$.

Let $\mathbf{W}_1, \ldots, \mathbf{W}_l$ are the components of $Y$ that are relevant to $\mathbf{Z}$ and consequently $\mathbf{X}$, $\mathbf{W}_{l+1}, \ldots, \mathbf{W}_q$ are not the outcome of $\mathbf{Z}$, the principal components of $\mathbf{Z}$ that are relevant for $\mathbf{W}$ are applicable for $\mathbf{W}_1, \ldots, \mathbf{W}_l$ only. The covariance matrix of $\mathbf{W}$ and $\mathbf{Z}$ ($\boldsymbol{\Sigma}_{WZ}$) is constructed referring to the terminology in @helland1994comparison that the principal components are termed as relevant for which $\boldsymbol{\Sigma}_{WZ}$ are non-zero.

Assume $a_1, \ldots, a_l$ number of principal components of $\mathbf{X}$ are relevant to $\mathbf{W}_1, \ldots, \mathbf{W}_l$ respectively. Let $\mathcal{P}_1, \ldots, \mathcal{P}_l$ are the sets of positions of these components, then $(\Sigma_{WZ})_{ij} \ne 0$ if $j \in \mathcal{P}_i$, $i = 1, \ldots, l$ and zero otherwise. This follows us to the matrix of regression coefficients as,

\begin{equation}
  \mathbf{A} =
  \begin{cases}
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Sigma}_{ZZ}^{-1} =
    \sum_{j \in \mathcal{P}_i}{\left(\frac{\sigma_{ij}}{\lambda_j} \mathbf{t}_j\right)} & \text{ for } i = 1, \ldots, l \\
    0 & \text{ otherwise }
  \end{cases}
\end{equation}

where, $\mathbf{t}_j$ is a $p$-vector with 1 at position $j$ and zero otherwise. As in the previous version of simrel by @saebo2015simrel, eigenvalues of $\boldsymbol{\Sigma}_{XX}$ is assumed to be different and has adopted the parametric representation as $\lambda_j 	=	e^{-\nu(j - 1)}\text{ for } \nu>0 \text{ and } j = 1, \ldots p$. Here, the parameter $\nu$ regulates the decline of $\lambda_j, j = 1, \ldots p$. Without loss of generality, for further simplification, the first and largest eigenvalues are set to one.

For complete parametrization of the matrix $\boldsymbol{\Xi}_{WZ}$ in equation~\@ref(eq:model3), covariances between $W$ and $Z$ ($\boldsymbol{\Sigma}_{WZ}$)  should be constructed such that it is positive definite and satisfy the relation,

\begin{align}
  \boldsymbol{\mathcal{R}}^{2}_{WZ}                                      &=
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Sigma}^{-1}_{ZZ}\boldsymbol{\Sigma}_{ZW}\boldsymbol{\Sigma}_{WW}^{-1} \nonumber \\
  \text{i.e, } \boldsymbol{\mathcal{R}}_{WZ}^{2}\boldsymbol{\Sigma}_{WW} &=
    \boldsymbol{\Sigma}_{WZ}\boldsymbol{\Lambda}^{-1}\boldsymbol{\Sigma}_{ZW}
(\#eq:sigmaRhoRelation)
\end{align}

For given $\boldsymbol{\mathcal{R}}_{WZ}^{2}$ and $\Sigma_{WW} = \mathbf{I}_m$, equation~\@ref(eq:sigmaRhoRelation) will be satisfied for some $\boldsymbol{\Sigma}_{WX}$ whose rows correspond to the relevant components for $\mathbf{W}$. As we have considered the situation that no relevant components are common, elements in $\boldsymbol{\Sigma}_{WZ}$ are sampled from a uniform distribution $\mathcal{U}(-1, 1) = \{s_{\mathcal{P}_{1i}}, s_{\mathcal{P}_{2i}}, \ldots s_{\mathcal{P}_{pi}}\}$, for each $i = 1, \ldots q$ as in @saebo2015simrel such that,

$$
\left(\boldsymbol{\sigma}_{WZ}\right)_{ij} = \text{sign}\left(s_{ij}\right)
\sqrt{
  \frac
    {\boldsymbol{\mathcal{R}}_{WZ}^{2}.\left|s_{ij}\right|}
    {\sum_{k\in\mathcal{P}_i}{\left|s_{ik}\right|}}
  \lambda_{j}
}
$$

for $j  \in \mathcal{P}_i$ and for each $i = 1, \ldots q$

## Data Simulation {#data-simulation}
After the construction of $\boldsymbol{\Xi}_{WZ}$, $n$ samples are generated from standard normal distribution of$\left(\mathbf{W}, \mathbf{Z} \right)$ considering their mean to be zero, i.e.$\boldsymbol{\mu}_W = 0$ and$\boldsymbol{\mu}_Z=0$. Since$\boldsymbol{\Xi}_{WZ}$ is positive definite,$\boldsymbol{\Xi}_{WZ}^{1/2}$ obtained from its Cholesky decomposition, can serve as one of its square root.  The simulation process constitute of following steps,

1) A matrix $\mathbf{U}_{n\times (p + q)}$ is sampled from standard normal distribution
2) Compute $\mathbf{G} = \boldsymbol{U\Xi}_{WZ}^{1/2}$

Here, first $m$  columns of $\mathbf{G}$  will serve as $\mathbf{W}$  and remaining $p$  columns will serve as $\mathbf{Z}$. Further, each row of $\mathbf{G}$  will be a vector sampled independently from joint normal distribution of $\left(\mathbf{W}, \mathbf{Z}\right)$. The final step to generate $\mathbf{X}$  and $\mathbf{Y}$  from $\mathbf{Z}$  and $\mathbf{W}$  requires corresponding rotation matrices which is discusses on following section.

## Rotation of predictor space {#rotation-predictor-space}
Simulation of predictor variables from principal components requires a construction of a rotation matrix $\mathbf{R}$ that defines a new basis for the same space as is spanned by the principle components. As any rotation matrix can be considered as $\mathbf{R}$, an eigenvalue matrix from eigenvalue decomposition of $\boldsymbol{\Sigma}_{XX}$ can be a candidate. Since simulation is a reverse engineering, the underlying covariance structure for the predictors are unknown. So, the method is free to construct a real valued orthogonal matrix that can serve for the purpose.

Among several methods [@anderson1987generation; @heiberger1978algorithm] to generate random orthogonal matrix the same method as is used in @saebo2015simrel is implemented here. The $\mathcal{Q}$ matrix obtained from QR-decomposition of a matrix filled with standard normal variates can serve as the rotation matrix $\mathbf{R}$.

The rotation can be a) unrestricted and b) restricted. The former one rotates all $p$ predictors making them some what relevant for the all response conponents and consequently all responses. However, only $q_i \le p$ predictors are relevant for for $i^\text{th}$ response component, the resticted rotation is implemented in `simrel-M`. This also ensure that $p-q_i$ predictors does not contribute anything on response component $i$ and consequently the simulated data can also be used for testing variable selection methods.

## Rotation of response space {#rotation-response-space}
`Simrel-M` has considered an exclusive relevant predictor space for each response components, i.e. a set of predictor variables only influence one response component. However, it allows user to simulate more response variable than response components. In this case, noise are added during the orthogonal rotation of response components. For example, if user wants to simulation 5 response variation from 3 response components. Two standard normal vectors are combined with response components and rotated simultaneously. The rotation can be both restricted and unrestricted as discussed in previous section. The restricted rotation is carried out combining response vectors along with noise vector in a block-wise manner according to the users choice. Illustration in fig-...

Suppose, in our previous example, if response components are combined as -- $\mathbf{W}_1, \mathbf{W}_4$, $\mathbf{W}_2$ and $\mathbf{W}_3, \mathbf{W}_5$. Here, any predictor variable is only relevant for $\mathbf{W}_1, \mathbf{W}_2$ and $\mathbf{W}_3$ while $\mathbf{W}_4$ and $\mathbf{W}_5$ are noise. The resulting response variables are $\mathbf{Y}_1 \ldots \mathbf{Y}_5$ where, the first and fourth response variable spans the same space as by the first response components $\mathbf{W}_1$ and noise component $\mathbf{W}_4$ and so on. Thus, the predictors and predictor space relevant for response component $\mathbf{W}_1$ is also relevant for response $\mathbf{Y}_1$ and $\mathbf{Y}_4$.


