# Statistical Model #

Let us consider a model in equation~\@ref(eq:rand-reg-model) as our point of departure.

\begin{equation}
  \begin{bmatrix}\mathbf{y}\\ \mathbf{x}\end{bmatrix} \sim N
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
      \boldsymbol{\Sigma}_{xy} & \boldsymbol{\Sigma}_{xx}
    \end{bmatrix}
  \right)
  (\#eq:rand-reg-model)
\end{equation}
where, $\mathbf{y}$ is a response vector with $m$ response variables $y_1, y_2, \ldots y_m$ with mean vector of $\boldsymbol{\mu}_y$ and $\mathbf{x}$ is vector of $p$ predictor variables with mean vector $\boldsymbol{\mu}_y$. Further, 

-------------------------- -----------------------------------------------------------------
$\boldsymbol{\Sigma}_{yy}$ is variance-covariance matrix of $\mathbf{y}$
$\boldsymbol{\Sigma}_{xx}$ is variance-covariance matrix of variables $\mathbf{x}$
$\boldsymbol{\Sigma}_{xy}$ is matrix of covariance between $\mathbf{x}$ and $\mathbf{y}$
-------------------------- -----------------------------------------------------------------

For model~\@ref(eq:rand-reg-model), standard theory in multivariate statistics may be used to show that $\mathbf{y}$ conditioned on $\mathbf{x}$ corresponds to the linear model,

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\beta}^t (\mathbf{x} - \boldsymbol{\mu}_x) + \boldsymbol{\varepsilon}
  (\#eq:linear-model)
\end{equation}
where, $\boldsymbol{\beta}^t$ is a matrix of regression coefficient and $\boldsymbol{\varepsilon}$ is error term such that $\boldsymbol{\varepsilon} \sim N\left(0, \boldsymbol{\Sigma}_{y|x}\right)$. The properties of the linear model in equation~\@ref(eq:linear-model) can be expressed in terms of covariance matrices from equation~\@ref(eq:rand-reg-model).

Regression Coefficients
: $$ \boldsymbol{\beta} = \boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}$$

Coefficient of Determination $\boldsymbol{\rho}_y^2$
: The diagonal elements of coefficient of determination matrix $\boldsymbol{\rho}_y^2$ gives the amount of variation that $\mathbf{X}$ has explained about $\mathbf{Y}$ in equation~\@ref(eq:linear-model). 
$$\boldsymbol{\rho}_y^2 = \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}$$

Error variance
: The conditional variance of $\mathbf{y}$ given $\mathbf{x}$ is, $$\boldsymbol{\Sigma}_{y|x} = \boldsymbol{\Sigma}_{yy} - \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}.$$ The diagonal elements of this matrix equals the theoretical minimum errors of prediction for each of the response variables.

Let us define a transformation of $\mathbf{X}$ and $\mathbf{Y}$ as, $\mathbf{z} = \mathbf{Rx}$ and $\mathbf{w} = \mathbf{Qy}$. Here, $\mathbf{R}_{p\times p}$ and $\mathbf{Q}_{m\times m}$ are rotation matrices which rotates $\mathbf{x}$ and $\mathbf{y}$ giving $\mathbf{z}$ and $\mathbf{w}$ respectively. The model in equation~\@ref(eq:rand-reg-model) can be expressed with these transformed variables as,

\begin{align}
  \begin{bmatrix}\mathbf{w} \\ 
  \mathbf{z}\end{bmatrix}  & \sim N \left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right) \\
  &= N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_w \\ \boldsymbol{\mu}_z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{ww} & \boldsymbol{\Sigma}_{wz} \\
      \boldsymbol{\Sigma}_{zw} & \boldsymbol{\Sigma}_{zz}
    \end{bmatrix} \right) \nonumber \\
  &= N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_y \\
      \boldsymbol{R\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{yy}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{yx}\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{xy}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{xx}\mathbf{R}^t
    \end{bmatrix}
  \right)
  (\#eq:model3)
\end{align}

In addition, a linear model relating $\mathbf{w}$ and $\mathbf{z}$ can be written as,

\begin{equation}
\mathbf{w} = 	\boldsymbol{\mu}_w + \boldsymbol{\alpha}^t \left(\mathbf{z} - \boldsymbol{\mu}_z\right) + \boldsymbol{\tau}
(\#eq:latent-model)
\end{equation}
where, $\boldsymbol{\alpha}$ is regression coefficient for the transformed model and $\boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{w|z}\right)$. Further, if both $\mathbf{Q}$ and $\mathbf{R}$ are orthonormal matrix such that $\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_q$ and $\mathbf{R}^t\mathbf{R} = \mathbf{I}_p$, the inverse transformation can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{yy} = \mathbf{Q}^t \boldsymbol{\Sigma}_{ww} \mathbf{Q} &
    \boldsymbol{\Sigma}_{yx} = \mathbf{Q}^t \boldsymbol{\Sigma}_{wz} \mathbf{R} \\
    \boldsymbol{\Sigma}_{xy} = \mathbf{R}^t \boldsymbol{\Sigma}_{zw} \mathbf{Q} &
    \boldsymbol{\Sigma}_{xx} = \mathbf{R}^t \boldsymbol{\Sigma}_{zz} \mathbf{R}
  \end{matrix}
  (\#eq:cov-yx-wz)
\end{equation}

Here, we can find a direct connection between different population properties between \@ref(eq:linear-model) and \@ref(eq:latent-model).

Regression Coefficients
  : 
  $$
  \begin{aligned}
  \boldsymbol{\alpha} &= \boldsymbol{\Sigma}_{wz} \boldsymbol{\Sigma}_{zz}^{-1}
      &&= \boldsymbol{Q\Sigma}_{YZ}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{xx}\mathbf{R}^t\right]^{-1} \\
      &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\right]\mathbf{R}^t
      &&= \mathbf{Q}\boldsymbol{\beta}\mathbf{R}^t
  \end{aligned}
  $$

Error Variance
  : Further, the noise variance of transformed model~\@ref(eq:latent-model) is,
  $$
  \begin{aligned}
    \boldsymbol{\Sigma}_{w|z}
    &= \boldsymbol{Q\Sigma}_{yy}\mathbf{Q}^t -
      \boldsymbol{Q \Sigma}_{yx}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{xx}\boldsymbol{R}^t\right]^{-1}
      \boldsymbol{R\Sigma}_{xy}\mathbf{Q}^t \nonumber \\
    &= \boldsymbol{Q\Sigma}_{yy}\mathbf{Q}^t - 
      \boldsymbol{Q \Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\mathbf{Q}^t \nonumber \\
    &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{yy} -
      \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\right]\mathbf{Q}^{t} \nonumber \\
    &= \mathbf{Q} \boldsymbol{\Sigma}_{y|x}\mathbf{Q}^t
  \end{aligned}
  $$

Coefficient of Determination
  : The coefficient of determination for model~\@ref(eq:latent-model) is,
  $$
  \begin{aligned}
    \boldsymbol{\rho}^2_w &= \boldsymbol{\Sigma}_{wz} 
    \boldsymbol{\Sigma}_{zz}^{-1} \boldsymbol{\Sigma}_{zw} 
    \boldsymbol{\Sigma}_{ww}^{-1} \\
  &=\mathbf{Q}^t
  \boldsymbol{\Sigma}_{yx}\mathbf{R}^t \left(\mathbf{R}\boldsymbol{\Sigma}_{xx}\mathbf{R}^t\right)^{-1}
  \mathbf{R}\boldsymbol{\Sigma}_{xy}\mathbf{Q}^t \left(\mathbf{Q} \boldsymbol{\Sigma}_{yy}^{-1} \mathbf{Q}^t\right) \nonumber \\
  &=\mathbf{Q}^t\left[\boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}\right]\mathbf{Q} \\
  &= \mathbf{Q}\boldsymbol{\rho}_{Y}^2 \mathbf{Q}^t
  \end{aligned}
  $$

From eigenvalue decomposition principle, if $\boldsymbol{\Sigma}_{xx} = \mathbf{R}\boldsymbol{\Lambda}\mathbf{R}^t$ and $\boldsymbol{\Sigma}_{yy} = \mathbf{Q}\boldsymbol{\Omega}\mathbf{Q}^t$ then $\mathbf{z}$ and $\mathbf{w}$ can be interpreted as principle components of $\mathbf{x}$ and $\mathbf{y}$ respectively. Here, $\boldsymbol{\Sigma}$ and $\boldsymbol{\Omega}$ are diagonal matrix of eigenvalues of $\boldsymbol{\Sigma}_{xx}$ and $\boldsymbol{\Sigma}_{yy}$ respectively. 

# Relevant Components #

Let us consider a single response linear model with $p$ predictors.

$$y = \mu_y + \boldsymbol{\beta}^t\left(\mathbf{x} - \mu_x\right) + \epsilon$$

where, $\epsilon \sim N(0, \sigma^2)$ and $\mathbf{x}$ are random and independent. Following the principle of relevant space and irrelevant space which are discussed extensively in @helland1994comparison, @Helland_2000, @helland2012near, @cook2013envelopes, @saebo2015simrel and @helland2017, we can assume that there exists a subspace of the full predictor space which is relevant for $\mathbf{y}$. An orthogonal space to this space does not contain any information about $\mathbf{y}$ and is considered as irrelevant. Here, the $y-$relevant subspace of $\mathbf{x}$ is spanned by a subset of eigenvectors of covariance matrix of $\mathbf{x}$, i.e. $\boldsymbol{\Sigma}_{xx}$.

This concept can be extended to $m$ response so that the subspace of $\mathbf{x}$ is relevant for a subspace of $\mathbf{y}$. This corresponds to the concept of simultaneous envelope [@Cook_2014] where relevant (material) and irrelevant (immaterial) space were discussed for both response and predictors.

# Model Parameterization #
In order to construct a covariance matrix of $\mathbf{z}$ and $\mathbf{w}$ for model in equation~\@ref(eq:model3), we need to identify $1/2 (p+m)(p+m+1)$ unknown parameters. For the purpose of this simulation, we implement some assumption to re-parameterize and simplify the model parameters. This enables us to construct diverse nature of model from few key parameters.

**Parameterization of $\boldsymbol{\Sigma}_{zz}$**
: If we consider the rotation matrix $\mathbf{R}$ equals to the eigenvectors of $\boldsymbol{\Sigma}_{xx}$, then $\mathbf{z}$ becomes the set of principle components of $\mathbf{x}$. In that case $\boldsymbol{\Sigma}_{zz}$ is a diagonal matrix with eigenvalues $\lambda_1, \ldots, \lambda_p$. Further, we adopt the following parametric representation of these eigenvalues,
$$\lambda_j = e^{-\gamma(i - 1)}, \gamma >0 \text{ and } j = 1, 2, \ldots, p$$
Here as $\gamma$ increases, the decline of eigenvalues becomes steeper and hence a single parameter $\gamma$ can be used for $\boldsymbol{\Sigma}_{zz}$.

**Parameterization of $\boldsymbol{\Sigma}_{ww}$**
: Here, we assume that $\mathbf{w}$'s are independent and thus their covariance matrix is considered to be Identity $\mathbf{I}_m$.

**Parameterization of $\boldsymbol{\Sigma}_{zw}$**
: After parameterization of $\boldsymbol{\Sigma}_{zz}$ and $\boldsymbol{\Sigma}_{ww}$, we are left with $m \times p$ number of unknowns corresponding to $\boldsymbol{\Sigma}_{zw}$. The elements in this covariance matrix depends on position of x-component that are relevant for $\mathbf{y}$. In order to re-parameterize this covariance matrix, it is necessary to discuss about the position of relevant components in details.

## Position of relevant components ##

Let $k_1$ components be relevant for $\mathbf{w}_1$, $k_2$ components be relevant for $\mathbf{w}_2$ and so on. Let the position of these components be given by the set $\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m$ respectively. Further, the covariance between $\mathbf{w}_j$ and $\mathbf{z}_i$ is non-zero only if $\mathbf{z}_i$ is relevant for $\mathbf{w}_j$. If $\sigma_{ij}$ is the covariance between $\mathbf{w}_j$ and $\mathbf{z_i}$ then $\sigma_{ij} \ne 0$ if $i \in \mathcal{P}_j$ where $i = 1, \ldots, p$ and $j = 1, \ldots, m$ and $\sigma_{ij} = 0$ otherwise.

In addition, the corresponding regression coefficient for $\mathbf{w}_j$ is,

$$
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i}\mathbf{t}_{ij},\qquad j = 1, 2, \ldots m
$$

where, for $j = 1, \ldots m$, $\mathbf{t}_{ij}$ is a vector with 1's and 0's such that $\mathbf{t}_{ij} = 1$ if the position of relevant components for $\mathbf{w}_j$ is in set $\mathcal{P}_j$ and 0 otherwise.

The position of relevant components have heavy impact on prediction. @helland1994comparison have shown that if relevant components have large eigenvalues (variance), prediction of $\mathbf{y}$ from $\mathbf{x}$ is relatively easy and if the eigenvalues (variance) of relevant components is small, the prediction becomes difficult given that coefficient of determination and other model parameters held constant. For example, if first and second components of $\mathbf{x}$ are relevant for $\mathbf{y}_1$ and fifth and sixth componets are relevant for $\mathbf{y}_2$, it is relatively easy to predict $\mathbf{y}_1$ than $\mathbf{y}_2$. Since, the first and second principle components have larger variance than fifth and sixth components.

Although the covariance matrix depends only on few relevant components, we can not choose these covariances freely since we also need to satisfy following two conditions:

* The covariance matrix $\Sigma_{zz}$, $\Sigma_{ww}$ and $\Sigma$ must be positive definite
* The covariance $\sigma_{ij}$ must satisfy user defined coefficient of determination

We have the relation, $$\boldsymbol{\rho}_w^2 = \boldsymbol{\Sigma}_{zw}^t\boldsymbol{\Sigma}_{zz}^{-1}\boldsymbol{\Sigma}_{zw}\boldsymbol{\Sigma}_{ww}^I$$ Applying our assumption for simulation, $\boldsymbol{\Sigma_{ww}} = \mathbf{I}_m$ and $\boldsymbol{\Sigma}_{zz} = \Lambda$, we obtain,

$$
\begin{aligned}
\boldsymbol{\rho}_w^2 &= \boldsymbol{\Sigma}_{zw}^t \Lambda^{-1} \boldsymbol{\Sigma}_{zw} \mathbf{I}_m \\
&= \begin{bmatrix}
\sum_{i = 1}^p \sigma_{i1}^2/\lambda_i          & \ldots & \sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i \\
\vdots                                          & \ddots & \vdots \\
\sum_{i = 1}^p \sigma_{i1}\sigma_{im}/\lambda_i & \ldots & \sum_{i = 1}^p \sigma_{im}^2/\lambda_i
\end{bmatrix}
\end{aligned}
$$

Furthermore, we assume that there are no overlapping relevant components for any two $\mathbf{w}$, i.e, $n\left(\mathcal{P}_j \cap \mathcal{P}_{j*}\right) = 0$ or $\sigma_{ij}\sigma_{ij*} = 0$ for $j\ne j*$. The additional unknown parameters in diagonal of $\boldsymbol{\rho}_w^2$ should agree with user specified coefficient of determination for $\mathbf{w}_j$. i.e, $\rho_{wj}^2$ is,

$$
\rho_{wj}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i}
$$

Here, only the relevant components have non-zero covariances with $\mathbf{w}_j$, so,

$$
\rho_{wj}^2 = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}^2}{\lambda_i}
$$

For some user defined $\rho_{jw}^2$, $\sigma_{ij}^2$ determined as follows,

1. Sample $k_j$ values from uniform distribution $\mathcal{U}(-1, 1)$ distribution. Let them be, $\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}$.
2. Define, $$\sigma_{ij} = \text{Sign}\left(\mathcal{S}_i\right)\sqrt{\frac{\rho_{wj}^2\left|\mathcal{S}_i\right|}{\sum_{k\in \mathcal{P}_j}\left|\mathcal{S}_k\right|} \lambda_i}$$
for $i \in \mathcal{P}_j$ and $j = 1, \ldots, m$

## Data Simulation ##

After the construction of covariance matrix, 
$$
  \boldsymbol{\Sigma} = 
  \begin{pmatrix}
    \boldsymbol{\Sigma}_{ww} & \boldsymbol{\Sigma}_{wz} \\
    \boldsymbol{\Sigma}_{zw} & \boldsymbol{\Sigma}_{zz}
  \end{pmatrix}
$$
$n$ observations are sampled from standard normal distribution of $\left(\mathbf{w}, \mathbf{z} \right)$ considering their mean to be zero, i.e. $\boldsymbol{\mu}_w = 0$ and $\boldsymbol{\mu}_z=0$. Let us define $\mathbf{G} = \mathbf{U}\boldsymbol{\Sigma}^{1/2}$, such that $\mathbf{G}^t \mathbf{G} = \boldsymbol{\Sigma}$. Since $\boldsymbol{\Sigma}$ is positive definite, $\boldsymbol{\Sigma}^{1/2}$ obtained from its Cholesky decomposition can serve as one of its square root and the matrix $\mathbf{U}_{n\times (p + q)}$ is sampled from standard normal distribution so that its covariance matrix $\mathbf{U}^t\mathbf{U} = \mathbf{I}$. In addition the covariance matrix of $\mathbf{G}$ is $\boldsymbol{\Sigma}$ which satisfies all user defined population properties.

Here the first $m$  columns of $\mathbf{G}$  will serve as $\mathbf{w}$  and remaining $p$  columns will serve as $\mathbf{z}$. Further, each row of $\mathbf{G}$  will be a vector sampled independently from joint normal distribution of $\left(\mathbf{w}, \mathbf{z}\right)$. Finally, these simulated matrices $\mathbf{w}$ and $\mathbf{z}$ are orthogonally rotated in order to obtain $\mathbf{y}$ and $\mathbf{x}$ respectively. Following section discuss about these rotation matrices in details.

## Rotation of predictor space ##

In order to make comments on predictor space, let us consider an example where a regression model with $p = 10$ predictors $(\mathbf{x})$ and $m = 4$ responses $(\mathbf{y})$. Let only 3 principle components $(w_1, w_2$ and $w_3)$ are needed to describe all 4 response variables. Further, let $\mathcal{P}_1 = \{1, 2\}, \mathcal{P}_2 = \{3, 4\}$ and $\mathcal{P}_3 = \{5, 6\}$ principle components of $\mathbf{x}$ are relevant for $w_1, w_2$ and $w_3$ respectively. Let $\mathcal{S}_1$, $\mathcal{S}_2$ and $\mathcal{S}_3$ be the space spanned by them. These space together $\mathcal{S}_k = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3$ is the minimum relevant space similar to the x-envelope as discussed by @cook2013envelopes.

Moreover, let $q_1 = 3, q_2 = 3$ and $q_3 = 2$ number of predictor variables we want to be relevant for $w_1, w_2$ and $w_3$ respectively. So that $q_1 = 3$ predictor can be obtained by rotating the principle components in $\mathcal{P}_1$ along with one more irrelevant principle components. Similarly, $q_2 = 3$ predictors, relevant for $w_2$, can be obtained by rotating principle components in $\mathcal{P}_2$ along with one more irrelevant components and $q_3 = 2$ predictors, relevant for $w_3$, can be obtained by rotating principle components in $\mathcal{P}_3$ without any additional irrelevant components. Let the space spanned by $q_1, q_2$ and $q_3$ number of predictors be $\mathcal{S}_{q_1}$, $\mathcal{S}_{q_2}$ and $\mathcal{S}_{q_3}$. Together they form a space $\mathcal{S}_q = \mathcal{S}_{q_1} \oplus \mathcal{S}_{q_2} \oplus \mathcal{S}_{q_3}$. This space is bigger than $\mathcal{S}_k$. Here, $\mathcal{S}_k$ is orthogonal to $\mathcal{S}_{p - k}$ and $\mathcal{S}_q$ is orthogonal to $\mathcal{S}_{p - q}$. Generally speaking, here we are splitting complete variable space $\mathcal{S}_p$ into two orthogonal space -- $\mathcal{S}_k$ relevant for $\mathbf{y}$ and $\mathcal{S}_{p - k}$ irrelevant for $\mathbf{y}$.

In the previous section, we discussed about constructing covariance matrix of latent structure. Figure~\@ref(fig:cov-plot-print) (left) shows a similar structure resembling the example here. The three colors represents their relevance with three latent structure of response $(w_1, w_2$ and $w_3)$. Here we can see that $z_1$ and $z_2$ (first and second principle components of $\mathbf{x}$) have non-zero covariance with $w_1$ (first latent component of response $\mathbf{y}$). In the similar manner other non-zero covariances are self-explanatory. 

```{r cov-plot-print, results = 'hide', echo = FALSE, warning = FALSE, message = FALSE, out.width = '100%', fig.asp = 0.4, fig.cap = 'Simulation of predictor and response variables after orthogonal transformation of principle components by a rotation matrix'}
pkgs <- c("simulatr", "ggplot2", "reshape2")
for (pkg in pkgs) require(pkg, character.only = T)

source("plot-function.R")
pars <- list(
  n      = 25,
  p      = 10,
  m      = 4, 
  gamma  = 0.8,
  q      = c(3, 3, 2),
  R2     = c(0.8, 0.8, 0.7),
  relpos = list(1:2, 3:4, 5:6),
  ypos   = list(1, 2, 3:4),
  type   = "multivariate"
)
sim.obj <- do.call(simulatr, pars)
plt1 <- plot(cov.df(sim.obj, type = "relpos", ordering = TRUE), "relpos") +
  ggtitle("PC Covariance Matrix")
plt2 <- plot(cov.df(sim.obj, type = "rotation"), "relpred") +
  ggtitle("Rotation Matrix")
plt3 <- plot(cov.df(sim.obj, type = "relpred"), "relpred") +
  ggtitle("simulated data cov. matrix")
do.call(share_legend, list(plt1, plt2, plt3, ncol = 3))
```

In order to simulate predictor variables $(\mathbf{x})$,  we construct matrix $\mathbf{R}$ which then is used for orthogonal rotation of principle components $\mathbf{z}$. This defines a new basis for the same space as is spanned by the principle components. In principle, there are many possible options for a rotation matrix. Among them, the eigenvector matrix of $\boldsymbol{\Sigma}_{xx}$ can be a candidate. However, in this reverse engineering both rotation matrices $\mathbf{R}$ and $\mathbf{Q}$ along with the covariance matrices $\boldsymbol{\Sigma}_{xx}$ are unknown. So, we are free to choose any $\mathbf{R}$ that satisfied the properties of a real valued rotation matrix, i.e $\mathbf{R}^{-1} = \mathbf{R}^t$ so that $\mathbf{R}$ is orthonormal and its determinant becomes $\pm 1$. Here the rotation matrix $\mathbf{R}$ should be block diagonal as in figure~\@ref(fig:cov-plot-print) (middle) in order to rotate spaces $\mathcal{S}_1, \mathcal{S}_2 \ldots$ separately. Figure~\@ref(fig:simulated-data) (left) shows the simulated principle components $\mathbf{z}$ that we are following in our example where we can see that the principle component $z_1$ and $z_2$ relevant for $w_1$ is getting rotated together with an irrelevant $z_10$. The resultant predictors (Figure~\@ref(fig:simulated-data), right) $x_1$, $x_2$ and $x_10$ will also be relevant for $w_1$. In the figure, we can see that principle components $z_8$ and $z_9$ and corresponding predictors are not relevant for any of the response.

```{r simulated-data, results = 'hide', echo = FALSE, warning = FALSE, message = FALSE, out.width = '100%', fig.asp = 0.4, fig.cap = 'Simulated Data before (left) and after (right) rotation'}
obs.plt1 <- get_obsplot(sim.obj)
obs.plt2 <- get_obsplot(sim.obj, FALSE)
do.call(share_legend, list(obs.plt1, obs.plt2, ncol = 2))
```

Among several methods [@anderson1987generation; @heiberger1978algorithm] for generating random orthogonal matrix, in this paper we are using orthogonal matrix $\mathcal{Q}$ obtained from QR-decomposition of a matrix filled with standard normal variate. The rotation here can be a) restricted and b) unrestricted. The later one rotates all principle components $\mathbf{z}$ together and makes all predictor variables somewhat relevant for all response variables. However, the former one performs a block-wise rotation so that it rotates certain selected principle components together. This gives control for specifying certain predictors as relevant for selected response, which was discussed in our example above. This also lets us to simulate irrelevant predictors such as $x_7$ and $x_8$ which can be detected during variables selection procedure.

<!-- ## Rotation of response space ## -->

<!-- `Simrel-M` has considered an exclusive relevant predictor space for each response components, i.e. a set of predictor variables only influence one response component. However, it allows user to simulate more response variable than response components. In this case, noise are added during the orthogonal rotation of response components. For example, if user wants to simulation 5 response variation from 3 response components. Two standard normal vectors are combined with response components and rotated simultaneously. The rotation can be both restricted and unrestricted as discussed in previous section. The restricted rotation is carried out combining response vectors along with noise vector in a block-wise manner according to the users choice. Illustration in fig-... -->

<!-- Suppose, in our previous example, if response components are combined as -- $\mathbf{w}_1, \mathbf{w}_4$, $\mathbf{w}_2$ and $\mathbf{w}_3, \mathbf{w}_5$. Here, any predictor variable is only relevant for $\mathbf{w}_1, \mathbf{w}_2$ and $\mathbf{w}_3$ while $\mathbf{w}_4$ and $\mathbf{w}_5$ are noise. The resulting response variables are $\mathbf{y}_1 \ldots \mathbf{y}_5$ where, the first and fourth response variable spans the same space as by the first response components $\mathbf{w}_1$ and noise component $\mathbf{w}_4$ and so on. Thus, the predictors and predictor space relevant for response component $\mathbf{w}_1$ is also relevant for response $\mathbf{y}_1$ and $\mathbf{y}_4$.  -->
