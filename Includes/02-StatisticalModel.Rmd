# Statistical Model #

In this section we describe the model and the model parameterization which is assumed throughout this paper. We assume:
\begin{equation}
  \begin{bmatrix}\mathbf{y}\\ \mathbf{x}\end{bmatrix} \sim N
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
      \boldsymbol{\Sigma}_{yx}^t & \boldsymbol{\Sigma}_{xx}
    \end{bmatrix}
  \right)
  (\#eq:rand-reg-model)
\end{equation}
where, $\mathbf{y}$ is a response vector with $m$ response variables $y_1, y_2, \ldots y_m$ with mean vector $\boldsymbol{\mu}_y$, and $\mathbf{x}$ is vector of $p$ predictor variables with mean vector $\boldsymbol{\mu}_x$. Further,

--------------------------------------- -----------------------------------------------------------------
$\boldsymbol{\Sigma}_{yy} (m \times m)$ is the variance-covariance matrix of $\mathbf{y}$
$\boldsymbol{\Sigma}_{xx} (p \times p)$ is the variance-covariance matrix of variables $\mathbf{x}$
$\boldsymbol{\Sigma}_{yx} (m \times p)$ is the matrix of covariance between $\mathbf{x}$ and $\mathbf{y}$
--------------------------------------- -----------------------------------------------------------------

\addtocounter{table}{-1}

Standard theory in multivariate statistics may be used to show that $\mathbf{y}$ conditioned on $\mathbf{x}$ corresponds to the linear model,

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\beta}^t (\mathbf{x} - \boldsymbol{\mu}_x) + \boldsymbol{\varepsilon}
  (\#eq:linear-model)
\end{equation}
where, $\boldsymbol{\beta}^t$ is a $(m \times p)$ matrix of regression coefficients, and $\boldsymbol{\varepsilon}$ is an error term such that $\boldsymbol{\varepsilon} \sim N\left(0, \boldsymbol{\Sigma}_{y|x}\right)$. The properties of the linear model \@ref(eq:linear-model) can be expressed in terms of covariance matrices in \@ref(eq:rand-reg-model).

Regression Coefficients
  : The matrix of regression coefficients is given by
    $$\boldsymbol{\beta} = \boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}$$

Coefficient of Determination
  : Since, a matrix of coefficient-of-determination represents the proportion of variation explained by the predictors, we can write this matrix as,
    $$
    \left(\rho_y^2\right)_{jj'} =
    \frac{\sigma_{xy_j}^t\Sigma_{xx}^{-1}\sigma_{xy_{j'}}}
        {\sqrt{\sigma_{y_j}^2\sigma_{y_{j'}}^2}} \forall j, j' = 1 \ldots m
    $$ 
    where, $\sigma_{xy_j}$, $\sigma_{xy_{j'}}$ are covariance between $\mathbf{x}$ and $y_j$, $y_{j'}$ respectively. Also, $\sigma_{y_j}^2$ and $\sigma_{y_{j'}}^2$ are unconditional variances of $y_j$ and $y_{j'}$.

    Here the numerator gives the variation that $\mathbf{x}$ explained which is equivalent to the covariance of fitted $\mathbf{y}$ in sample space. The denominator gives the total variation present in $\mathbf{y}$. The diagonal element of this matrix is the proportion of variation in a response $y_j, j = 1, \ldots m$ explained by the predictor.

Conditional variance
  : The conditional variance-covariance matrix of $\mathbf{y}$ given $\mathbf{x}$ is, $$\boldsymbol{\Sigma}_{y|x} = \boldsymbol{\Sigma}_{yy} - \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}.$$ The diagonal elements of this matrix equals the minimum least square error of prediction $\left[\mathrm{E}(y - \hat{y})^2\right]$ for each of the response variables.

Let us define a transformation of $\mathbf{x}$ and $\mathbf{y}$ as, $\mathbf{z} = \mathbf{Rx}$ and $\mathbf{w} = \mathbf{Qy}$. Here, $\mathbf{R}_{p\times p}$ and $\mathbf{Q}_{m\times m}$ are rotation matrices that rotate $\mathbf{x}$ and $\mathbf{y}$ to yield $\mathbf{z}$ and $\mathbf{w}$, respectively. The model \@ref(eq:rand-reg-model) can be re-expressed in terms of these transformed variables as:

\begin{align}
  \begin{bmatrix}\mathbf{w} \\
  \mathbf{z}\end{bmatrix}  & \sim N \left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right)
  = N \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_w \\ \boldsymbol{\mu}_z
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{ww} & \boldsymbol{\Sigma}_{wz} \\
      \boldsymbol{\Sigma}_{zw} & \boldsymbol{\Sigma}_{zz}
    \end{bmatrix} \right) \nonumber \\
  &= N \left(
    \begin{bmatrix}
      \boldsymbol{Q\mu}_y \\
      \boldsymbol{R\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
      \boldsymbol{Q\Sigma}_{yy}\boldsymbol{Q}^t & \boldsymbol{Q\Sigma}_{yx}\mathbf{R}^t \\
      \boldsymbol{R\Sigma}_{xy}\boldsymbol{Q}^t & \boldsymbol{R\Sigma}_{xx}\mathbf{R}^t
    \end{bmatrix}
  \right)
  (\#eq:model3)
\end{align}

In addition, a linear model relating $\mathbf{w}$ conditioned on $\mathbf{z}$ can be written as,

\begin{equation}
\mathbf{w} =  \boldsymbol{\mu}_w + \boldsymbol{\alpha}^t \left(\mathbf{z} - \boldsymbol{\mu}_z\right) + \boldsymbol{\tau}
(\#eq:latent-model)
\end{equation}
where $\boldsymbol{\alpha}$ is the regression coefficient vector for the transformed model and $\boldsymbol{\tau} \sim N\left(\mathbf{0}, \boldsymbol{\Sigma}_{w|z}\right)$. Further, if both $\mathbf{Q}$ and $\mathbf{R}$ are orthonormal matrices, i.e., $\mathbf{Q}^t\mathbf{Q} = \mathbf{I}_m$ and $\mathbf{R}^t\mathbf{R} = \mathbf{I}_p$, the inverse transformation can be defined as,

\begin{equation}
  \begin{matrix}
    \boldsymbol{\Sigma}_{yy} = \mathbf{Q}^t \boldsymbol{\Sigma}_{ww} \mathbf{Q} &
    \boldsymbol{\Sigma}_{yx} = \mathbf{Q}^t \boldsymbol{\Sigma}_{wz} \mathbf{R} \\
    \boldsymbol{\Sigma}_{xy} = \mathbf{R}^t \boldsymbol{\Sigma}_{zw} \mathbf{Q} &
    \boldsymbol{\Sigma}_{xx} = \mathbf{R}^t \boldsymbol{\Sigma}_{zz} \mathbf{R}
  \end{matrix}
  (\#eq:cov-yx-wz)
\end{equation}

From this, we can find a direct connection between different population properties of \@ref(eq:linear-model) and \@ref(eq:latent-model).

Regression Coefficients
  :
  $$
  \begin{aligned}
  \boldsymbol{\alpha} &= \boldsymbol{\Sigma}_{wz} \boldsymbol{\Sigma}_{zz}^{-1}
      = \boldsymbol{Q\Sigma}_{yz}\mathbf{R}^t\left[\boldsymbol{R\Sigma}_{xx}\mathbf{R}^t\right]^{-1}
      = \mathbf{Q}\left[\boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\right]\mathbf{R}^t
      = \mathbf{Q}\boldsymbol{\beta}\mathbf{R}^t
  \end{aligned}
  $$

Conditional Variance
  : Further, the conditional variance-covariance matrix of $\mathbf{w}$ given $\mathbf{z}$ is,
  $$
  \begin{aligned}
    \boldsymbol{\Sigma}_{w|z}
    &= \boldsymbol{\Sigma}_{ww} - \boldsymbol{\Sigma}_{wz}\boldsymbol{\Sigma}_{zz}^{-1}\boldsymbol{\Sigma}_{zw} \\
    &= \boldsymbol{Q\Sigma}_{yy}\mathbf{Q}^t -
      \boldsymbol{Q \Sigma}_{yx}\mathbf{R}^t \left[\boldsymbol{R\Sigma}_{xx}\boldsymbol{R}^t\right]^{-1}
      \boldsymbol{R\Sigma}_{xy}\mathbf{Q}^t \nonumber \\
    &= \boldsymbol{Q\Sigma}_{yy}\mathbf{Q}^t -
      \boldsymbol{Q \Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\mathbf{Q}^t \\
    &= \mathbf{Q}\left[\boldsymbol{\Sigma}_{yy} -
      \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\right]\mathbf{Q}^{t}
    = \mathbf{Q} \boldsymbol{\Sigma}_{y|x}\mathbf{Q}^t
  \end{aligned}
  $$

Coefficient of Determination 
  : The coefficient-of-determination matrix corresponding to $\mathbf{w}$ can be written as,
  $$
  \begin{aligned}
  \left(\rho_w^2\right)_{j{j'}} &=
    \boldsymbol{\Sigma}_{ww}^{-1/2}
    \boldsymbol{\Sigma}_{wz}
    \boldsymbol{\Sigma}_{zz}^{-1} 
    \boldsymbol{\Sigma}_{zw}
    \boldsymbol{\Sigma}_{ww}^{-1/2} \\ &=
  \frac{\sigma_{zw_i}^t\Sigma_{zz}^{-1}\sigma_{zw_{j'}}}
      {\sqrt{\sigma_{w_i}^2\sigma_{w_{j'}}^2}} \forall j, {j'} = 1 \ldots m
  \end{aligned}
  $$ 
  where, $\sigma_{zw_j}$ and $\sigma_{zw_{j'}}$ are covariances of $\mathbf{z}$ with $w_j$ and $w_{j'}$ respectively. Also, $\sigma_{w_j}^2$ and $\sigma_{w_{j'}}^2$ are unconditional variances of $w_j$ and $w_{j'}$. For simplicity, we will denote $\sigma_{z_iw_j}$ by $\sigma_{ij}$.

  Since the rotation matrices gives direct connection between the covariance of \@ref(eq:rand-reg-model) and \@ref(eq:model3), a straight forward relationship can be imagined between the termes in above matrix with their counterpart covariance matrices of $\mathbf{xy}$-space.

From the eigenvalue decomposition principle, if $\boldsymbol{\Sigma}_{xx} = \mathbf{R}\boldsymbol{\Lambda}\mathbf{R}^t$ and $\boldsymbol{\Sigma}_{yy} = \mathbf{Q}\boldsymbol{\Omega}\mathbf{Q}^t$ then $\mathbf{z}$ and $\mathbf{w}$ can be interpreted as principal components of $\mathbf{x}$ and $\mathbf{y}$ respectively. In this paper, these principal components will be termed as _predictor components_ and _response components_ respectively.  Here, $\boldsymbol{\Lambda}$ and $\boldsymbol{\Omega}$ are diagonal matrices of eigenvalues of $\boldsymbol{\Sigma}_{xx}$ and $\boldsymbol{\Sigma}_{yy}$, respectively.

# Relevant Components #

Consider a single response linear model with $p$ predictors.

$$y = \mu_y + \boldsymbol{\beta}^t\left(\mathbf{x} - \mu_x\right) + \epsilon$$

where, $\epsilon \sim N(0, \sigma^2)$ and $\mathbf{x}$ is a vector of random predictors. Following the concept of relevant space and irrelevant space which is discussed extensively in @helland1994comparison, @Helland2000, @helland2012near, @cook2013envelopes, and @saebo2015simrel, we can assume that there exists a subspace of the full predictor space which is relevant for $y$. An orthogonal space to this space does not contain any information about $y$ and is considered as irrelevant. Here, the $y-$relevant subspace of $\mathbf{x}$ is spanned by a subset of the principal components defined by the eigenvectors of the covariance matrix of $\mathbf{x}$, i.e. $\boldsymbol{\Sigma}_{xx}$.

This concept can be extended to $m$ responses so that the subspace of $\mathbf{x}$ is relevant for a subspace of $\mathbf{y}$. This corresponds to the concept of simultaneous envelopes [@cook2015simultaneous] where relevant (material) and irrelevant (immaterial) space were discussed for both response and predictor variables.

## Model Parameterization ##

In order to construct a fully specified and unrestricted covariance matrix of $\mathbf{z}$ and $\mathbf{w}$ for the model in equation \@ref(eq:model3), we need to identify $1/2 (p+m)(p+m+1)$ unknown parameters. For the purpose of simulation, we implement some assumptions to re-parameterize and simplify the model. This enables us to construct a wide range of model properties from only few key parameters.

**Parameterization of $\boldsymbol{\Sigma}_{zz}$**
: If we let the rotation matrix $\mathbf{R}$ correspond to the eigenvectors of $\boldsymbol{\Sigma}_{xx}$, then $\mathbf{z}$ becomes the set of principal components of $\mathbf{x}$. In that case $\boldsymbol{\Sigma}_{zz}$ is a diagonal matrix with eigenvalues $\lambda_1, \ldots, \lambda_p$. Further, we adopt the same parametric representation as @saebo2015simrel for these eigenvalues:
  \begin{equation}
  \lambda_i = e^{-\gamma(i - 1)}, \gamma >0 \text{ and } i = 1, 2, \ldots, p
  (\#eq:gamma-parameter)
  \end{equation}
  Here, as $\gamma$ increases, the decline of eigenvalues becomes steeper, hence the parameter $\gamma$ controls the level of multicollinearity in $\mathbf{x}$. Hence, we can write $\Sigma_{zz} = \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)$.

**Parameterization of $\boldsymbol{\Sigma}_{ww}$**
: In similar manner, a parametric representation of eigenvalues corresponding to $\Sigma_{ww}$ is adopted as,
  \begin{equation}
  \kappa_j = e^{-\eta(j - 1)}, \eta >0 \text{ and } j = 1, 2, \ldots, m
  (\#eq:eta-parameter)
  \end{equation}
  Here, the decline of eigenvalues becomes steeper as $\eta$ increases from zero. At $\eta = 0$, all $w$ will have equal variance. Hence we can write $\Sigma_{ww} = \text{diag}(\kappa_1, \ldots, \kappa_m)$.

**Parameterization of $\boldsymbol{\Sigma}_{zw}$**
: After parameterization of $\boldsymbol{\Sigma}_{zz}$ and $\boldsymbol{\Sigma}_{ww}$, we are left with $m \times p$ number of unknowns corresponding to $\boldsymbol{\Sigma}_{zw}$. Some of the elements of $\boldsymbol{\Sigma}_{zw}$ may be equal to zero, which implies that the given $z$ is irrelevant for the given variable $w$. The non-zero elements define which of the $z$ are relevant for $\mathbf{w}$. We typically refer to the indices of these $z$ variables as the positions of relevant components. In order to re-parameterize this covariance matrix, it is necessary to discuss the position of relevant components in detail.

### Position of relevant components ###

Let $k_1$ components be relevant for $w_1$, $k_2$ components be relevant for $w_2$ and so on. Let the positions of these components be given by the index sets $\mathcal{P}_1, \mathcal{P}_2, \ldots, \mathcal{P}_m$ respectively. Further, the covariance between $w_j$ and $z_i$ is non-zero only if $z_i$ is relevant for $w_j$. If $\sigma_{ij}$ is the covariance between $w_j$ and $z_i$ then $\sigma_{ij} \ne 0$ if $i \in \mathcal{P}_j$ where $i = 1, \ldots, p$ and $j = 1, \ldots, m$ and $\sigma_{ij} = 0$ otherwise.

In addition, the true regression coefficients $\alpha$ for $w_j$ \@ref(eq:latent-model) is given by:

$$
\boldsymbol{\alpha}_j = \Lambda^{-1} \sigma_{ij} = \sum_{i \in \mathcal{P}_j}\frac{\sigma_{ij}}{\lambda_i},\qquad j = 1, 2, \ldots m
$$

The positions of the relevant components have heavy impact on prediction. @helland1994comparison have shown that if the relevant components have large eigenvalues (variances), which here implies small index values in $\mathcal{P}_j$, prediction of $\mathbf{y}$ from $\mathbf{x}$ is relatively easy and if the eigenvalues (variances) of relevant components are small, the prediction becomes difficult, given that the coefficient of determination and other model parameters are held constant. For example, if the first and second components, $z_1$ and $z_2$, are relevant for $w_1$ and fifth and sixth components, $z_5$ and $z_6$, are relevant for $w_2$, it is relatively easier to predict $w_1$ than $w_2$, other properties being similar. This might be so, because the first and second principal components have larger variances than the fifth and sixth components.

Although the covariance matrix may depend on few relevant components, we can not choose these covariances freely since we also need to satisfy following two conditions:

* The covariance matrices $\Sigma_{zz}$, $\Sigma_{ww}$ and $\Sigma$ must be positive definite
* The covariance $\sigma_{ij}$ must satisfy user defined coefficient of determination

We have the relation, 
$$
  \begin{aligned}
    \boldsymbol{\rho}_w^2 &= 
    \boldsymbol{\Sigma}_{ww}^{-1/2}
    \boldsymbol{\Sigma}_{zw}^t
    \boldsymbol{\Sigma}_{zz}^{-1}
    \boldsymbol{\Sigma}_{zw}
    \boldsymbol{\Sigma}_{ww}^{-1/2} \\ &=
    \frac{\sigma_{ij}^t\Lambda^{-1}\sigma_{ij'}}
        {\sqrt{\sigma_{j}^2\sigma_{j'}^2}} \forall j, {j'} = 1 \ldots m
  \end{aligned}
$$ 
Applying our assumptions that, $\boldsymbol{\Sigma_{ww}} = \text{diag}(\kappa_1, \ldots, \kappa_m)$ \@ref(eq:eta-parameter) and $\boldsymbol{\Sigma}_{zz} = \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)$ \@ref(eq:gamma-parameter), we obtain,


$$
  \rho_{w}^2 = \Sigma_{ww}^{-1/2}\Sigma_{zw}^t\Lambda^{-1}\Sigma_{zw}\Sigma_{ww}^{-1/2} = 
  \begin{bmatrix}
    \displaystyle\sum_{i = 1}^p\dfrac{\sigma_{i1}^2}{\lambda_i\kappa_1} & 
    \ldots &
    \displaystyle\sum_{i=1}^p\dfrac{\sigma_{i1}\sigma_{im}}{\lambda_i\sqrt{\kappa_1\kappa_m}} \\
    \vdots & \ddots & \vdots \\
    \displaystyle\sum_{i=1}^p\dfrac{\sigma_{i1}\sigma_{im}}{\lambda_i\sqrt{\kappa_1\kappa_m}} & 
    \ldots & 
    \displaystyle\sum_{i = 1}^p\dfrac{\sigma_{im}^2}{\lambda_i\kappa_m}
  \end{bmatrix} 
$$

Furthermore, we assume that there are no overlapping relevant components for any two $w$, i.e, $\mathcal{P}_j \cap \mathcal{P}_{j*} = \varnothing$ or $\sigma_{ij}\sigma_{ij*} = 0$ for $j\ne j*$. The additional unknown parameters in the diagonal of $\boldsymbol{\rho}_w^2$ should agree with user specified coefficients of determination for $\mathbf{w}$. i.e, $\rho_{w_j}^2$ is,

$$
\rho_{w_j}^2 = \sum_{i = 1}^p\frac{\sigma_{ij}^2}{\lambda_i\kappa_j}
$$

Here, only the relevant components have non-zero covariances with $w_j$, so,

$$
\rho_{w_j}^2 = \sum_{i \in \mathcal{P}_j} \frac{\sigma_{ij}^2}{\lambda_i\kappa_j}
$$

For some user defined $\rho_{w_j}^2$ the $\sigma_{ij}^2$ is determined as follows,

1. Sample $k_j$ values from a uniform distribution $\mathcal{U}(-1, 1)$ distribution. Let them be denoted $\mathcal{S}_{\mathcal{P}_1}, \ldots, \mathcal{S}_{\mathcal{P}_{k_j}}$.
2. Define, $$\sigma_{ij} = \text{Sign}(\mathcal{S}_i)\sqrt{\frac{\rho_{w_j}^2\left|\mathcal{S}_i\right|}{\sum_{k\in\mathcal{P}_j}\left|\mathcal{S}_k\right|}\lambda_i\kappa_j}$$
for $i \in \mathcal{P}_j$ and $j = 1, \ldots m$


### Data Simulation ###

From the above given parameterizations and the user defined choices of model parameters, a fully defined and known covariance matrix $\boldsymbol{\Sigma}$ of $(\mathbf{w, z})$ is given. For the simulation of a single observation of $(\mathbf{w, z})$ let us define $\mathbf{g} = \boldsymbol{\Sigma}^{-1/2}\mathbf{u}$ such that $\text{cov}(\mathbf{g}) = \boldsymbol{\Sigma}$. Here $\boldsymbol{\Sigma}^{-1/2}$ is obtained from Choleskey decomposition of $\boldsymbol{\Sigma}$, and $\mathbf{u}$ is simulated from independent standard normal distribution.

Similarly, in order to simulate $n$ observations, we define $\underset{n \times (m + p)}{\mathbf{G}} = \mathbf{U}\boldsymbol{\Sigma}^{-1/2}$.  Here the first $m$  columns of $\mathbf{G}$  will serve as $\mathbf{W}$  and remaining $p$  columns will serve as $\mathbf{Z}$. Further, each row of $\mathbf{G}$  will be a vector sampled independently from the joint normal distribution of $\left(\mathbf{w}, \mathbf{z}\right)$. Finally, these simulated matrices $\mathbf{W}$ and $\mathbf{Z}$ are orthogonally rotated in order to obtain $\mathbf{Y}$ and $\mathbf{X}$ respectively. The following section discuss about these rotation matrices in detail.

## Rotation of predictor space
```{r basic-simulation, message=FALSE, warning=FALSE, echo = FALSE}
set.seed(123)
pars <- list(
  n      = 25,
  p      = 10,
  m      = 4,
  gamma  = 0.8,
  q      = c(3, 3, 2),
  R2     = c(0.8, 0.8, 0.7),
  relpos = list(1:2, 3:4, 5:6),
  ypos   = list(1, 2, 3:4),
  type   = "multivariate"
)
sim.obj    <- do.call(simrel, pars)
relpos.z   <- lapply(sim.obj$relpos, function(x) if (length(x) > 0) paste0("z_{", x, "}"))
relpos.x   <- lapply(sim.obj$relpos, function(x) if (length(x) > 0) paste0("x_{", x, "}"))
irrelpos.z <- paste0("z_{", setdiff(1:sim.obj$p, unlist(sim.obj$relpos)), "}")
irrelpos.x <- paste0("x_{", setdiff(1:sim.obj$p, unlist(sim.obj$relpos)), "}")
rel.z      <- lapply(sim.obj$relpred, function(x) if (length(x) > 0) paste0("z_{", x, "}"))
rel.x      <- lapply(sim.obj$relpred, function(x) if (length(x) > 0) paste0("x_{", x, "}"))
irrel.z    <- paste0("z_{", setdiff(1:sim.obj$p, unlist(sim.obj$relpred)), "}")
irrel.x    <- paste0("x_{", setdiff(1:sim.obj$p, unlist(sim.obj$relpred)), "}")
```

Initially, let us consider an example where a regression model with $p = 10$ predictors $(\mathbf{x})$ and $m = 4$ responses $(\mathbf{y})$. Let's assume that only three response components $(w_1, w_2$ and $w_3)$ are needed to describe all four response variables. Further, let the index sets $\mathcal{P}_1 = \{1, 2\}, \mathcal{P}_2 = \{3, 4\}$ and $\mathcal{P}_3 = \{5, 6\}$ define the positions of the predictor components of $\mathbf{x}$ that are relevant for $w_1, w_2$ and $w_3$ respectively. Let $\mathcal{S}_1$, $\mathcal{S}_2$ and $\mathcal{S}_3$ be the orthogonal spaces spanned by each set of predictor components. These spaces together span $\mathcal{S}_k = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3$ which is the minimum relevant space and equivalent to the x-envelope as discussed by @cook2013envelopes.

Moreover, let $q_1 = 3, q_2 = 3$ and $q_3 = 2$ be the number of predictor variables we want to have relevant for $w_1, w_2$ and $w_3$ respectively. Then $q_1 = 3$ predictors may be obtained by rotating the predictor components in $\mathcal{P}_1$ along with one more irrelevant component. Similarly, $q_2 = 3$ predictors, relevant for $w_2$, can be obtained by rotating predictor components in $\mathcal{P}_2$ along with one more irrelevant component and finally, $q_3 = 2$ predictors, relevant for $w_3$, can be obtained by rotating the components in $\mathcal{P}_3$ without any additional irrelevant component. Let the space spanned by the $q_1, q_2$ and $q_3$ number of predictors be $\mathcal{S}_{q_1}$, $\mathcal{S}_{q_2}$ and $\mathcal{S}_{q_3}$. Together they span a space $\mathcal{S}_q = \mathcal{S}_{q_1} \oplus \mathcal{S}_{q_2} \oplus \mathcal{S}_{q_3}$. This space is bigger than $\mathcal{S}_k$ since in the process two irrelevant components were included in the rotations. Here, $\mathcal{S}_k$ is orthogonal to $\mathcal{S}_{p - k}$ and $\mathcal{S}_q$ is orthogonal to $\mathcal{S}_{p - q}$. Generally speaking, here we are splitting the complete variable space $\mathcal{S}_p$ into two orthogonal spaces -- $\mathcal{S}_k$ relevant for $\mathbf{w}$ and $\mathcal{S}_{p - k}$ irrelevant for $\mathbf{w}$.

In the previous section, we discussed about the construction of a covariance matrix for the latent structure. Figure \@ref(fig:cov-plot-print)(a) shows a similar structure resembling the example here. The three colors represent the relevance with the three latent response components $(w_1, w_2$ and $w_3)$. Here we can see that $`r rel.z[[1]][1]`$ and $`r rel.z[[1]][2]`$ (first and second predictor components of $\mathbf{x}$) have non-zero covariance with $w_1$ (first latent component of response $\mathbf{y}$). In the similar manner other non-zero covariances are self-explanatory.

```{r cov-plot-print, echo = FALSE, warning = FALSE, message = FALSE, out.width = '33%', fig.asp = 1.2, fig.subcap=c("Relevant Position", "Rotation Matrix", "Relevant Predictors"), fig.cap = 'Simulation of predictor and response variables after orthogonal transformation of predictor and response components by rotation matrices $Q$ and $R$ shown as the upper left and the lower right block matrices in (b).', fig.width=1.5, dpi = 800, fig.pos = "!htb"}
plt1 <- plot(cov.df(sim.obj, type = "relpos", ordering = TRUE), "relpos")
plt2 <- plot(cov.df(sim.obj, type = "rotation"), "relpred")
plt3 <- plot(cov.df(sim.obj, type = "relpred"), "relpred")
plusTheme <- theme_grey(base_size = 7) +
  theme(text = element_text(size = 5.5),
        legend.title = element_blank(),
        legend.position = "top",
        plot.title = element_blank(),
        legend.key.size = unit(0.23, "cm"),
        legend.margin = margin(2, 2, 0, 2),
        plot.margin = margin(2, 2, 1, 1),
        panel.grid = element_blank(),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        axis.ticks = element_line(size = 0.05))
savePlot <- function(plt, filename, width = 5.5, height = 5.5, dev = "tiff"){
  ggsave(plt,  filename = filename,
         units = "cm",  width = width,  height = height,
         device = dev,  dpi = 800)
}
(plt1 <- plt1 + plusTheme)
(plt2 <- plt2 + plusTheme)
(plt3 <- plt3 + plusTheme)

## Save TIFF
if (!file.exists("images/cov-plot-print-1.tiff")) {
  savePlot(plt1, filename = "images/cov-plot-print-1.tiff")
}
if (!file.exists("images/cov-plot-print-2.tiff")) {
  savePlot(plt2, filename = "images/cov-plot-print-2.tiff")
}
if (!file.exists("images/cov-plot-print-3.tiff")) {
  savePlot(plt3, filename = "images/cov-plot-print-3.tiff")
}
```

In order to simulate predictor variables $(\mathbf{x})$,  we construct matrix $\mathbf{R}$ which then is used for orthogonal rotation of the predictor components $\mathbf{z}$. This defines a new basis for the same space as is spanned by the predictor components. In principle, there are many possible options for defining a rotation matrix. Among them, the eigenvector matrix of $\boldsymbol{\Sigma}_{xx}$ can be a candidate. However, in this reverse engineering approach both rotation matrices $\mathbf{R}$ and $\mathbf{Q}$ along with the covariance matrices $\boldsymbol{\Sigma}_{xx}$ are unknown. So, we are free to choose any $\mathbf{R}$ that satisfied the properties of a real valued rotation matrix, i.e $\mathbf{R}^{-1} = \mathbf{R}^t$ so that $\mathbf{R}$ is orthonormal. Here the rotation matrix $\mathbf{R}$ should be block diagonal as in Figure \@ref(fig:cov-plot-print)(b) in order to rotate spaces $\mathcal{S}_1, \mathcal{S}_2 \ldots$ separately. Figure \@ref(fig:simulated-data)(a) shows the simulated predictor components $\mathbf{z}$ that we are following in our example where we can see that the components $`r rel.z[[1]][1]`$ and $`r rel.z[[1]][2]`$ (relevant for $w_1$) is getting rotated together with an irrelevant component $`r rel.z[[1]][3]`$. The resultant predictors (Figure \@ref(fig:simulated-data)(b)) $`r rel.x[[1]][-3]`$ and $`r rel.x[[1]][3]`$ will hence also be relevant for $w_1$. In the figure, we can see that components $`r irrelpos.z[-4]`$ and $`r irrelpos.z[4]`$ are not relevant for any responses before rotation, however, the $`r setdiff(irrelpos.x, irrel.x)`$ predictors become relevant after rotation keeping $`r irrel.x[1]`$ and $`r irrel.x[2]`$ still irrelevant.

```{r simulated-data, echo = FALSE, warning = FALSE, message = FALSE, out.width = '48%', fig.cap = 'Simulated Data before', fig.align='center', fig.subcap=c("Before Rotation", "After Rotation"), fig.pos='!htb', fig.width=1.7, fig.asp=0.8}
withGrid <- theme_grey(base_size = 8) +
  theme(text = element_text(size = 6),
        legend.title = element_blank(),
        legend.position = "top",
        plot.title = element_blank(),
        legend.key.size = unit(0.25, "cm"),
        legend.margin = margin(2, 2, 0, 2),
        plot.margin = margin(2, 2, 1, 1),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        axis.ticks = element_line(size = 0.05))
obsPlt1 <- get_obsplot(sim.obj, base.size = 8,  point.size = 0.7) + coord_cartesian(ylim = c(-3, 3))
obsPlt2 <- get_obsplot(sim.obj, FALSE, base.size = 8,  point.size = 0.7) + coord_cartesian(ylim = c(-3, 3))
obsPlt1 + withGrid
obsPlt2 + withGrid

## Save TIFF
if (!file.exists("images/simulated-data-1.tiff")) {
  savePlot(obsPlt1, filename = "images/simulated-data-1.tiff", width = 7)
}
if (!file.exists("images/simulated-data-2.tiff")) {
  savePlot(obsPlt2, filename = "images/simulated-data-2.tiff", width = 7)
}
```

Among several methods [@anderson1987generation; @heiberger1978algorithm] for generating random orthogonal matrix, in this paper we are using orthogonal matrix $\mathcal{Q}$ obtained from QR-decomposition of a matrix filled with standard normal variate. The rotation here can be a) restricted and b) unrestricted. The latter rotates all components $\mathbf{z}$ together and makes all predictor variables somewhat relevant for all response components. However, the former performs a block-wise rotation so that it rotates certain selected predictor components together. This gives control for specifying certain predictors as relevant for selected responses, which was discussed in our example above. This also allows us to simulate irrelevant predictors such as $`r irrel.x[1]`$ and $`r irrel.x[2]`$ which can be detected during variable selection procedures.

## Rotation of response space

The previous example has four response variables with only three informative components $w_1, w_2$ and $w_3$. During the rotation procedure, the response space is also rotated along with the predictor space. Figure \@ref(fig:cov-plot-print) shows that the informative response component $w_3$ is rotated together with the uninformative response component $w_4$ so that the predictors which were relevant for $w_3$ will be relevant for response variables $y_3$ and $y_4$. Similarly, response components $w_1$ and $w_2$ are rotated separately so that predictors relevant for $w_1$ and $w_2$ will only be relevant for $y_1$ and $y_2$ respectively, which we can see in Figure~\@ref(fig:simulated-data). In the r-package _simrel-m_, the combining of the response components is specified by a parameter `ypos`.
